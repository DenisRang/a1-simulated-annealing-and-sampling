{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a1-task1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXpmdZMFb1Rp",
        "colab_type": "text"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github.com/DenisRang/a1-simulated-annealing-and-sampling/blob/master/a1_task1.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTnck0rdiKjz",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukPpwJ5BkqaV",
        "colab_type": "text"
      },
      "source": [
        "### Import common packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5EU-yHmSIK6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "34f1c67b-0446-4aa6-a00d-6f69500874b6"
      },
      "source": [
        "import numpy as np\n",
        "import numpy.random as rn\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASOdaWsJkYAQ",
        "colab_type": "text"
      },
      "source": [
        "### Observe Iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auWPkEhTSL9n",
        "colab_type": "code",
        "outputId": "a3cfa405-35df-4805-af32-2c02ffd02251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "    \n",
        "# Load the Iris dataset\n",
        "iris_dataset = load_iris()\n",
        "\n",
        "print('The feature values for Obs 0 are: ', iris_dataset.data[0])\n",
        "print('The feature names are: ', iris_dataset.feature_names)\n",
        "print('The target value for Obs 0 is:', iris_dataset.target[0])\n",
        "print('The target name for Obs 0 is:', iris_dataset.target_names[iris_dataset.target[0]])\n",
        "print('The minimum values of the four features are:', np.min(iris_dataset.data, axis = 0))\n",
        "print('The maximum values of the four features are:', np.max(iris_dataset.data, axis = 0))\n",
        "print('The unique target values are:', np.unique(iris_dataset.target))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The feature values for Obs 0 are:  [5.1 3.5 1.4 0.2]\n",
            "The feature names are:  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "The target value for Obs 0 is: 0\n",
            "The target name for Obs 0 is: setosa\n",
            "The minimum values of the four features are: [4.3 2.  1.  0.1]\n",
            "The maximum values of the four features are: [7.9 4.4 6.9 2.5]\n",
            "The unique target values are: [0 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNSVPe18SyUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "NUM_CLASSES = 3\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    iris_dataset.data, iris_dataset.target)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_valid = keras.utils.to_categorical(y_valid, NUM_CLASSES)\n",
        "y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLuaD8xJWLKN",
        "colab_type": "text"
      },
      "source": [
        "### Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOh8P8_Gh1Za",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential([\n",
        "    keras.layers.Dense(5, activation=\"relu\", use_bias=False, input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(5, activation=\"relu\", use_bias=False),\n",
        "    keras.layers.Dense(NUM_CLASSES, activation=\"softmax\",use_bias=False)\n",
        "  ])\n",
        "  opt = keras.optimizers.SGD(lr=.01)\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "def evaluate_model_state(weights):\n",
        "  model.set_weights(weights)\n",
        "  train_evaluating = model.evaluate(X_train,y_train,batch_size=X_train.shape[0],verbose=0)\n",
        "  test_evaluating = model.evaluate(X_test,y_test,batch_size=X_test.shape[0],verbose=0)\n",
        "  print(f'Train loss: {train_evaluating[0]:.3f}, accuracy: {train_evaluating[1]:.3f}')\n",
        "  print(f'Test loss: {test_evaluating[0]:.3f}, accuracy: {test_evaluating[1]:.3f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svl8blzwtz3o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "5d7a1331-89bb-4f35-d3e4-12cf2ab8fb14"
      },
      "source": [
        "model = build_model()\n",
        "model.summary()\n",
        "initial_weights = model.get_weights()\n",
        "evaluate_model_state(initial_weights)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_21 (Dense)             (None, 5)                 20        \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 5)                 25        \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 3)                 15        \n",
            "=================================================================\n",
            "Total params: 60\n",
            "Trainable params: 60\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train loss: 0.858, accuracy: 0.357\n",
            "Test loss: 1.002, accuracy: 0.263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y175_2yJth9P",
        "colab_type": "text"
      },
      "source": [
        "### By default, Keras uses Glorot initialization with a uniform distribution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swYV6vLJV3vw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "bbbfd6df-55a7-40eb-be02-200769e76119"
      },
      "source": [
        "print(initial_weights)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[ 0.22054231,  0.22916901,  0.07192302, -0.8043059 , -0.498211  ],\n",
            "       [-0.6238756 ,  0.55699134,  0.6127715 ,  0.09454632, -0.38827342],\n",
            "       [ 0.42361486, -0.6964945 ,  0.42162454, -0.04250377, -0.16968489],\n",
            "       [-0.765055  ,  0.19359207,  0.52382755, -0.6470417 ,  0.79719996]],\n",
            "      dtype=float32), array([[-0.49101827,  0.02012753,  0.16411269,  0.5486325 ,  0.01057982],\n",
            "       [-0.26942337, -0.55275804, -0.38396582,  0.5652425 ,  0.14161402],\n",
            "       [-0.08487016,  0.27456534, -0.38373148,  0.33987522, -0.7701875 ],\n",
            "       [-0.13724935,  0.13591063, -0.7104934 ,  0.7089046 ,  0.36674953],\n",
            "       [ 0.4061767 , -0.7643064 , -0.18775403, -0.5719166 ,  0.46294427]],\n",
            "      dtype=float32), array([[ 0.17377144, -0.37475264, -0.4740113 ],\n",
            "       [-0.5442736 , -0.11661631, -0.19872105],\n",
            "       [ 0.63624364,  0.67785984,  0.7644586 ],\n",
            "       [ 0.74997026, -0.8186787 , -0.35235137],\n",
            "       [ 0.11053187, -0.50812536,  0.05526239]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy2_5u8khAKH",
        "colab_type": "text"
      },
      "source": [
        "## Using simulated annealing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s4Kr415hJ47",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br1MpqjUmaYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def annealing(cost_function,\n",
        "            random_neighbour,\n",
        "            temperature,\n",
        "            debug=True):\n",
        "  \"\"\" Optimize the black-box function 'cost_function' with the simulated annealing algorithm.\"\"\"\n",
        "  state = copy.deepcopy(initial_weights)\n",
        "  cost = cost_function(state)\n",
        "  step = 1\n",
        "  T = 1\n",
        "  costs, states, temperatures = [cost], [state], [T]\n",
        "  temperature_cooled_down = 0.0001\n",
        "\n",
        "  while T > temperature_cooled_down:\n",
        "    new_state = random_neighbour(state)\n",
        "    new_cost = cost_function(new_state)\n",
        "    alpha = get_alpha(cost, new_cost, T)\n",
        "    u = rn.random()\n",
        "    if debug: print(f\"Step #{step}:  T = {T:.2f}, cost = {cost:.2f}, new_cost = {new_cost:.2f}, alpha = {alpha:.2f}, u = {u:.2f}\")\n",
        "    if alpha >= u:\n",
        "      state, cost = new_state, new_cost\n",
        "      costs.append(cost)\n",
        "      states.append(state)\n",
        "      temperatures.append(T)\n",
        "    step += 1\n",
        "    T = temperature(T)\n",
        "  return step, states, costs, temperatures\n",
        "\n",
        "def cost_function(weights):\n",
        "  \"\"\" Function to minimize.\"\"\"\n",
        "  model.set_weights(weights)\n",
        "  return model.evaluate(X_train,y_train,batch_size=X_train.shape[0],verbose=0)[0]\n",
        "\n",
        "def clip(weight):\n",
        "  \"\"\" Force weight to be in the appropriate interval for neural network weights.\"\"\"\n",
        "  interval = (-1, 1)\n",
        "  a, b = interval\n",
        "  return max(min(weight, b), a)\n",
        "\n",
        "def random_neighbour(weights):\n",
        "  result_weights = copy.deepcopy(initial_weights)\n",
        "  \"\"\"Generate new weights using the normal distribution with mean equal to previous weights\"\"\"\n",
        "  sigma = 1\n",
        "  for layer_idx in range(0,len(weights)):\n",
        "    for neuron_idx in range(0,len(weights[layer_idx])):     \n",
        "      for weight_idx in range(0,len(weights[layer_idx][neuron_idx])):\n",
        "        random_weight=rn.normal(result_weights[layer_idx][neuron_idx][weight_idx],sigma)\n",
        "        result_weights[layer_idx][neuron_idx][weight_idx]=clip(random_weight)\n",
        "  return result_weights\n",
        "\n",
        "def get_alpha(cost, new_cost, temperature):\n",
        "  \"\"\"Calculate acceptance ratio\"\"\"\n",
        "  return np.exp(- (new_cost - cost) / temperature)\n",
        "\n",
        "def temperature(old_temperature):\n",
        "  \"\"\" Temperature dicreasing as the process goes on.\"\"\"\n",
        "  a = 0.99\n",
        "  return old_temperature*a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgFUEWoDmtLp",
        "colab_type": "code",
        "outputId": "5f879fdd-edb7-4746-f0dc-1e1043c6c6cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start_time = time.time()\n",
        "num_steps, states, costs, temperatures = annealing(cost_function,\n",
        "                                                          random_neighbour,\n",
        "                                                          temperature,\n",
        "                                                          debug=True)\n",
        "\n",
        "finish_time = time.time()"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step #1:  T = 1.00, cost = 0.86, new_cost = 1.46, alpha = 0.55, u = 0.62\n",
            "Step #2:  T = 0.99, cost = 0.86, new_cost = 1.18, alpha = 0.72, u = 0.58\n",
            "Step #3:  T = 0.98, cost = 1.18, new_cost = 0.95, alpha = 1.27, u = 0.59\n",
            "Step #4:  T = 0.97, cost = 0.95, new_cost = 0.99, alpha = 0.96, u = 0.28\n",
            "Step #5:  T = 0.96, cost = 0.99, new_cost = 3.62, alpha = 0.06, u = 0.55\n",
            "Step #6:  T = 0.95, cost = 0.99, new_cost = 1.21, alpha = 0.79, u = 0.55\n",
            "Step #7:  T = 0.94, cost = 1.21, new_cost = 2.61, alpha = 0.23, u = 0.16\n",
            "Step #8:  T = 0.93, cost = 2.61, new_cost = 1.50, alpha = 3.29, u = 0.54\n",
            "Step #9:  T = 0.92, cost = 1.50, new_cost = 1.61, alpha = 0.88, u = 0.96\n",
            "Step #10:  T = 0.91, cost = 1.50, new_cost = 2.26, alpha = 0.43, u = 0.79\n",
            "Step #11:  T = 0.90, cost = 1.50, new_cost = 2.57, alpha = 0.31, u = 0.53\n",
            "Step #12:  T = 0.90, cost = 1.50, new_cost = 3.29, alpha = 0.14, u = 0.36\n",
            "Step #13:  T = 0.89, cost = 1.50, new_cost = 1.02, alpha = 1.73, u = 0.99\n",
            "Step #14:  T = 0.88, cost = 1.02, new_cost = 1.15, alpha = 0.86, u = 0.17\n",
            "Step #15:  T = 0.87, cost = 1.15, new_cost = 2.66, alpha = 0.18, u = 0.09\n",
            "Step #16:  T = 0.86, cost = 2.66, new_cost = 1.26, alpha = 5.06, u = 0.12\n",
            "Step #17:  T = 0.85, cost = 1.26, new_cost = 1.87, alpha = 0.49, u = 0.57\n",
            "Step #18:  T = 0.84, cost = 1.26, new_cost = 1.18, alpha = 1.11, u = 0.15\n",
            "Step #19:  T = 0.83, cost = 1.18, new_cost = 1.41, alpha = 0.76, u = 0.10\n",
            "Step #20:  T = 0.83, cost = 1.41, new_cost = 2.61, alpha = 0.23, u = 0.68\n",
            "Step #21:  T = 0.82, cost = 1.41, new_cost = 1.11, alpha = 1.43, u = 0.23\n",
            "Step #22:  T = 0.81, cost = 1.11, new_cost = 0.86, alpha = 1.37, u = 0.25\n",
            "Step #23:  T = 0.80, cost = 0.86, new_cost = 1.34, alpha = 0.55, u = 0.66\n",
            "Step #24:  T = 0.79, cost = 0.86, new_cost = 1.09, alpha = 0.75, u = 0.95\n",
            "Step #25:  T = 0.79, cost = 0.86, new_cost = 0.94, alpha = 0.91, u = 0.31\n",
            "Step #26:  T = 0.78, cost = 0.94, new_cost = 0.95, alpha = 0.98, u = 0.86\n",
            "Step #27:  T = 0.77, cost = 0.95, new_cost = 1.21, alpha = 0.71, u = 0.90\n",
            "Step #28:  T = 0.76, cost = 0.95, new_cost = 1.10, alpha = 0.82, u = 0.14\n",
            "Step #29:  T = 0.75, cost = 1.10, new_cost = 1.93, alpha = 0.33, u = 0.63\n",
            "Step #30:  T = 0.75, cost = 1.10, new_cost = 0.88, alpha = 1.34, u = 0.45\n",
            "Step #31:  T = 0.74, cost = 0.88, new_cost = 2.06, alpha = 0.20, u = 0.33\n",
            "Step #32:  T = 0.73, cost = 0.88, new_cost = 1.22, alpha = 0.63, u = 0.75\n",
            "Step #33:  T = 0.72, cost = 0.88, new_cost = 1.55, alpha = 0.40, u = 0.39\n",
            "Step #34:  T = 0.72, cost = 1.55, new_cost = 1.42, alpha = 1.19, u = 0.72\n",
            "Step #35:  T = 0.71, cost = 1.42, new_cost = 1.55, alpha = 0.84, u = 1.00\n",
            "Step #36:  T = 0.70, cost = 1.42, new_cost = 2.04, alpha = 0.42, u = 0.09\n",
            "Step #37:  T = 0.70, cost = 2.04, new_cost = 3.89, alpha = 0.07, u = 0.36\n",
            "Step #38:  T = 0.69, cost = 2.04, new_cost = 2.13, alpha = 0.88, u = 0.33\n",
            "Step #39:  T = 0.68, cost = 2.13, new_cost = 0.91, alpha = 5.95, u = 0.90\n",
            "Step #40:  T = 0.68, cost = 0.91, new_cost = 1.64, alpha = 0.34, u = 0.25\n",
            "Step #41:  T = 0.67, cost = 1.64, new_cost = 0.92, alpha = 2.95, u = 0.98\n",
            "Step #42:  T = 0.66, cost = 0.92, new_cost = 1.09, alpha = 0.78, u = 0.09\n",
            "Step #43:  T = 0.66, cost = 1.09, new_cost = 1.80, alpha = 0.34, u = 0.42\n",
            "Step #44:  T = 0.65, cost = 1.09, new_cost = 1.22, alpha = 0.81, u = 0.27\n",
            "Step #45:  T = 0.64, cost = 1.22, new_cost = 1.38, alpha = 0.79, u = 0.41\n",
            "Step #46:  T = 0.64, cost = 1.38, new_cost = 1.40, alpha = 0.96, u = 0.00\n",
            "Step #47:  T = 0.63, cost = 1.40, new_cost = 3.94, alpha = 0.02, u = 0.32\n",
            "Step #48:  T = 0.62, cost = 1.40, new_cost = 1.25, alpha = 1.26, u = 0.85\n",
            "Step #49:  T = 0.62, cost = 1.25, new_cost = 1.42, alpha = 0.77, u = 0.84\n",
            "Step #50:  T = 0.61, cost = 1.25, new_cost = 1.28, alpha = 0.96, u = 0.15\n",
            "Step #51:  T = 0.61, cost = 1.28, new_cost = 0.82, alpha = 2.14, u = 0.00\n",
            "Step #52:  T = 0.60, cost = 0.82, new_cost = 1.69, alpha = 0.23, u = 0.94\n",
            "Step #53:  T = 0.59, cost = 0.82, new_cost = 1.02, alpha = 0.71, u = 0.48\n",
            "Step #54:  T = 0.59, cost = 1.02, new_cost = 2.32, alpha = 0.11, u = 0.41\n",
            "Step #55:  T = 0.58, cost = 1.02, new_cost = 0.68, alpha = 1.79, u = 0.79\n",
            "Step #56:  T = 0.58, cost = 0.68, new_cost = 1.27, alpha = 0.36, u = 0.03\n",
            "Step #57:  T = 0.57, cost = 1.27, new_cost = 1.16, alpha = 1.22, u = 0.16\n",
            "Step #58:  T = 0.56, cost = 1.16, new_cost = 2.33, alpha = 0.13, u = 0.86\n",
            "Step #59:  T = 0.56, cost = 1.16, new_cost = 2.89, alpha = 0.04, u = 0.87\n",
            "Step #60:  T = 0.55, cost = 1.16, new_cost = 1.88, alpha = 0.27, u = 0.25\n",
            "Step #61:  T = 0.55, cost = 1.88, new_cost = 2.02, alpha = 0.77, u = 0.39\n",
            "Step #62:  T = 0.54, cost = 2.02, new_cost = 2.35, alpha = 0.55, u = 0.71\n",
            "Step #63:  T = 0.54, cost = 2.02, new_cost = 2.56, alpha = 0.37, u = 0.19\n",
            "Step #64:  T = 0.53, cost = 2.56, new_cost = 0.99, alpha = 19.27, u = 0.27\n",
            "Step #65:  T = 0.53, cost = 0.99, new_cost = 1.19, alpha = 0.68, u = 0.12\n",
            "Step #66:  T = 0.52, cost = 1.19, new_cost = 1.26, alpha = 0.88, u = 0.72\n",
            "Step #67:  T = 0.52, cost = 1.26, new_cost = 1.00, alpha = 1.66, u = 0.57\n",
            "Step #68:  T = 0.51, cost = 1.00, new_cost = 1.12, alpha = 0.79, u = 0.25\n",
            "Step #69:  T = 0.50, cost = 1.12, new_cost = 1.44, alpha = 0.53, u = 0.90\n",
            "Step #70:  T = 0.50, cost = 1.12, new_cost = 1.12, alpha = 1.00, u = 0.47\n",
            "Step #71:  T = 0.49, cost = 1.12, new_cost = 1.17, alpha = 0.90, u = 0.18\n",
            "Step #72:  T = 0.49, cost = 1.17, new_cost = 1.31, alpha = 0.75, u = 0.03\n",
            "Step #73:  T = 0.48, cost = 1.31, new_cost = 1.03, alpha = 1.78, u = 0.39\n",
            "Step #74:  T = 0.48, cost = 1.03, new_cost = 1.42, alpha = 0.44, u = 0.57\n",
            "Step #75:  T = 0.48, cost = 1.03, new_cost = 1.05, alpha = 0.96, u = 0.74\n",
            "Step #76:  T = 0.47, cost = 1.05, new_cost = 2.03, alpha = 0.12, u = 0.96\n",
            "Step #77:  T = 0.47, cost = 1.05, new_cost = 1.66, alpha = 0.27, u = 0.01\n",
            "Step #78:  T = 0.46, cost = 1.66, new_cost = 1.39, alpha = 1.78, u = 0.50\n",
            "Step #79:  T = 0.46, cost = 1.39, new_cost = 2.05, alpha = 0.24, u = 0.33\n",
            "Step #80:  T = 0.45, cost = 1.39, new_cost = 1.99, alpha = 0.26, u = 0.57\n",
            "Step #81:  T = 0.45, cost = 1.39, new_cost = 3.00, alpha = 0.03, u = 0.88\n",
            "Step #82:  T = 0.44, cost = 1.39, new_cost = 1.31, alpha = 1.20, u = 0.03\n",
            "Step #83:  T = 0.44, cost = 1.31, new_cost = 1.47, alpha = 0.70, u = 0.42\n",
            "Step #84:  T = 0.43, cost = 1.47, new_cost = 3.61, alpha = 0.01, u = 0.07\n",
            "Step #85:  T = 0.43, cost = 1.47, new_cost = 4.33, alpha = 0.00, u = 0.50\n",
            "Step #86:  T = 0.43, cost = 1.47, new_cost = 1.03, alpha = 2.81, u = 0.52\n",
            "Step #87:  T = 0.42, cost = 1.03, new_cost = 1.19, alpha = 0.68, u = 0.25\n",
            "Step #88:  T = 0.42, cost = 1.19, new_cost = 1.07, alpha = 1.34, u = 0.72\n",
            "Step #89:  T = 0.41, cost = 1.07, new_cost = 1.37, alpha = 0.48, u = 0.30\n",
            "Step #90:  T = 0.41, cost = 1.37, new_cost = 0.70, alpha = 5.07, u = 0.59\n",
            "Step #91:  T = 0.40, cost = 0.70, new_cost = 1.99, alpha = 0.04, u = 0.99\n",
            "Step #92:  T = 0.40, cost = 0.70, new_cost = 1.79, alpha = 0.07, u = 0.26\n",
            "Step #93:  T = 0.40, cost = 0.70, new_cost = 2.25, alpha = 0.02, u = 0.89\n",
            "Step #94:  T = 0.39, cost = 0.70, new_cost = 2.47, alpha = 0.01, u = 0.50\n",
            "Step #95:  T = 0.39, cost = 0.70, new_cost = 1.15, alpha = 0.32, u = 0.11\n",
            "Step #96:  T = 0.38, cost = 1.15, new_cost = 1.68, alpha = 0.25, u = 0.36\n",
            "Step #97:  T = 0.38, cost = 1.15, new_cost = 1.95, alpha = 0.12, u = 0.73\n",
            "Step #98:  T = 0.38, cost = 1.15, new_cost = 1.63, alpha = 0.28, u = 0.16\n",
            "Step #99:  T = 0.37, cost = 1.63, new_cost = 1.46, alpha = 1.56, u = 0.57\n",
            "Step #100:  T = 0.37, cost = 1.46, new_cost = 1.46, alpha = 1.01, u = 1.00\n",
            "Step #101:  T = 0.37, cost = 1.46, new_cost = 0.89, alpha = 4.74, u = 0.54\n",
            "Step #102:  T = 0.36, cost = 0.89, new_cost = 1.63, alpha = 0.13, u = 0.18\n",
            "Step #103:  T = 0.36, cost = 0.89, new_cost = 1.23, alpha = 0.39, u = 0.33\n",
            "Step #104:  T = 0.36, cost = 1.23, new_cost = 0.86, alpha = 2.82, u = 0.52\n",
            "Step #105:  T = 0.35, cost = 0.86, new_cost = 1.23, alpha = 0.35, u = 0.37\n",
            "Step #106:  T = 0.35, cost = 0.86, new_cost = 1.71, alpha = 0.09, u = 0.80\n",
            "Step #107:  T = 0.34, cost = 0.86, new_cost = 1.31, alpha = 0.27, u = 0.15\n",
            "Step #108:  T = 0.34, cost = 1.31, new_cost = 1.17, alpha = 1.52, u = 0.29\n",
            "Step #109:  T = 0.34, cost = 1.17, new_cost = 0.68, alpha = 4.28, u = 0.65\n",
            "Step #110:  T = 0.33, cost = 0.68, new_cost = 1.57, alpha = 0.07, u = 0.26\n",
            "Step #111:  T = 0.33, cost = 0.68, new_cost = 1.19, alpha = 0.21, u = 0.78\n",
            "Step #112:  T = 0.33, cost = 0.68, new_cost = 1.22, alpha = 0.19, u = 0.96\n",
            "Step #113:  T = 0.32, cost = 0.68, new_cost = 2.49, alpha = 0.00, u = 0.89\n",
            "Step #114:  T = 0.32, cost = 0.68, new_cost = 0.77, alpha = 0.76, u = 0.05\n",
            "Step #115:  T = 0.32, cost = 0.77, new_cost = 1.09, alpha = 0.36, u = 0.44\n",
            "Step #116:  T = 0.31, cost = 0.77, new_cost = 1.29, alpha = 0.19, u = 0.45\n",
            "Step #117:  T = 0.31, cost = 0.77, new_cost = 1.09, alpha = 0.35, u = 0.59\n",
            "Step #118:  T = 0.31, cost = 0.77, new_cost = 1.23, alpha = 0.22, u = 0.70\n",
            "Step #119:  T = 0.31, cost = 0.77, new_cost = 1.21, alpha = 0.24, u = 0.47\n",
            "Step #120:  T = 0.30, cost = 0.77, new_cost = 1.24, alpha = 0.21, u = 0.87\n",
            "Step #121:  T = 0.30, cost = 0.77, new_cost = 0.98, alpha = 0.49, u = 0.48\n",
            "Step #122:  T = 0.30, cost = 0.98, new_cost = 1.32, alpha = 0.32, u = 0.27\n",
            "Step #123:  T = 0.29, cost = 1.32, new_cost = 0.82, alpha = 5.41, u = 0.73\n",
            "Step #124:  T = 0.29, cost = 0.82, new_cost = 2.31, alpha = 0.01, u = 0.33\n",
            "Step #125:  T = 0.29, cost = 0.82, new_cost = 1.59, alpha = 0.07, u = 0.97\n",
            "Step #126:  T = 0.28, cost = 0.82, new_cost = 1.81, alpha = 0.03, u = 0.11\n",
            "Step #127:  T = 0.28, cost = 0.82, new_cost = 2.20, alpha = 0.01, u = 0.33\n",
            "Step #128:  T = 0.28, cost = 0.82, new_cost = 0.96, alpha = 0.60, u = 0.66\n",
            "Step #129:  T = 0.28, cost = 0.82, new_cost = 2.54, alpha = 0.00, u = 0.26\n",
            "Step #130:  T = 0.27, cost = 0.82, new_cost = 2.27, alpha = 0.00, u = 0.73\n",
            "Step #131:  T = 0.27, cost = 0.82, new_cost = 1.88, alpha = 0.02, u = 0.06\n",
            "Step #132:  T = 0.27, cost = 0.82, new_cost = 3.32, alpha = 0.00, u = 0.50\n",
            "Step #133:  T = 0.27, cost = 0.82, new_cost = 1.56, alpha = 0.06, u = 0.71\n",
            "Step #134:  T = 0.26, cost = 0.82, new_cost = 1.17, alpha = 0.27, u = 0.72\n",
            "Step #135:  T = 0.26, cost = 0.82, new_cost = 2.07, alpha = 0.01, u = 0.12\n",
            "Step #136:  T = 0.26, cost = 0.82, new_cost = 2.55, alpha = 0.00, u = 0.74\n",
            "Step #137:  T = 0.25, cost = 0.82, new_cost = 1.63, alpha = 0.04, u = 0.95\n",
            "Step #138:  T = 0.25, cost = 0.82, new_cost = 1.63, alpha = 0.04, u = 0.86\n",
            "Step #139:  T = 0.25, cost = 0.82, new_cost = 2.73, alpha = 0.00, u = 0.59\n",
            "Step #140:  T = 0.25, cost = 0.82, new_cost = 1.57, alpha = 0.05, u = 0.68\n",
            "Step #141:  T = 0.24, cost = 0.82, new_cost = 0.90, alpha = 0.73, u = 0.15\n",
            "Step #142:  T = 0.24, cost = 0.90, new_cost = 2.07, alpha = 0.01, u = 0.57\n",
            "Step #143:  T = 0.24, cost = 0.90, new_cost = 2.32, alpha = 0.00, u = 0.60\n",
            "Step #144:  T = 0.24, cost = 0.90, new_cost = 0.84, alpha = 1.29, u = 0.12\n",
            "Step #145:  T = 0.24, cost = 0.84, new_cost = 1.51, alpha = 0.06, u = 0.91\n",
            "Step #146:  T = 0.23, cost = 0.84, new_cost = 3.32, alpha = 0.00, u = 0.71\n",
            "Step #147:  T = 0.23, cost = 0.84, new_cost = 1.32, alpha = 0.12, u = 0.34\n",
            "Step #148:  T = 0.23, cost = 0.84, new_cost = 2.70, alpha = 0.00, u = 0.21\n",
            "Step #149:  T = 0.23, cost = 0.84, new_cost = 1.20, alpha = 0.20, u = 0.56\n",
            "Step #150:  T = 0.22, cost = 0.84, new_cost = 0.96, alpha = 0.58, u = 0.67\n",
            "Step #151:  T = 0.22, cost = 0.84, new_cost = 4.81, alpha = 0.00, u = 0.08\n",
            "Step #152:  T = 0.22, cost = 0.84, new_cost = 2.46, alpha = 0.00, u = 0.01\n",
            "Step #153:  T = 0.22, cost = 0.84, new_cost = 1.38, alpha = 0.08, u = 0.12\n",
            "Step #154:  T = 0.21, cost = 0.84, new_cost = 0.81, alpha = 1.13, u = 0.91\n",
            "Step #155:  T = 0.21, cost = 0.81, new_cost = 2.14, alpha = 0.00, u = 0.74\n",
            "Step #156:  T = 0.21, cost = 0.81, new_cost = 1.67, alpha = 0.02, u = 0.34\n",
            "Step #157:  T = 0.21, cost = 0.81, new_cost = 1.30, alpha = 0.09, u = 0.42\n",
            "Step #158:  T = 0.21, cost = 0.81, new_cost = 2.91, alpha = 0.00, u = 0.73\n",
            "Step #159:  T = 0.20, cost = 0.81, new_cost = 2.23, alpha = 0.00, u = 0.71\n",
            "Step #160:  T = 0.20, cost = 0.81, new_cost = 1.04, alpha = 0.32, u = 0.13\n",
            "Step #161:  T = 0.20, cost = 1.04, new_cost = 1.92, alpha = 0.01, u = 0.50\n",
            "Step #162:  T = 0.20, cost = 1.04, new_cost = 2.29, alpha = 0.00, u = 0.46\n",
            "Step #163:  T = 0.20, cost = 1.04, new_cost = 2.08, alpha = 0.01, u = 0.16\n",
            "Step #164:  T = 0.19, cost = 1.04, new_cost = 2.42, alpha = 0.00, u = 0.68\n",
            "Step #165:  T = 0.19, cost = 1.04, new_cost = 0.79, alpha = 3.65, u = 0.22\n",
            "Step #166:  T = 0.19, cost = 0.79, new_cost = 0.85, alpha = 0.73, u = 0.25\n",
            "Step #167:  T = 0.19, cost = 0.85, new_cost = 1.00, alpha = 0.45, u = 0.43\n",
            "Step #168:  T = 0.19, cost = 1.00, new_cost = 1.31, alpha = 0.19, u = 0.10\n",
            "Step #169:  T = 0.18, cost = 1.31, new_cost = 2.02, alpha = 0.02, u = 0.70\n",
            "Step #170:  T = 0.18, cost = 1.31, new_cost = 1.80, alpha = 0.07, u = 0.49\n",
            "Step #171:  T = 0.18, cost = 1.31, new_cost = 1.73, alpha = 0.10, u = 0.42\n",
            "Step #172:  T = 0.18, cost = 1.31, new_cost = 2.14, alpha = 0.01, u = 0.33\n",
            "Step #173:  T = 0.18, cost = 1.31, new_cost = 3.09, alpha = 0.00, u = 0.75\n",
            "Step #174:  T = 0.18, cost = 1.31, new_cost = 1.85, alpha = 0.05, u = 0.21\n",
            "Step #175:  T = 0.17, cost = 1.31, new_cost = 2.16, alpha = 0.01, u = 0.45\n",
            "Step #176:  T = 0.17, cost = 1.31, new_cost = 1.46, alpha = 0.42, u = 0.36\n",
            "Step #177:  T = 0.17, cost = 1.46, new_cost = 1.30, alpha = 2.59, u = 0.31\n",
            "Step #178:  T = 0.17, cost = 1.30, new_cost = 4.00, alpha = 0.00, u = 0.29\n",
            "Step #179:  T = 0.17, cost = 1.30, new_cost = 0.89, alpha = 11.17, u = 0.67\n",
            "Step #180:  T = 0.17, cost = 0.89, new_cost = 1.07, alpha = 0.34, u = 0.11\n",
            "Step #181:  T = 0.16, cost = 1.07, new_cost = 2.01, alpha = 0.00, u = 0.23\n",
            "Step #182:  T = 0.16, cost = 1.07, new_cost = 3.80, alpha = 0.00, u = 0.92\n",
            "Step #183:  T = 0.16, cost = 1.07, new_cost = 1.35, alpha = 0.18, u = 0.51\n",
            "Step #184:  T = 0.16, cost = 1.07, new_cost = 1.31, alpha = 0.23, u = 0.05\n",
            "Step #185:  T = 0.16, cost = 1.31, new_cost = 2.04, alpha = 0.01, u = 0.17\n",
            "Step #186:  T = 0.16, cost = 1.31, new_cost = 0.82, alpha = 23.99, u = 0.13\n",
            "Step #187:  T = 0.15, cost = 0.82, new_cost = 1.46, alpha = 0.02, u = 0.85\n",
            "Step #188:  T = 0.15, cost = 0.82, new_cost = 2.19, alpha = 0.00, u = 0.93\n",
            "Step #189:  T = 0.15, cost = 0.82, new_cost = 1.28, alpha = 0.05, u = 0.80\n",
            "Step #190:  T = 0.15, cost = 0.82, new_cost = 1.20, alpha = 0.08, u = 0.05\n",
            "Step #191:  T = 0.15, cost = 1.20, new_cost = 1.41, alpha = 0.23, u = 0.33\n",
            "Step #192:  T = 0.15, cost = 1.20, new_cost = 0.82, alpha = 12.81, u = 0.22\n",
            "Step #193:  T = 0.15, cost = 0.82, new_cost = 1.03, alpha = 0.24, u = 0.39\n",
            "Step #194:  T = 0.14, cost = 0.82, new_cost = 0.69, alpha = 2.51, u = 0.10\n",
            "Step #195:  T = 0.14, cost = 0.69, new_cost = 1.20, alpha = 0.03, u = 0.73\n",
            "Step #196:  T = 0.14, cost = 0.69, new_cost = 1.33, alpha = 0.01, u = 0.81\n",
            "Step #197:  T = 0.14, cost = 0.69, new_cost = 2.92, alpha = 0.00, u = 0.72\n",
            "Step #198:  T = 0.14, cost = 0.69, new_cost = 0.81, alpha = 0.41, u = 0.34\n",
            "Step #199:  T = 0.14, cost = 0.81, new_cost = 1.15, alpha = 0.09, u = 0.59\n",
            "Step #200:  T = 0.14, cost = 0.81, new_cost = 1.31, alpha = 0.03, u = 0.74\n",
            "Step #201:  T = 0.13, cost = 0.81, new_cost = 1.25, alpha = 0.04, u = 0.16\n",
            "Step #202:  T = 0.13, cost = 0.81, new_cost = 1.89, alpha = 0.00, u = 0.24\n",
            "Step #203:  T = 0.13, cost = 0.81, new_cost = 1.26, alpha = 0.03, u = 0.83\n",
            "Step #204:  T = 0.13, cost = 0.81, new_cost = 2.20, alpha = 0.00, u = 0.19\n",
            "Step #205:  T = 0.13, cost = 0.81, new_cost = 1.59, alpha = 0.00, u = 0.35\n",
            "Step #206:  T = 0.13, cost = 0.81, new_cost = 1.35, alpha = 0.01, u = 0.19\n",
            "Step #207:  T = 0.13, cost = 0.81, new_cost = 2.04, alpha = 0.00, u = 0.39\n",
            "Step #208:  T = 0.12, cost = 0.81, new_cost = 0.99, alpha = 0.24, u = 0.17\n",
            "Step #209:  T = 0.12, cost = 0.99, new_cost = 1.66, alpha = 0.00, u = 0.44\n",
            "Step #210:  T = 0.12, cost = 0.99, new_cost = 1.92, alpha = 0.00, u = 0.84\n",
            "Step #211:  T = 0.12, cost = 0.99, new_cost = 2.33, alpha = 0.00, u = 0.15\n",
            "Step #212:  T = 0.12, cost = 0.99, new_cost = 0.82, alpha = 4.20, u = 0.13\n",
            "Step #213:  T = 0.12, cost = 0.82, new_cost = 1.51, alpha = 0.00, u = 0.65\n",
            "Step #214:  T = 0.12, cost = 0.82, new_cost = 2.57, alpha = 0.00, u = 0.96\n",
            "Step #215:  T = 0.12, cost = 0.82, new_cost = 0.79, alpha = 1.30, u = 0.92\n",
            "Step #216:  T = 0.12, cost = 0.79, new_cost = 1.15, alpha = 0.04, u = 0.75\n",
            "Step #217:  T = 0.11, cost = 0.79, new_cost = 1.50, alpha = 0.00, u = 0.65\n",
            "Step #218:  T = 0.11, cost = 0.79, new_cost = 1.49, alpha = 0.00, u = 0.88\n",
            "Step #219:  T = 0.11, cost = 0.79, new_cost = 0.98, alpha = 0.18, u = 0.40\n",
            "Step #220:  T = 0.11, cost = 0.79, new_cost = 1.02, alpha = 0.12, u = 0.57\n",
            "Step #221:  T = 0.11, cost = 0.79, new_cost = 1.14, alpha = 0.04, u = 0.71\n",
            "Step #222:  T = 0.11, cost = 0.79, new_cost = 1.52, alpha = 0.00, u = 0.57\n",
            "Step #223:  T = 0.11, cost = 0.79, new_cost = 1.08, alpha = 0.07, u = 0.17\n",
            "Step #224:  T = 0.11, cost = 0.79, new_cost = 1.62, alpha = 0.00, u = 0.26\n",
            "Step #225:  T = 0.11, cost = 0.79, new_cost = 0.99, alpha = 0.15, u = 0.38\n",
            "Step #226:  T = 0.10, cost = 0.79, new_cost = 1.80, alpha = 0.00, u = 0.17\n",
            "Step #227:  T = 0.10, cost = 0.79, new_cost = 1.27, alpha = 0.01, u = 0.54\n",
            "Step #228:  T = 0.10, cost = 0.79, new_cost = 1.65, alpha = 0.00, u = 0.32\n",
            "Step #229:  T = 0.10, cost = 0.79, new_cost = 1.19, alpha = 0.02, u = 0.57\n",
            "Step #230:  T = 0.10, cost = 0.79, new_cost = 0.92, alpha = 0.28, u = 0.70\n",
            "Step #231:  T = 0.10, cost = 0.79, new_cost = 1.87, alpha = 0.00, u = 0.68\n",
            "Step #232:  T = 0.10, cost = 0.79, new_cost = 1.75, alpha = 0.00, u = 0.04\n",
            "Step #233:  T = 0.10, cost = 0.79, new_cost = 2.77, alpha = 0.00, u = 0.49\n",
            "Step #234:  T = 0.10, cost = 0.79, new_cost = 1.48, alpha = 0.00, u = 0.25\n",
            "Step #235:  T = 0.10, cost = 0.79, new_cost = 1.78, alpha = 0.00, u = 0.79\n",
            "Step #236:  T = 0.09, cost = 0.79, new_cost = 1.05, alpha = 0.06, u = 0.65\n",
            "Step #237:  T = 0.09, cost = 0.79, new_cost = 0.91, alpha = 0.25, u = 0.06\n",
            "Step #238:  T = 0.09, cost = 0.91, new_cost = 1.49, alpha = 0.00, u = 0.21\n",
            "Step #239:  T = 0.09, cost = 0.91, new_cost = 1.91, alpha = 0.00, u = 0.86\n",
            "Step #240:  T = 0.09, cost = 0.91, new_cost = 2.20, alpha = 0.00, u = 0.59\n",
            "Step #241:  T = 0.09, cost = 0.91, new_cost = 2.74, alpha = 0.00, u = 0.45\n",
            "Step #242:  T = 0.09, cost = 0.91, new_cost = 3.03, alpha = 0.00, u = 0.28\n",
            "Step #243:  T = 0.09, cost = 0.91, new_cost = 2.57, alpha = 0.00, u = 0.44\n",
            "Step #244:  T = 0.09, cost = 0.91, new_cost = 1.62, alpha = 0.00, u = 0.85\n",
            "Step #245:  T = 0.09, cost = 0.91, new_cost = 2.20, alpha = 0.00, u = 0.48\n",
            "Step #246:  T = 0.09, cost = 0.91, new_cost = 1.82, alpha = 0.00, u = 0.46\n",
            "Step #247:  T = 0.08, cost = 0.91, new_cost = 1.17, alpha = 0.05, u = 0.79\n",
            "Step #248:  T = 0.08, cost = 0.91, new_cost = 2.04, alpha = 0.00, u = 0.96\n",
            "Step #249:  T = 0.08, cost = 0.91, new_cost = 3.20, alpha = 0.00, u = 0.36\n",
            "Step #250:  T = 0.08, cost = 0.91, new_cost = 1.54, alpha = 0.00, u = 0.38\n",
            "Step #251:  T = 0.08, cost = 0.91, new_cost = 1.05, alpha = 0.19, u = 0.33\n",
            "Step #252:  T = 0.08, cost = 0.91, new_cost = 1.09, alpha = 0.11, u = 0.18\n",
            "Step #253:  T = 0.08, cost = 0.91, new_cost = 1.27, alpha = 0.01, u = 0.24\n",
            "Step #254:  T = 0.08, cost = 0.91, new_cost = 2.65, alpha = 0.00, u = 0.05\n",
            "Step #255:  T = 0.08, cost = 0.91, new_cost = 0.83, alpha = 3.04, u = 0.41\n",
            "Step #256:  T = 0.08, cost = 0.83, new_cost = 2.16, alpha = 0.00, u = 0.23\n",
            "Step #257:  T = 0.08, cost = 0.83, new_cost = 0.94, alpha = 0.22, u = 0.82\n",
            "Step #258:  T = 0.08, cost = 0.83, new_cost = 1.06, alpha = 0.05, u = 0.91\n",
            "Step #259:  T = 0.07, cost = 0.83, new_cost = 0.88, alpha = 0.47, u = 0.13\n",
            "Step #260:  T = 0.07, cost = 0.88, new_cost = 2.30, alpha = 0.00, u = 0.37\n",
            "Step #261:  T = 0.07, cost = 0.88, new_cost = 1.15, alpha = 0.03, u = 0.75\n",
            "Step #262:  T = 0.07, cost = 0.88, new_cost = 2.42, alpha = 0.00, u = 0.03\n",
            "Step #263:  T = 0.07, cost = 0.88, new_cost = 1.51, alpha = 0.00, u = 0.81\n",
            "Step #264:  T = 0.07, cost = 0.88, new_cost = 1.32, alpha = 0.00, u = 0.46\n",
            "Step #265:  T = 0.07, cost = 0.88, new_cost = 1.28, alpha = 0.00, u = 0.92\n",
            "Step #266:  T = 0.07, cost = 0.88, new_cost = 1.30, alpha = 0.00, u = 0.32\n",
            "Step #267:  T = 0.07, cost = 0.88, new_cost = 2.00, alpha = 0.00, u = 0.78\n",
            "Step #268:  T = 0.07, cost = 0.88, new_cost = 1.53, alpha = 0.00, u = 0.86\n",
            "Step #269:  T = 0.07, cost = 0.88, new_cost = 0.81, alpha = 2.83, u = 0.06\n",
            "Step #270:  T = 0.07, cost = 0.81, new_cost = 1.34, alpha = 0.00, u = 0.82\n",
            "Step #271:  T = 0.07, cost = 0.81, new_cost = 1.10, alpha = 0.01, u = 0.66\n",
            "Step #272:  T = 0.07, cost = 0.81, new_cost = 0.93, alpha = 0.16, u = 0.73\n",
            "Step #273:  T = 0.06, cost = 0.81, new_cost = 1.33, alpha = 0.00, u = 0.81\n",
            "Step #274:  T = 0.06, cost = 0.81, new_cost = 1.09, alpha = 0.01, u = 0.08\n",
            "Step #275:  T = 0.06, cost = 0.81, new_cost = 1.07, alpha = 0.02, u = 0.94\n",
            "Step #276:  T = 0.06, cost = 0.81, new_cost = 1.36, alpha = 0.00, u = 0.83\n",
            "Step #277:  T = 0.06, cost = 0.81, new_cost = 0.79, alpha = 1.46, u = 0.15\n",
            "Step #278:  T = 0.06, cost = 0.79, new_cost = 1.88, alpha = 0.00, u = 0.84\n",
            "Step #279:  T = 0.06, cost = 0.79, new_cost = 1.53, alpha = 0.00, u = 0.75\n",
            "Step #280:  T = 0.06, cost = 0.79, new_cost = 1.24, alpha = 0.00, u = 0.40\n",
            "Step #281:  T = 0.06, cost = 0.79, new_cost = 1.46, alpha = 0.00, u = 0.79\n",
            "Step #282:  T = 0.06, cost = 0.79, new_cost = 2.91, alpha = 0.00, u = 0.32\n",
            "Step #283:  T = 0.06, cost = 0.79, new_cost = 1.73, alpha = 0.00, u = 0.23\n",
            "Step #284:  T = 0.06, cost = 0.79, new_cost = 4.20, alpha = 0.00, u = 0.07\n",
            "Step #285:  T = 0.06, cost = 0.79, new_cost = 0.82, alpha = 0.62, u = 0.36\n",
            "Step #286:  T = 0.06, cost = 0.82, new_cost = 1.02, alpha = 0.03, u = 0.03\n",
            "Step #287:  T = 0.06, cost = 1.02, new_cost = 1.13, alpha = 0.14, u = 0.03\n",
            "Step #288:  T = 0.06, cost = 1.13, new_cost = 1.48, alpha = 0.00, u = 0.84\n",
            "Step #289:  T = 0.06, cost = 1.13, new_cost = 1.00, alpha = 10.06, u = 0.94\n",
            "Step #290:  T = 0.05, cost = 1.00, new_cost = 1.09, alpha = 0.19, u = 0.44\n",
            "Step #291:  T = 0.05, cost = 1.00, new_cost = 1.35, alpha = 0.00, u = 0.80\n",
            "Step #292:  T = 0.05, cost = 1.00, new_cost = 0.98, alpha = 1.64, u = 0.07\n",
            "Step #293:  T = 0.05, cost = 0.98, new_cost = 2.49, alpha = 0.00, u = 0.47\n",
            "Step #294:  T = 0.05, cost = 0.98, new_cost = 2.16, alpha = 0.00, u = 0.29\n",
            "Step #295:  T = 0.05, cost = 0.98, new_cost = 1.70, alpha = 0.00, u = 0.45\n",
            "Step #296:  T = 0.05, cost = 0.98, new_cost = 1.07, alpha = 0.16, u = 0.87\n",
            "Step #297:  T = 0.05, cost = 0.98, new_cost = 1.45, alpha = 0.00, u = 0.89\n",
            "Step #298:  T = 0.05, cost = 0.98, new_cost = 0.88, alpha = 6.41, u = 0.62\n",
            "Step #299:  T = 0.05, cost = 0.88, new_cost = 1.02, alpha = 0.07, u = 0.06\n",
            "Step #300:  T = 0.05, cost = 1.02, new_cost = 0.89, alpha = 12.21, u = 0.58\n",
            "Step #301:  T = 0.05, cost = 0.89, new_cost = 0.91, alpha = 0.66, u = 0.62\n",
            "Step #302:  T = 0.05, cost = 0.91, new_cost = 1.55, alpha = 0.00, u = 0.19\n",
            "Step #303:  T = 0.05, cost = 0.91, new_cost = 2.04, alpha = 0.00, u = 0.38\n",
            "Step #304:  T = 0.05, cost = 0.91, new_cost = 1.07, alpha = 0.04, u = 0.10\n",
            "Step #305:  T = 0.05, cost = 0.91, new_cost = 0.88, alpha = 2.06, u = 0.79\n",
            "Step #306:  T = 0.05, cost = 0.88, new_cost = 1.45, alpha = 0.00, u = 0.94\n",
            "Step #307:  T = 0.05, cost = 0.88, new_cost = 2.25, alpha = 0.00, u = 0.73\n",
            "Step #308:  T = 0.05, cost = 0.88, new_cost = 1.28, alpha = 0.00, u = 0.46\n",
            "Step #309:  T = 0.05, cost = 0.88, new_cost = 1.33, alpha = 0.00, u = 0.08\n",
            "Step #310:  T = 0.04, cost = 0.88, new_cost = 1.35, alpha = 0.00, u = 0.42\n",
            "Step #311:  T = 0.04, cost = 0.88, new_cost = 2.40, alpha = 0.00, u = 0.15\n",
            "Step #312:  T = 0.04, cost = 0.88, new_cost = 1.89, alpha = 0.00, u = 0.94\n",
            "Step #313:  T = 0.04, cost = 0.88, new_cost = 1.60, alpha = 0.00, u = 0.15\n",
            "Step #314:  T = 0.04, cost = 0.88, new_cost = 2.06, alpha = 0.00, u = 0.36\n",
            "Step #315:  T = 0.04, cost = 0.88, new_cost = 1.24, alpha = 0.00, u = 0.27\n",
            "Step #316:  T = 0.04, cost = 0.88, new_cost = 1.69, alpha = 0.00, u = 0.28\n",
            "Step #317:  T = 0.04, cost = 0.88, new_cost = 1.57, alpha = 0.00, u = 0.04\n",
            "Step #318:  T = 0.04, cost = 0.88, new_cost = 1.35, alpha = 0.00, u = 0.14\n",
            "Step #319:  T = 0.04, cost = 0.88, new_cost = 3.87, alpha = 0.00, u = 0.80\n",
            "Step #320:  T = 0.04, cost = 0.88, new_cost = 1.79, alpha = 0.00, u = 0.62\n",
            "Step #321:  T = 0.04, cost = 0.88, new_cost = 1.39, alpha = 0.00, u = 0.82\n",
            "Step #322:  T = 0.04, cost = 0.88, new_cost = 0.91, alpha = 0.40, u = 0.64\n",
            "Step #323:  T = 0.04, cost = 0.88, new_cost = 1.11, alpha = 0.00, u = 0.82\n",
            "Step #324:  T = 0.04, cost = 0.88, new_cost = 4.19, alpha = 0.00, u = 0.66\n",
            "Step #325:  T = 0.04, cost = 0.88, new_cost = 1.34, alpha = 0.00, u = 0.33\n",
            "Step #326:  T = 0.04, cost = 0.88, new_cost = 3.15, alpha = 0.00, u = 0.56\n",
            "Step #327:  T = 0.04, cost = 0.88, new_cost = 2.53, alpha = 0.00, u = 0.73\n",
            "Step #328:  T = 0.04, cost = 0.88, new_cost = 1.40, alpha = 0.00, u = 0.31\n",
            "Step #329:  T = 0.04, cost = 0.88, new_cost = 1.31, alpha = 0.00, u = 0.65\n",
            "Step #330:  T = 0.04, cost = 0.88, new_cost = 1.59, alpha = 0.00, u = 0.52\n",
            "Step #331:  T = 0.04, cost = 0.88, new_cost = 3.11, alpha = 0.00, u = 0.51\n",
            "Step #332:  T = 0.04, cost = 0.88, new_cost = 1.55, alpha = 0.00, u = 0.60\n",
            "Step #333:  T = 0.04, cost = 0.88, new_cost = 1.44, alpha = 0.00, u = 0.75\n",
            "Step #334:  T = 0.04, cost = 0.88, new_cost = 1.02, alpha = 0.02, u = 0.79\n",
            "Step #335:  T = 0.03, cost = 0.88, new_cost = 2.81, alpha = 0.00, u = 0.11\n",
            "Step #336:  T = 0.03, cost = 0.88, new_cost = 1.53, alpha = 0.00, u = 0.60\n",
            "Step #337:  T = 0.03, cost = 0.88, new_cost = 1.18, alpha = 0.00, u = 0.98\n",
            "Step #338:  T = 0.03, cost = 0.88, new_cost = 1.01, alpha = 0.02, u = 0.44\n",
            "Step #339:  T = 0.03, cost = 0.88, new_cost = 1.62, alpha = 0.00, u = 0.55\n",
            "Step #340:  T = 0.03, cost = 0.88, new_cost = 0.79, alpha = 14.30, u = 0.96\n",
            "Step #341:  T = 0.03, cost = 0.79, new_cost = 1.15, alpha = 0.00, u = 0.26\n",
            "Step #342:  T = 0.03, cost = 0.79, new_cost = 0.94, alpha = 0.01, u = 0.61\n",
            "Step #343:  T = 0.03, cost = 0.79, new_cost = 0.89, alpha = 0.04, u = 0.70\n",
            "Step #344:  T = 0.03, cost = 0.79, new_cost = 1.13, alpha = 0.00, u = 0.81\n",
            "Step #345:  T = 0.03, cost = 0.79, new_cost = 0.85, alpha = 0.15, u = 0.62\n",
            "Step #346:  T = 0.03, cost = 0.79, new_cost = 2.17, alpha = 0.00, u = 0.21\n",
            "Step #347:  T = 0.03, cost = 0.79, new_cost = 2.10, alpha = 0.00, u = 0.68\n",
            "Step #348:  T = 0.03, cost = 0.79, new_cost = 1.02, alpha = 0.00, u = 0.98\n",
            "Step #349:  T = 0.03, cost = 0.79, new_cost = 2.95, alpha = 0.00, u = 0.38\n",
            "Step #350:  T = 0.03, cost = 0.79, new_cost = 1.36, alpha = 0.00, u = 0.01\n",
            "Step #351:  T = 0.03, cost = 0.79, new_cost = 0.98, alpha = 0.00, u = 0.19\n",
            "Step #352:  T = 0.03, cost = 0.79, new_cost = 1.35, alpha = 0.00, u = 0.34\n",
            "Step #353:  T = 0.03, cost = 0.79, new_cost = 4.23, alpha = 0.00, u = 0.16\n",
            "Step #354:  T = 0.03, cost = 0.79, new_cost = 1.14, alpha = 0.00, u = 0.17\n",
            "Step #355:  T = 0.03, cost = 0.79, new_cost = 1.40, alpha = 0.00, u = 0.53\n",
            "Step #356:  T = 0.03, cost = 0.79, new_cost = 1.00, alpha = 0.00, u = 0.48\n",
            "Step #357:  T = 0.03, cost = 0.79, new_cost = 2.91, alpha = 0.00, u = 0.55\n",
            "Step #358:  T = 0.03, cost = 0.79, new_cost = 0.74, alpha = 7.17, u = 0.32\n",
            "Step #359:  T = 0.03, cost = 0.74, new_cost = 1.68, alpha = 0.00, u = 0.16\n",
            "Step #360:  T = 0.03, cost = 0.74, new_cost = 2.72, alpha = 0.00, u = 0.40\n",
            "Step #361:  T = 0.03, cost = 0.74, new_cost = 1.39, alpha = 0.00, u = 0.97\n",
            "Step #362:  T = 0.03, cost = 0.74, new_cost = 2.51, alpha = 0.00, u = 0.13\n",
            "Step #363:  T = 0.03, cost = 0.74, new_cost = 1.09, alpha = 0.00, u = 0.04\n",
            "Step #364:  T = 0.03, cost = 0.74, new_cost = 1.23, alpha = 0.00, u = 0.15\n",
            "Step #365:  T = 0.03, cost = 0.74, new_cost = 4.04, alpha = 0.00, u = 0.14\n",
            "Step #366:  T = 0.03, cost = 0.74, new_cost = 1.14, alpha = 0.00, u = 0.61\n",
            "Step #367:  T = 0.03, cost = 0.74, new_cost = 1.56, alpha = 0.00, u = 0.07\n",
            "Step #368:  T = 0.03, cost = 0.74, new_cost = 2.19, alpha = 0.00, u = 0.65\n",
            "Step #369:  T = 0.02, cost = 0.74, new_cost = 1.58, alpha = 0.00, u = 0.24\n",
            "Step #370:  T = 0.02, cost = 0.74, new_cost = 1.64, alpha = 0.00, u = 0.09\n",
            "Step #371:  T = 0.02, cost = 0.74, new_cost = 1.52, alpha = 0.00, u = 0.10\n",
            "Step #372:  T = 0.02, cost = 0.74, new_cost = 2.06, alpha = 0.00, u = 0.03\n",
            "Step #373:  T = 0.02, cost = 0.74, new_cost = 3.15, alpha = 0.00, u = 0.95\n",
            "Step #374:  T = 0.02, cost = 0.74, new_cost = 1.56, alpha = 0.00, u = 0.54\n",
            "Step #375:  T = 0.02, cost = 0.74, new_cost = 1.74, alpha = 0.00, u = 0.28\n",
            "Step #376:  T = 0.02, cost = 0.74, new_cost = 1.80, alpha = 0.00, u = 0.47\n",
            "Step #377:  T = 0.02, cost = 0.74, new_cost = 0.80, alpha = 0.05, u = 0.10\n",
            "Step #378:  T = 0.02, cost = 0.74, new_cost = 1.60, alpha = 0.00, u = 0.13\n",
            "Step #379:  T = 0.02, cost = 0.74, new_cost = 2.31, alpha = 0.00, u = 0.25\n",
            "Step #380:  T = 0.02, cost = 0.74, new_cost = 1.52, alpha = 0.00, u = 0.15\n",
            "Step #381:  T = 0.02, cost = 0.74, new_cost = 1.11, alpha = 0.00, u = 0.67\n",
            "Step #382:  T = 0.02, cost = 0.74, new_cost = 0.95, alpha = 0.00, u = 0.77\n",
            "Step #383:  T = 0.02, cost = 0.74, new_cost = 1.64, alpha = 0.00, u = 0.87\n",
            "Step #384:  T = 0.02, cost = 0.74, new_cost = 4.80, alpha = 0.00, u = 0.12\n",
            "Step #385:  T = 0.02, cost = 0.74, new_cost = 0.91, alpha = 0.00, u = 0.96\n",
            "Step #386:  T = 0.02, cost = 0.74, new_cost = 1.85, alpha = 0.00, u = 0.00\n",
            "Step #387:  T = 0.02, cost = 0.74, new_cost = 0.76, alpha = 0.31, u = 0.19\n",
            "Step #388:  T = 0.02, cost = 0.76, new_cost = 1.03, alpha = 0.00, u = 0.23\n",
            "Step #389:  T = 0.02, cost = 0.76, new_cost = 1.50, alpha = 0.00, u = 0.08\n",
            "Step #390:  T = 0.02, cost = 0.76, new_cost = 1.80, alpha = 0.00, u = 0.90\n",
            "Step #391:  T = 0.02, cost = 0.76, new_cost = 1.58, alpha = 0.00, u = 0.40\n",
            "Step #392:  T = 0.02, cost = 0.76, new_cost = 2.06, alpha = 0.00, u = 0.49\n",
            "Step #393:  T = 0.02, cost = 0.76, new_cost = 1.15, alpha = 0.00, u = 0.25\n",
            "Step #394:  T = 0.02, cost = 0.76, new_cost = 0.86, alpha = 0.00, u = 0.65\n",
            "Step #395:  T = 0.02, cost = 0.76, new_cost = 1.06, alpha = 0.00, u = 0.01\n",
            "Step #396:  T = 0.02, cost = 0.76, new_cost = 1.02, alpha = 0.00, u = 0.46\n",
            "Step #397:  T = 0.02, cost = 0.76, new_cost = 1.26, alpha = 0.00, u = 0.46\n",
            "Step #398:  T = 0.02, cost = 0.76, new_cost = 1.62, alpha = 0.00, u = 0.25\n",
            "Step #399:  T = 0.02, cost = 0.76, new_cost = 4.64, alpha = 0.00, u = 0.99\n",
            "Step #400:  T = 0.02, cost = 0.76, new_cost = 1.18, alpha = 0.00, u = 0.27\n",
            "Step #401:  T = 0.02, cost = 0.76, new_cost = 0.88, alpha = 0.00, u = 0.03\n",
            "Step #402:  T = 0.02, cost = 0.76, new_cost = 1.23, alpha = 0.00, u = 0.74\n",
            "Step #403:  T = 0.02, cost = 0.76, new_cost = 1.12, alpha = 0.00, u = 0.30\n",
            "Step #404:  T = 0.02, cost = 0.76, new_cost = 1.08, alpha = 0.00, u = 0.93\n",
            "Step #405:  T = 0.02, cost = 0.76, new_cost = 1.12, alpha = 0.00, u = 0.87\n",
            "Step #406:  T = 0.02, cost = 0.76, new_cost = 1.10, alpha = 0.00, u = 0.57\n",
            "Step #407:  T = 0.02, cost = 0.76, new_cost = 1.82, alpha = 0.00, u = 0.93\n",
            "Step #408:  T = 0.02, cost = 0.76, new_cost = 2.91, alpha = 0.00, u = 0.29\n",
            "Step #409:  T = 0.02, cost = 0.76, new_cost = 0.97, alpha = 0.00, u = 0.12\n",
            "Step #410:  T = 0.02, cost = 0.76, new_cost = 0.79, alpha = 0.13, u = 0.90\n",
            "Step #411:  T = 0.02, cost = 0.76, new_cost = 1.07, alpha = 0.00, u = 0.16\n",
            "Step #412:  T = 0.02, cost = 0.76, new_cost = 1.69, alpha = 0.00, u = 0.65\n",
            "Step #413:  T = 0.02, cost = 0.76, new_cost = 1.25, alpha = 0.00, u = 0.25\n",
            "Step #414:  T = 0.02, cost = 0.76, new_cost = 1.37, alpha = 0.00, u = 0.74\n",
            "Step #415:  T = 0.02, cost = 0.76, new_cost = 2.21, alpha = 0.00, u = 0.09\n",
            "Step #416:  T = 0.02, cost = 0.76, new_cost = 1.14, alpha = 0.00, u = 0.38\n",
            "Step #417:  T = 0.02, cost = 0.76, new_cost = 0.81, alpha = 0.05, u = 0.07\n",
            "Step #418:  T = 0.02, cost = 0.76, new_cost = 2.30, alpha = 0.00, u = 0.46\n",
            "Step #419:  T = 0.01, cost = 0.76, new_cost = 1.95, alpha = 0.00, u = 0.38\n",
            "Step #420:  T = 0.01, cost = 0.76, new_cost = 1.09, alpha = 0.00, u = 0.40\n",
            "Step #421:  T = 0.01, cost = 0.76, new_cost = 1.23, alpha = 0.00, u = 0.75\n",
            "Step #422:  T = 0.01, cost = 0.76, new_cost = 1.10, alpha = 0.00, u = 0.66\n",
            "Step #423:  T = 0.01, cost = 0.76, new_cost = 0.90, alpha = 0.00, u = 0.32\n",
            "Step #424:  T = 0.01, cost = 0.76, new_cost = 1.00, alpha = 0.00, u = 0.98\n",
            "Step #425:  T = 0.01, cost = 0.76, new_cost = 1.70, alpha = 0.00, u = 0.42\n",
            "Step #426:  T = 0.01, cost = 0.76, new_cost = 1.76, alpha = 0.00, u = 0.42\n",
            "Step #427:  T = 0.01, cost = 0.76, new_cost = 1.56, alpha = 0.00, u = 0.67\n",
            "Step #428:  T = 0.01, cost = 0.76, new_cost = 1.19, alpha = 0.00, u = 0.11\n",
            "Step #429:  T = 0.01, cost = 0.76, new_cost = 1.37, alpha = 0.00, u = 0.21\n",
            "Step #430:  T = 0.01, cost = 0.76, new_cost = 1.07, alpha = 0.00, u = 0.95\n",
            "Step #431:  T = 0.01, cost = 0.76, new_cost = 1.43, alpha = 0.00, u = 0.78\n",
            "Step #432:  T = 0.01, cost = 0.76, new_cost = 1.28, alpha = 0.00, u = 0.50\n",
            "Step #433:  T = 0.01, cost = 0.76, new_cost = 3.46, alpha = 0.00, u = 0.46\n",
            "Step #434:  T = 0.01, cost = 0.76, new_cost = 1.57, alpha = 0.00, u = 0.40\n",
            "Step #435:  T = 0.01, cost = 0.76, new_cost = 1.39, alpha = 0.00, u = 0.29\n",
            "Step #436:  T = 0.01, cost = 0.76, new_cost = 2.06, alpha = 0.00, u = 0.75\n",
            "Step #437:  T = 0.01, cost = 0.76, new_cost = 2.40, alpha = 0.00, u = 0.09\n",
            "Step #438:  T = 0.01, cost = 0.76, new_cost = 1.57, alpha = 0.00, u = 0.34\n",
            "Step #439:  T = 0.01, cost = 0.76, new_cost = 1.77, alpha = 0.00, u = 0.96\n",
            "Step #440:  T = 0.01, cost = 0.76, new_cost = 1.29, alpha = 0.00, u = 0.05\n",
            "Step #441:  T = 0.01, cost = 0.76, new_cost = 2.06, alpha = 0.00, u = 0.99\n",
            "Step #442:  T = 0.01, cost = 0.76, new_cost = 1.61, alpha = 0.00, u = 0.75\n",
            "Step #443:  T = 0.01, cost = 0.76, new_cost = 3.15, alpha = 0.00, u = 0.44\n",
            "Step #444:  T = 0.01, cost = 0.76, new_cost = 1.40, alpha = 0.00, u = 0.19\n",
            "Step #445:  T = 0.01, cost = 0.76, new_cost = 2.95, alpha = 0.00, u = 0.72\n",
            "Step #446:  T = 0.01, cost = 0.76, new_cost = 0.74, alpha = 7.06, u = 0.50\n",
            "Step #447:  T = 0.01, cost = 0.74, new_cost = 1.08, alpha = 0.00, u = 0.42\n",
            "Step #448:  T = 0.01, cost = 0.74, new_cost = 1.04, alpha = 0.00, u = 0.10\n",
            "Step #449:  T = 0.01, cost = 0.74, new_cost = 1.12, alpha = 0.00, u = 0.64\n",
            "Step #450:  T = 0.01, cost = 0.74, new_cost = 1.29, alpha = 0.00, u = 0.42\n",
            "Step #451:  T = 0.01, cost = 0.74, new_cost = 0.84, alpha = 0.00, u = 0.62\n",
            "Step #452:  T = 0.01, cost = 0.74, new_cost = 1.94, alpha = 0.00, u = 0.90\n",
            "Step #453:  T = 0.01, cost = 0.74, new_cost = 1.58, alpha = 0.00, u = 0.71\n",
            "Step #454:  T = 0.01, cost = 0.74, new_cost = 1.09, alpha = 0.00, u = 0.19\n",
            "Step #455:  T = 0.01, cost = 0.74, new_cost = 0.69, alpha = 69.50, u = 0.40\n",
            "Step #456:  T = 0.01, cost = 0.69, new_cost = 1.08, alpha = 0.00, u = 0.04\n",
            "Step #457:  T = 0.01, cost = 0.69, new_cost = 1.72, alpha = 0.00, u = 0.96\n",
            "Step #458:  T = 0.01, cost = 0.69, new_cost = 1.42, alpha = 0.00, u = 0.05\n",
            "Step #459:  T = 0.01, cost = 0.69, new_cost = 1.09, alpha = 0.00, u = 0.41\n",
            "Step #460:  T = 0.01, cost = 0.69, new_cost = 1.01, alpha = 0.00, u = 0.62\n",
            "Step #461:  T = 0.01, cost = 0.69, new_cost = 3.25, alpha = 0.00, u = 0.13\n",
            "Step #462:  T = 0.01, cost = 0.69, new_cost = 1.09, alpha = 0.00, u = 0.45\n",
            "Step #463:  T = 0.01, cost = 0.69, new_cost = 1.63, alpha = 0.00, u = 0.84\n",
            "Step #464:  T = 0.01, cost = 0.69, new_cost = 1.22, alpha = 0.00, u = 0.15\n",
            "Step #465:  T = 0.01, cost = 0.69, new_cost = 1.61, alpha = 0.00, u = 0.92\n",
            "Step #466:  T = 0.01, cost = 0.69, new_cost = 1.12, alpha = 0.00, u = 0.27\n",
            "Step #467:  T = 0.01, cost = 0.69, new_cost = 1.95, alpha = 0.00, u = 0.84\n",
            "Step #468:  T = 0.01, cost = 0.69, new_cost = 2.46, alpha = 0.00, u = 0.76\n",
            "Step #469:  T = 0.01, cost = 0.69, new_cost = 1.58, alpha = 0.00, u = 0.50\n",
            "Step #470:  T = 0.01, cost = 0.69, new_cost = 1.29, alpha = 0.00, u = 0.80\n",
            "Step #471:  T = 0.01, cost = 0.69, new_cost = 1.15, alpha = 0.00, u = 0.24\n",
            "Step #472:  T = 0.01, cost = 0.69, new_cost = 1.52, alpha = 0.00, u = 0.97\n",
            "Step #473:  T = 0.01, cost = 0.69, new_cost = 1.67, alpha = 0.00, u = 0.46\n",
            "Step #474:  T = 0.01, cost = 0.69, new_cost = 2.29, alpha = 0.00, u = 0.58\n",
            "Step #475:  T = 0.01, cost = 0.69, new_cost = 2.36, alpha = 0.00, u = 0.22\n",
            "Step #476:  T = 0.01, cost = 0.69, new_cost = 1.74, alpha = 0.00, u = 0.94\n",
            "Step #477:  T = 0.01, cost = 0.69, new_cost = 4.49, alpha = 0.00, u = 0.91\n",
            "Step #478:  T = 0.01, cost = 0.69, new_cost = 1.27, alpha = 0.00, u = 0.52\n",
            "Step #479:  T = 0.01, cost = 0.69, new_cost = 2.00, alpha = 0.00, u = 0.32\n",
            "Step #480:  T = 0.01, cost = 0.69, new_cost = 3.58, alpha = 0.00, u = 0.12\n",
            "Step #481:  T = 0.01, cost = 0.69, new_cost = 0.84, alpha = 0.00, u = 0.35\n",
            "Step #482:  T = 0.01, cost = 0.69, new_cost = 1.25, alpha = 0.00, u = 0.86\n",
            "Step #483:  T = 0.01, cost = 0.69, new_cost = 1.68, alpha = 0.00, u = 0.79\n",
            "Step #484:  T = 0.01, cost = 0.69, new_cost = 0.79, alpha = 0.00, u = 0.44\n",
            "Step #485:  T = 0.01, cost = 0.69, new_cost = 1.21, alpha = 0.00, u = 0.76\n",
            "Step #486:  T = 0.01, cost = 0.69, new_cost = 1.11, alpha = 0.00, u = 0.33\n",
            "Step #487:  T = 0.01, cost = 0.69, new_cost = 1.99, alpha = 0.00, u = 0.51\n",
            "Step #488:  T = 0.01, cost = 0.69, new_cost = 2.16, alpha = 0.00, u = 0.28\n",
            "Step #489:  T = 0.01, cost = 0.69, new_cost = 1.74, alpha = 0.00, u = 0.27\n",
            "Step #490:  T = 0.01, cost = 0.69, new_cost = 1.05, alpha = 0.00, u = 0.75\n",
            "Step #491:  T = 0.01, cost = 0.69, new_cost = 1.97, alpha = 0.00, u = 0.88\n",
            "Step #492:  T = 0.01, cost = 0.69, new_cost = 2.64, alpha = 0.00, u = 0.79\n",
            "Step #493:  T = 0.01, cost = 0.69, new_cost = 1.64, alpha = 0.00, u = 0.96\n",
            "Step #494:  T = 0.01, cost = 0.69, new_cost = 2.37, alpha = 0.00, u = 1.00\n",
            "Step #495:  T = 0.01, cost = 0.69, new_cost = 3.01, alpha = 0.00, u = 0.35\n",
            "Step #496:  T = 0.01, cost = 0.69, new_cost = 0.94, alpha = 0.00, u = 0.43\n",
            "Step #497:  T = 0.01, cost = 0.69, new_cost = 1.34, alpha = 0.00, u = 0.07\n",
            "Step #498:  T = 0.01, cost = 0.69, new_cost = 1.66, alpha = 0.00, u = 0.14\n",
            "Step #499:  T = 0.01, cost = 0.69, new_cost = 1.02, alpha = 0.00, u = 0.53\n",
            "Step #500:  T = 0.01, cost = 0.69, new_cost = 1.64, alpha = 0.00, u = 0.06\n",
            "Step #501:  T = 0.01, cost = 0.69, new_cost = 2.06, alpha = 0.00, u = 0.21\n",
            "Step #502:  T = 0.01, cost = 0.69, new_cost = 0.97, alpha = 0.00, u = 0.96\n",
            "Step #503:  T = 0.01, cost = 0.69, new_cost = 1.42, alpha = 0.00, u = 0.30\n",
            "Step #504:  T = 0.01, cost = 0.69, new_cost = 1.89, alpha = 0.00, u = 0.30\n",
            "Step #505:  T = 0.01, cost = 0.69, new_cost = 1.65, alpha = 0.00, u = 0.44\n",
            "Step #506:  T = 0.01, cost = 0.69, new_cost = 0.98, alpha = 0.00, u = 0.00\n",
            "Step #507:  T = 0.01, cost = 0.69, new_cost = 1.22, alpha = 0.00, u = 0.42\n",
            "Step #508:  T = 0.01, cost = 0.69, new_cost = 4.14, alpha = 0.00, u = 0.83\n",
            "Step #509:  T = 0.01, cost = 0.69, new_cost = 3.72, alpha = 0.00, u = 0.68\n",
            "Step #510:  T = 0.01, cost = 0.69, new_cost = 1.41, alpha = 0.00, u = 0.09\n",
            "Step #511:  T = 0.01, cost = 0.69, new_cost = 1.24, alpha = 0.00, u = 0.21\n",
            "Step #512:  T = 0.01, cost = 0.69, new_cost = 1.86, alpha = 0.00, u = 0.08\n",
            "Step #513:  T = 0.01, cost = 0.69, new_cost = 3.56, alpha = 0.00, u = 0.23\n",
            "Step #514:  T = 0.01, cost = 0.69, new_cost = 3.82, alpha = 0.00, u = 0.25\n",
            "Step #515:  T = 0.01, cost = 0.69, new_cost = 0.99, alpha = 0.00, u = 0.74\n",
            "Step #516:  T = 0.01, cost = 0.69, new_cost = 0.88, alpha = 0.00, u = 0.68\n",
            "Step #517:  T = 0.01, cost = 0.69, new_cost = 1.48, alpha = 0.00, u = 0.53\n",
            "Step #518:  T = 0.01, cost = 0.69, new_cost = 2.70, alpha = 0.00, u = 0.12\n",
            "Step #519:  T = 0.01, cost = 0.69, new_cost = 1.15, alpha = 0.00, u = 0.67\n",
            "Step #520:  T = 0.01, cost = 0.69, new_cost = 1.34, alpha = 0.00, u = 0.21\n",
            "Step #521:  T = 0.01, cost = 0.69, new_cost = 1.01, alpha = 0.00, u = 0.69\n",
            "Step #522:  T = 0.01, cost = 0.69, new_cost = 1.77, alpha = 0.00, u = 0.16\n",
            "Step #523:  T = 0.01, cost = 0.69, new_cost = 1.04, alpha = 0.00, u = 0.94\n",
            "Step #524:  T = 0.01, cost = 0.69, new_cost = 0.96, alpha = 0.00, u = 0.00\n",
            "Step #525:  T = 0.01, cost = 0.69, new_cost = 1.04, alpha = 0.00, u = 0.26\n",
            "Step #526:  T = 0.01, cost = 0.69, new_cost = 3.62, alpha = 0.00, u = 0.21\n",
            "Step #527:  T = 0.01, cost = 0.69, new_cost = 1.05, alpha = 0.00, u = 0.77\n",
            "Step #528:  T = 0.01, cost = 0.69, new_cost = 1.01, alpha = 0.00, u = 0.05\n",
            "Step #529:  T = 0.00, cost = 0.69, new_cost = 2.16, alpha = 0.00, u = 0.58\n",
            "Step #530:  T = 0.00, cost = 0.69, new_cost = 1.99, alpha = 0.00, u = 0.18\n",
            "Step #531:  T = 0.00, cost = 0.69, new_cost = 4.19, alpha = 0.00, u = 0.33\n",
            "Step #532:  T = 0.00, cost = 0.69, new_cost = 1.64, alpha = 0.00, u = 0.42\n",
            "Step #533:  T = 0.00, cost = 0.69, new_cost = 1.07, alpha = 0.00, u = 0.88\n",
            "Step #534:  T = 0.00, cost = 0.69, new_cost = 1.02, alpha = 0.00, u = 0.45\n",
            "Step #535:  T = 0.00, cost = 0.69, new_cost = 1.27, alpha = 0.00, u = 0.15\n",
            "Step #536:  T = 0.00, cost = 0.69, new_cost = 1.06, alpha = 0.00, u = 0.81\n",
            "Step #537:  T = 0.00, cost = 0.69, new_cost = 1.71, alpha = 0.00, u = 0.58\n",
            "Step #538:  T = 0.00, cost = 0.69, new_cost = 2.72, alpha = 0.00, u = 1.00\n",
            "Step #539:  T = 0.00, cost = 0.69, new_cost = 1.44, alpha = 0.00, u = 0.99\n",
            "Step #540:  T = 0.00, cost = 0.69, new_cost = 4.28, alpha = 0.00, u = 0.98\n",
            "Step #541:  T = 0.00, cost = 0.69, new_cost = 0.98, alpha = 0.00, u = 0.99\n",
            "Step #542:  T = 0.00, cost = 0.69, new_cost = 2.26, alpha = 0.00, u = 0.86\n",
            "Step #543:  T = 0.00, cost = 0.69, new_cost = 1.67, alpha = 0.00, u = 0.94\n",
            "Step #544:  T = 0.00, cost = 0.69, new_cost = 1.65, alpha = 0.00, u = 0.30\n",
            "Step #545:  T = 0.00, cost = 0.69, new_cost = 1.25, alpha = 0.00, u = 0.10\n",
            "Step #546:  T = 0.00, cost = 0.69, new_cost = 0.82, alpha = 0.00, u = 0.00\n",
            "Step #547:  T = 0.00, cost = 0.69, new_cost = 1.94, alpha = 0.00, u = 0.83\n",
            "Step #548:  T = 0.00, cost = 0.69, new_cost = 1.61, alpha = 0.00, u = 0.26\n",
            "Step #549:  T = 0.00, cost = 0.69, new_cost = 1.41, alpha = 0.00, u = 0.82\n",
            "Step #550:  T = 0.00, cost = 0.69, new_cost = 0.88, alpha = 0.00, u = 0.93\n",
            "Step #551:  T = 0.00, cost = 0.69, new_cost = 1.29, alpha = 0.00, u = 0.93\n",
            "Step #552:  T = 0.00, cost = 0.69, new_cost = 1.19, alpha = 0.00, u = 0.26\n",
            "Step #553:  T = 0.00, cost = 0.69, new_cost = 1.80, alpha = 0.00, u = 0.13\n",
            "Step #554:  T = 0.00, cost = 0.69, new_cost = 2.64, alpha = 0.00, u = 0.15\n",
            "Step #555:  T = 0.00, cost = 0.69, new_cost = 2.33, alpha = 0.00, u = 0.26\n",
            "Step #556:  T = 0.00, cost = 0.69, new_cost = 1.01, alpha = 0.00, u = 0.13\n",
            "Step #557:  T = 0.00, cost = 0.69, new_cost = 1.09, alpha = 0.00, u = 0.88\n",
            "Step #558:  T = 0.00, cost = 0.69, new_cost = 1.56, alpha = 0.00, u = 0.62\n",
            "Step #559:  T = 0.00, cost = 0.69, new_cost = 0.78, alpha = 0.00, u = 0.93\n",
            "Step #560:  T = 0.00, cost = 0.69, new_cost = 0.97, alpha = 0.00, u = 0.87\n",
            "Step #561:  T = 0.00, cost = 0.69, new_cost = 1.90, alpha = 0.00, u = 0.59\n",
            "Step #562:  T = 0.00, cost = 0.69, new_cost = 1.15, alpha = 0.00, u = 0.88\n",
            "Step #563:  T = 0.00, cost = 0.69, new_cost = 1.13, alpha = 0.00, u = 0.92\n",
            "Step #564:  T = 0.00, cost = 0.69, new_cost = 1.28, alpha = 0.00, u = 0.76\n",
            "Step #565:  T = 0.00, cost = 0.69, new_cost = 0.95, alpha = 0.00, u = 0.83\n",
            "Step #566:  T = 0.00, cost = 0.69, new_cost = 2.17, alpha = 0.00, u = 0.50\n",
            "Step #567:  T = 0.00, cost = 0.69, new_cost = 1.30, alpha = 0.00, u = 0.53\n",
            "Step #568:  T = 0.00, cost = 0.69, new_cost = 1.27, alpha = 0.00, u = 0.72\n",
            "Step #569:  T = 0.00, cost = 0.69, new_cost = 2.34, alpha = 0.00, u = 0.58\n",
            "Step #570:  T = 0.00, cost = 0.69, new_cost = 1.94, alpha = 0.00, u = 0.75\n",
            "Step #571:  T = 0.00, cost = 0.69, new_cost = 1.21, alpha = 0.00, u = 0.82\n",
            "Step #572:  T = 0.00, cost = 0.69, new_cost = 2.23, alpha = 0.00, u = 0.38\n",
            "Step #573:  T = 0.00, cost = 0.69, new_cost = 1.78, alpha = 0.00, u = 0.18\n",
            "Step #574:  T = 0.00, cost = 0.69, new_cost = 1.09, alpha = 0.00, u = 0.10\n",
            "Step #575:  T = 0.00, cost = 0.69, new_cost = 1.25, alpha = 0.00, u = 0.73\n",
            "Step #576:  T = 0.00, cost = 0.69, new_cost = 1.60, alpha = 0.00, u = 0.63\n",
            "Step #577:  T = 0.00, cost = 0.69, new_cost = 1.67, alpha = 0.00, u = 0.97\n",
            "Step #578:  T = 0.00, cost = 0.69, new_cost = 2.53, alpha = 0.00, u = 0.62\n",
            "Step #579:  T = 0.00, cost = 0.69, new_cost = 2.25, alpha = 0.00, u = 0.83\n",
            "Step #580:  T = 0.00, cost = 0.69, new_cost = 1.58, alpha = 0.00, u = 0.77\n",
            "Step #581:  T = 0.00, cost = 0.69, new_cost = 1.95, alpha = 0.00, u = 0.80\n",
            "Step #582:  T = 0.00, cost = 0.69, new_cost = 2.02, alpha = 0.00, u = 0.50\n",
            "Step #583:  T = 0.00, cost = 0.69, new_cost = 1.31, alpha = 0.00, u = 0.39\n",
            "Step #584:  T = 0.00, cost = 0.69, new_cost = 0.96, alpha = 0.00, u = 0.71\n",
            "Step #585:  T = 0.00, cost = 0.69, new_cost = 1.92, alpha = 0.00, u = 0.05\n",
            "Step #586:  T = 0.00, cost = 0.69, new_cost = 1.29, alpha = 0.00, u = 0.17\n",
            "Step #587:  T = 0.00, cost = 0.69, new_cost = 2.81, alpha = 0.00, u = 0.79\n",
            "Step #588:  T = 0.00, cost = 0.69, new_cost = 1.69, alpha = 0.00, u = 0.49\n",
            "Step #589:  T = 0.00, cost = 0.69, new_cost = 1.47, alpha = 0.00, u = 0.47\n",
            "Step #590:  T = 0.00, cost = 0.69, new_cost = 2.32, alpha = 0.00, u = 0.57\n",
            "Step #591:  T = 0.00, cost = 0.69, new_cost = 1.84, alpha = 0.00, u = 0.12\n",
            "Step #592:  T = 0.00, cost = 0.69, new_cost = 1.15, alpha = 0.00, u = 0.83\n",
            "Step #593:  T = 0.00, cost = 0.69, new_cost = 1.01, alpha = 0.00, u = 0.59\n",
            "Step #594:  T = 0.00, cost = 0.69, new_cost = 1.10, alpha = 0.00, u = 0.52\n",
            "Step #595:  T = 0.00, cost = 0.69, new_cost = 1.13, alpha = 0.00, u = 0.71\n",
            "Step #596:  T = 0.00, cost = 0.69, new_cost = 3.33, alpha = 0.00, u = 0.94\n",
            "Step #597:  T = 0.00, cost = 0.69, new_cost = 1.64, alpha = 0.00, u = 0.15\n",
            "Step #598:  T = 0.00, cost = 0.69, new_cost = 1.65, alpha = 0.00, u = 0.88\n",
            "Step #599:  T = 0.00, cost = 0.69, new_cost = 1.21, alpha = 0.00, u = 0.12\n",
            "Step #600:  T = 0.00, cost = 0.69, new_cost = 1.98, alpha = 0.00, u = 0.51\n",
            "Step #601:  T = 0.00, cost = 0.69, new_cost = 1.85, alpha = 0.00, u = 0.35\n",
            "Step #602:  T = 0.00, cost = 0.69, new_cost = 3.17, alpha = 0.00, u = 0.96\n",
            "Step #603:  T = 0.00, cost = 0.69, new_cost = 0.72, alpha = 0.00, u = 0.79\n",
            "Step #604:  T = 0.00, cost = 0.69, new_cost = 1.73, alpha = 0.00, u = 0.56\n",
            "Step #605:  T = 0.00, cost = 0.69, new_cost = 0.81, alpha = 0.00, u = 0.02\n",
            "Step #606:  T = 0.00, cost = 0.69, new_cost = 1.66, alpha = 0.00, u = 0.83\n",
            "Step #607:  T = 0.00, cost = 0.69, new_cost = 4.62, alpha = 0.00, u = 0.79\n",
            "Step #608:  T = 0.00, cost = 0.69, new_cost = 2.07, alpha = 0.00, u = 0.28\n",
            "Step #609:  T = 0.00, cost = 0.69, new_cost = 1.65, alpha = 0.00, u = 0.91\n",
            "Step #610:  T = 0.00, cost = 0.69, new_cost = 2.49, alpha = 0.00, u = 0.92\n",
            "Step #611:  T = 0.00, cost = 0.69, new_cost = 1.11, alpha = 0.00, u = 0.89\n",
            "Step #612:  T = 0.00, cost = 0.69, new_cost = 1.58, alpha = 0.00, u = 0.33\n",
            "Step #613:  T = 0.00, cost = 0.69, new_cost = 1.07, alpha = 0.00, u = 0.50\n",
            "Step #614:  T = 0.00, cost = 0.69, new_cost = 1.77, alpha = 0.00, u = 0.48\n",
            "Step #615:  T = 0.00, cost = 0.69, new_cost = 0.94, alpha = 0.00, u = 0.72\n",
            "Step #616:  T = 0.00, cost = 0.69, new_cost = 1.17, alpha = 0.00, u = 0.07\n",
            "Step #617:  T = 0.00, cost = 0.69, new_cost = 1.52, alpha = 0.00, u = 0.91\n",
            "Step #618:  T = 0.00, cost = 0.69, new_cost = 1.29, alpha = 0.00, u = 0.91\n",
            "Step #619:  T = 0.00, cost = 0.69, new_cost = 2.79, alpha = 0.00, u = 0.87\n",
            "Step #620:  T = 0.00, cost = 0.69, new_cost = 2.64, alpha = 0.00, u = 0.12\n",
            "Step #621:  T = 0.00, cost = 0.69, new_cost = 2.07, alpha = 0.00, u = 0.95\n",
            "Step #622:  T = 0.00, cost = 0.69, new_cost = 1.37, alpha = 0.00, u = 0.97\n",
            "Step #623:  T = 0.00, cost = 0.69, new_cost = 2.07, alpha = 0.00, u = 0.37\n",
            "Step #624:  T = 0.00, cost = 0.69, new_cost = 1.02, alpha = 0.00, u = 0.28\n",
            "Step #625:  T = 0.00, cost = 0.69, new_cost = 0.98, alpha = 0.00, u = 0.97\n",
            "Step #626:  T = 0.00, cost = 0.69, new_cost = 1.05, alpha = 0.00, u = 0.85\n",
            "Step #627:  T = 0.00, cost = 0.69, new_cost = 2.44, alpha = 0.00, u = 0.45\n",
            "Step #628:  T = 0.00, cost = 0.69, new_cost = 0.98, alpha = 0.00, u = 0.45\n",
            "Step #629:  T = 0.00, cost = 0.69, new_cost = 2.38, alpha = 0.00, u = 0.75\n",
            "Step #630:  T = 0.00, cost = 0.69, new_cost = 1.29, alpha = 0.00, u = 0.75\n",
            "Step #631:  T = 0.00, cost = 0.69, new_cost = 0.91, alpha = 0.00, u = 0.84\n",
            "Step #632:  T = 0.00, cost = 0.69, new_cost = 2.08, alpha = 0.00, u = 0.31\n",
            "Step #633:  T = 0.00, cost = 0.69, new_cost = 1.65, alpha = 0.00, u = 0.56\n",
            "Step #634:  T = 0.00, cost = 0.69, new_cost = 2.15, alpha = 0.00, u = 0.98\n",
            "Step #635:  T = 0.00, cost = 0.69, new_cost = 1.11, alpha = 0.00, u = 0.99\n",
            "Step #636:  T = 0.00, cost = 0.69, new_cost = 0.97, alpha = 0.00, u = 0.41\n",
            "Step #637:  T = 0.00, cost = 0.69, new_cost = 1.01, alpha = 0.00, u = 0.47\n",
            "Step #638:  T = 0.00, cost = 0.69, new_cost = 0.73, alpha = 0.00, u = 0.09\n",
            "Step #639:  T = 0.00, cost = 0.69, new_cost = 1.42, alpha = 0.00, u = 0.53\n",
            "Step #640:  T = 0.00, cost = 0.69, new_cost = 1.27, alpha = 0.00, u = 0.81\n",
            "Step #641:  T = 0.00, cost = 0.69, new_cost = 2.06, alpha = 0.00, u = 0.86\n",
            "Step #642:  T = 0.00, cost = 0.69, new_cost = 1.65, alpha = 0.00, u = 0.27\n",
            "Step #643:  T = 0.00, cost = 0.69, new_cost = 2.84, alpha = 0.00, u = 0.73\n",
            "Step #644:  T = 0.00, cost = 0.69, new_cost = 0.75, alpha = 0.00, u = 0.13\n",
            "Step #645:  T = 0.00, cost = 0.69, new_cost = 3.54, alpha = 0.00, u = 0.35\n",
            "Step #646:  T = 0.00, cost = 0.69, new_cost = 1.07, alpha = 0.00, u = 0.02\n",
            "Step #647:  T = 0.00, cost = 0.69, new_cost = 1.47, alpha = 0.00, u = 0.64\n",
            "Step #648:  T = 0.00, cost = 0.69, new_cost = 2.07, alpha = 0.00, u = 0.33\n",
            "Step #649:  T = 0.00, cost = 0.69, new_cost = 1.43, alpha = 0.00, u = 0.81\n",
            "Step #650:  T = 0.00, cost = 0.69, new_cost = 0.82, alpha = 0.00, u = 0.23\n",
            "Step #651:  T = 0.00, cost = 0.69, new_cost = 1.28, alpha = 0.00, u = 0.83\n",
            "Step #652:  T = 0.00, cost = 0.69, new_cost = 3.27, alpha = 0.00, u = 0.15\n",
            "Step #653:  T = 0.00, cost = 0.69, new_cost = 1.42, alpha = 0.00, u = 0.58\n",
            "Step #654:  T = 0.00, cost = 0.69, new_cost = 1.15, alpha = 0.00, u = 0.56\n",
            "Step #655:  T = 0.00, cost = 0.69, new_cost = 0.96, alpha = 0.00, u = 0.17\n",
            "Step #656:  T = 0.00, cost = 0.69, new_cost = 0.76, alpha = 0.00, u = 0.88\n",
            "Step #657:  T = 0.00, cost = 0.69, new_cost = 0.99, alpha = 0.00, u = 0.60\n",
            "Step #658:  T = 0.00, cost = 0.69, new_cost = 1.40, alpha = 0.00, u = 0.49\n",
            "Step #659:  T = 0.00, cost = 0.69, new_cost = 0.94, alpha = 0.00, u = 0.37\n",
            "Step #660:  T = 0.00, cost = 0.69, new_cost = 1.09, alpha = 0.00, u = 0.66\n",
            "Step #661:  T = 0.00, cost = 0.69, new_cost = 0.62, alpha = 279574850785095700185088.00, u = 0.60\n",
            "Step #662:  T = 0.00, cost = 0.62, new_cost = 2.13, alpha = 0.00, u = 0.66\n",
            "Step #663:  T = 0.00, cost = 0.62, new_cost = 0.98, alpha = 0.00, u = 0.19\n",
            "Step #664:  T = 0.00, cost = 0.62, new_cost = 1.39, alpha = 0.00, u = 0.04\n",
            "Step #665:  T = 0.00, cost = 0.62, new_cost = 0.75, alpha = 0.00, u = 0.52\n",
            "Step #666:  T = 0.00, cost = 0.62, new_cost = 0.84, alpha = 0.00, u = 0.38\n",
            "Step #667:  T = 0.00, cost = 0.62, new_cost = 1.02, alpha = 0.00, u = 0.21\n",
            "Step #668:  T = 0.00, cost = 0.62, new_cost = 1.92, alpha = 0.00, u = 0.48\n",
            "Step #669:  T = 0.00, cost = 0.62, new_cost = 1.38, alpha = 0.00, u = 0.09\n",
            "Step #670:  T = 0.00, cost = 0.62, new_cost = 1.46, alpha = 0.00, u = 0.22\n",
            "Step #671:  T = 0.00, cost = 0.62, new_cost = 2.08, alpha = 0.00, u = 0.35\n",
            "Step #672:  T = 0.00, cost = 0.62, new_cost = 1.54, alpha = 0.00, u = 0.51\n",
            "Step #673:  T = 0.00, cost = 0.62, new_cost = 2.88, alpha = 0.00, u = 0.01\n",
            "Step #674:  T = 0.00, cost = 0.62, new_cost = 0.96, alpha = 0.00, u = 0.83\n",
            "Step #675:  T = 0.00, cost = 0.62, new_cost = 1.21, alpha = 0.00, u = 0.73\n",
            "Step #676:  T = 0.00, cost = 0.62, new_cost = 1.30, alpha = 0.00, u = 0.22\n",
            "Step #677:  T = 0.00, cost = 0.62, new_cost = 1.25, alpha = 0.00, u = 0.70\n",
            "Step #678:  T = 0.00, cost = 0.62, new_cost = 1.62, alpha = 0.00, u = 0.87\n",
            "Step #679:  T = 0.00, cost = 0.62, new_cost = 3.36, alpha = 0.00, u = 0.79\n",
            "Step #680:  T = 0.00, cost = 0.62, new_cost = 1.43, alpha = 0.00, u = 0.36\n",
            "Step #681:  T = 0.00, cost = 0.62, new_cost = 0.81, alpha = 0.00, u = 0.07\n",
            "Step #682:  T = 0.00, cost = 0.62, new_cost = 1.24, alpha = 0.00, u = 0.23\n",
            "Step #683:  T = 0.00, cost = 0.62, new_cost = 1.26, alpha = 0.00, u = 0.07\n",
            "Step #684:  T = 0.00, cost = 0.62, new_cost = 1.09, alpha = 0.00, u = 0.08\n",
            "Step #685:  T = 0.00, cost = 0.62, new_cost = 1.77, alpha = 0.00, u = 0.40\n",
            "Step #686:  T = 0.00, cost = 0.62, new_cost = 1.29, alpha = 0.00, u = 0.94\n",
            "Step #687:  T = 0.00, cost = 0.62, new_cost = 1.14, alpha = 0.00, u = 0.05\n",
            "Step #688:  T = 0.00, cost = 0.62, new_cost = 1.86, alpha = 0.00, u = 0.76\n",
            "Step #689:  T = 0.00, cost = 0.62, new_cost = 1.97, alpha = 0.00, u = 0.92\n",
            "Step #690:  T = 0.00, cost = 0.62, new_cost = 1.57, alpha = 0.00, u = 0.18\n",
            "Step #691:  T = 0.00, cost = 0.62, new_cost = 2.45, alpha = 0.00, u = 0.89\n",
            "Step #692:  T = 0.00, cost = 0.62, new_cost = 1.46, alpha = 0.00, u = 0.86\n",
            "Step #693:  T = 0.00, cost = 0.62, new_cost = 1.70, alpha = 0.00, u = 0.92\n",
            "Step #694:  T = 0.00, cost = 0.62, new_cost = 1.65, alpha = 0.00, u = 0.36\n",
            "Step #695:  T = 0.00, cost = 0.62, new_cost = 0.95, alpha = 0.00, u = 0.54\n",
            "Step #696:  T = 0.00, cost = 0.62, new_cost = 0.83, alpha = 0.00, u = 0.73\n",
            "Step #697:  T = 0.00, cost = 0.62, new_cost = 0.84, alpha = 0.00, u = 0.12\n",
            "Step #698:  T = 0.00, cost = 0.62, new_cost = 1.58, alpha = 0.00, u = 0.57\n",
            "Step #699:  T = 0.00, cost = 0.62, new_cost = 1.41, alpha = 0.00, u = 0.24\n",
            "Step #700:  T = 0.00, cost = 0.62, new_cost = 2.68, alpha = 0.00, u = 0.36\n",
            "Step #701:  T = 0.00, cost = 0.62, new_cost = 1.40, alpha = 0.00, u = 0.35\n",
            "Step #702:  T = 0.00, cost = 0.62, new_cost = 0.92, alpha = 0.00, u = 0.74\n",
            "Step #703:  T = 0.00, cost = 0.62, new_cost = 1.06, alpha = 0.00, u = 0.93\n",
            "Step #704:  T = 0.00, cost = 0.62, new_cost = 1.45, alpha = 0.00, u = 0.55\n",
            "Step #705:  T = 0.00, cost = 0.62, new_cost = 1.02, alpha = 0.00, u = 0.98\n",
            "Step #706:  T = 0.00, cost = 0.62, new_cost = 1.43, alpha = 0.00, u = 0.16\n",
            "Step #707:  T = 0.00, cost = 0.62, new_cost = 2.73, alpha = 0.00, u = 0.61\n",
            "Step #708:  T = 0.00, cost = 0.62, new_cost = 0.89, alpha = 0.00, u = 0.69\n",
            "Step #709:  T = 0.00, cost = 0.62, new_cost = 3.15, alpha = 0.00, u = 0.31\n",
            "Step #710:  T = 0.00, cost = 0.62, new_cost = 2.62, alpha = 0.00, u = 0.45\n",
            "Step #711:  T = 0.00, cost = 0.62, new_cost = 1.11, alpha = 0.00, u = 0.98\n",
            "Step #712:  T = 0.00, cost = 0.62, new_cost = 1.00, alpha = 0.00, u = 0.12\n",
            "Step #713:  T = 0.00, cost = 0.62, new_cost = 2.65, alpha = 0.00, u = 0.02\n",
            "Step #714:  T = 0.00, cost = 0.62, new_cost = 1.22, alpha = 0.00, u = 0.30\n",
            "Step #715:  T = 0.00, cost = 0.62, new_cost = 1.35, alpha = 0.00, u = 0.55\n",
            "Step #716:  T = 0.00, cost = 0.62, new_cost = 1.97, alpha = 0.00, u = 0.57\n",
            "Step #717:  T = 0.00, cost = 0.62, new_cost = 1.88, alpha = 0.00, u = 0.02\n",
            "Step #718:  T = 0.00, cost = 0.62, new_cost = 2.30, alpha = 0.00, u = 0.36\n",
            "Step #719:  T = 0.00, cost = 0.62, new_cost = 1.11, alpha = 0.00, u = 0.90\n",
            "Step #720:  T = 0.00, cost = 0.62, new_cost = 1.33, alpha = 0.00, u = 0.19\n",
            "Step #721:  T = 0.00, cost = 0.62, new_cost = 1.66, alpha = 0.00, u = 0.39\n",
            "Step #722:  T = 0.00, cost = 0.62, new_cost = 1.40, alpha = 0.00, u = 0.69\n",
            "Step #723:  T = 0.00, cost = 0.62, new_cost = 2.00, alpha = 0.00, u = 0.85\n",
            "Step #724:  T = 0.00, cost = 0.62, new_cost = 5.42, alpha = 0.00, u = 0.45\n",
            "Step #725:  T = 0.00, cost = 0.62, new_cost = 1.10, alpha = 0.00, u = 0.98\n",
            "Step #726:  T = 0.00, cost = 0.62, new_cost = 1.08, alpha = 0.00, u = 0.96\n",
            "Step #727:  T = 0.00, cost = 0.62, new_cost = 3.25, alpha = 0.00, u = 1.00\n",
            "Step #728:  T = 0.00, cost = 0.62, new_cost = 2.71, alpha = 0.00, u = 0.98\n",
            "Step #729:  T = 0.00, cost = 0.62, new_cost = 2.04, alpha = 0.00, u = 0.23\n",
            "Step #730:  T = 0.00, cost = 0.62, new_cost = 5.13, alpha = 0.00, u = 0.25\n",
            "Step #731:  T = 0.00, cost = 0.62, new_cost = 1.00, alpha = 0.00, u = 0.25\n",
            "Step #732:  T = 0.00, cost = 0.62, new_cost = 1.25, alpha = 0.00, u = 0.31\n",
            "Step #733:  T = 0.00, cost = 0.62, new_cost = 1.29, alpha = 0.00, u = 0.03\n",
            "Step #734:  T = 0.00, cost = 0.62, new_cost = 1.92, alpha = 0.00, u = 0.73\n",
            "Step #735:  T = 0.00, cost = 0.62, new_cost = 1.16, alpha = 0.00, u = 0.48\n",
            "Step #736:  T = 0.00, cost = 0.62, new_cost = 1.13, alpha = 0.00, u = 0.93\n",
            "Step #737:  T = 0.00, cost = 0.62, new_cost = 1.95, alpha = 0.00, u = 0.81\n",
            "Step #738:  T = 0.00, cost = 0.62, new_cost = 0.98, alpha = 0.00, u = 0.41\n",
            "Step #739:  T = 0.00, cost = 0.62, new_cost = 1.48, alpha = 0.00, u = 0.02\n",
            "Step #740:  T = 0.00, cost = 0.62, new_cost = 0.73, alpha = 0.00, u = 0.52\n",
            "Step #741:  T = 0.00, cost = 0.62, new_cost = 4.03, alpha = 0.00, u = 0.80\n",
            "Step #742:  T = 0.00, cost = 0.62, new_cost = 1.15, alpha = 0.00, u = 0.14\n",
            "Step #743:  T = 0.00, cost = 0.62, new_cost = 0.74, alpha = 0.00, u = 0.84\n",
            "Step #744:  T = 0.00, cost = 0.62, new_cost = 1.18, alpha = 0.00, u = 0.93\n",
            "Step #745:  T = 0.00, cost = 0.62, new_cost = 2.32, alpha = 0.00, u = 0.42\n",
            "Step #746:  T = 0.00, cost = 0.62, new_cost = 2.00, alpha = 0.00, u = 0.74\n",
            "Step #747:  T = 0.00, cost = 0.62, new_cost = 0.93, alpha = 0.00, u = 0.39\n",
            "Step #748:  T = 0.00, cost = 0.62, new_cost = 2.12, alpha = 0.00, u = 0.54\n",
            "Step #749:  T = 0.00, cost = 0.62, new_cost = 0.85, alpha = 0.00, u = 0.77\n",
            "Step #750:  T = 0.00, cost = 0.62, new_cost = 1.25, alpha = 0.00, u = 0.57\n",
            "Step #751:  T = 0.00, cost = 0.62, new_cost = 2.24, alpha = 0.00, u = 0.37\n",
            "Step #752:  T = 0.00, cost = 0.62, new_cost = 1.18, alpha = 0.00, u = 0.43\n",
            "Step #753:  T = 0.00, cost = 0.62, new_cost = 1.22, alpha = 0.00, u = 0.97\n",
            "Step #754:  T = 0.00, cost = 0.62, new_cost = 0.67, alpha = 0.00, u = 0.13\n",
            "Step #755:  T = 0.00, cost = 0.62, new_cost = 1.26, alpha = 0.00, u = 0.76\n",
            "Step #756:  T = 0.00, cost = 0.62, new_cost = 2.30, alpha = 0.00, u = 0.11\n",
            "Step #757:  T = 0.00, cost = 0.62, new_cost = 1.07, alpha = 0.00, u = 0.50\n",
            "Step #758:  T = 0.00, cost = 0.62, new_cost = 1.29, alpha = 0.00, u = 0.80\n",
            "Step #759:  T = 0.00, cost = 0.62, new_cost = 1.72, alpha = 0.00, u = 0.77\n",
            "Step #760:  T = 0.00, cost = 0.62, new_cost = 1.31, alpha = 0.00, u = 0.57\n",
            "Step #761:  T = 0.00, cost = 0.62, new_cost = 2.36, alpha = 0.00, u = 0.07\n",
            "Step #762:  T = 0.00, cost = 0.62, new_cost = 2.08, alpha = 0.00, u = 0.52\n",
            "Step #763:  T = 0.00, cost = 0.62, new_cost = 1.16, alpha = 0.00, u = 0.07\n",
            "Step #764:  T = 0.00, cost = 0.62, new_cost = 1.49, alpha = 0.00, u = 0.05\n",
            "Step #765:  T = 0.00, cost = 0.62, new_cost = 0.78, alpha = 0.00, u = 0.21\n",
            "Step #766:  T = 0.00, cost = 0.62, new_cost = 1.46, alpha = 0.00, u = 0.63\n",
            "Step #767:  T = 0.00, cost = 0.62, new_cost = 1.03, alpha = 0.00, u = 0.19\n",
            "Step #768:  T = 0.00, cost = 0.62, new_cost = 1.19, alpha = 0.00, u = 0.61\n",
            "Step #769:  T = 0.00, cost = 0.62, new_cost = 1.63, alpha = 0.00, u = 0.75\n",
            "Step #770:  T = 0.00, cost = 0.62, new_cost = 1.22, alpha = 0.00, u = 0.26\n",
            "Step #771:  T = 0.00, cost = 0.62, new_cost = 1.91, alpha = 0.00, u = 0.94\n",
            "Step #772:  T = 0.00, cost = 0.62, new_cost = 1.52, alpha = 0.00, u = 0.26\n",
            "Step #773:  T = 0.00, cost = 0.62, new_cost = 1.09, alpha = 0.00, u = 0.03\n",
            "Step #774:  T = 0.00, cost = 0.62, new_cost = 1.01, alpha = 0.00, u = 0.54\n",
            "Step #775:  T = 0.00, cost = 0.62, new_cost = 3.58, alpha = 0.00, u = 0.15\n",
            "Step #776:  T = 0.00, cost = 0.62, new_cost = 2.28, alpha = 0.00, u = 0.72\n",
            "Step #777:  T = 0.00, cost = 0.62, new_cost = 2.80, alpha = 0.00, u = 0.60\n",
            "Step #778:  T = 0.00, cost = 0.62, new_cost = 3.30, alpha = 0.00, u = 0.53\n",
            "Step #779:  T = 0.00, cost = 0.62, new_cost = 2.47, alpha = 0.00, u = 0.64\n",
            "Step #780:  T = 0.00, cost = 0.62, new_cost = 1.38, alpha = 0.00, u = 0.24\n",
            "Step #781:  T = 0.00, cost = 0.62, new_cost = 1.49, alpha = 0.00, u = 0.74\n",
            "Step #782:  T = 0.00, cost = 0.62, new_cost = 2.72, alpha = 0.00, u = 0.67\n",
            "Step #783:  T = 0.00, cost = 0.62, new_cost = 4.83, alpha = 0.00, u = 0.83\n",
            "Step #784:  T = 0.00, cost = 0.62, new_cost = 1.16, alpha = 0.00, u = 0.25\n",
            "Step #785:  T = 0.00, cost = 0.62, new_cost = 1.01, alpha = 0.00, u = 0.07\n",
            "Step #786:  T = 0.00, cost = 0.62, new_cost = 1.43, alpha = 0.00, u = 0.50\n",
            "Step #787:  T = 0.00, cost = 0.62, new_cost = 0.92, alpha = 0.00, u = 0.10\n",
            "Step #788:  T = 0.00, cost = 0.62, new_cost = 1.12, alpha = 0.00, u = 0.79\n",
            "Step #789:  T = 0.00, cost = 0.62, new_cost = 1.71, alpha = 0.00, u = 0.02\n",
            "Step #790:  T = 0.00, cost = 0.62, new_cost = 2.98, alpha = 0.00, u = 0.44\n",
            "Step #791:  T = 0.00, cost = 0.62, new_cost = 2.04, alpha = 0.00, u = 0.11\n",
            "Step #792:  T = 0.00, cost = 0.62, new_cost = 1.63, alpha = 0.00, u = 0.75\n",
            "Step #793:  T = 0.00, cost = 0.62, new_cost = 2.93, alpha = 0.00, u = 0.29\n",
            "Step #794:  T = 0.00, cost = 0.62, new_cost = 0.92, alpha = 0.00, u = 1.00\n",
            "Step #795:  T = 0.00, cost = 0.62, new_cost = 0.93, alpha = 0.00, u = 0.45\n",
            "Step #796:  T = 0.00, cost = 0.62, new_cost = 1.28, alpha = 0.00, u = 0.95\n",
            "Step #797:  T = 0.00, cost = 0.62, new_cost = 1.58, alpha = 0.00, u = 0.36\n",
            "Step #798:  T = 0.00, cost = 0.62, new_cost = 1.35, alpha = 0.00, u = 0.05\n",
            "Step #799:  T = 0.00, cost = 0.62, new_cost = 1.07, alpha = 0.00, u = 0.16\n",
            "Step #800:  T = 0.00, cost = 0.62, new_cost = 1.86, alpha = 0.00, u = 0.37\n",
            "Step #801:  T = 0.00, cost = 0.62, new_cost = 1.25, alpha = 0.00, u = 0.10\n",
            "Step #802:  T = 0.00, cost = 0.62, new_cost = 2.17, alpha = 0.00, u = 0.16\n",
            "Step #803:  T = 0.00, cost = 0.62, new_cost = 1.12, alpha = 0.00, u = 0.67\n",
            "Step #804:  T = 0.00, cost = 0.62, new_cost = 1.83, alpha = 0.00, u = 0.29\n",
            "Step #805:  T = 0.00, cost = 0.62, new_cost = 1.29, alpha = 0.00, u = 0.23\n",
            "Step #806:  T = 0.00, cost = 0.62, new_cost = 1.89, alpha = 0.00, u = 0.48\n",
            "Step #807:  T = 0.00, cost = 0.62, new_cost = 1.01, alpha = 0.00, u = 0.27\n",
            "Step #808:  T = 0.00, cost = 0.62, new_cost = 1.71, alpha = 0.00, u = 0.88\n",
            "Step #809:  T = 0.00, cost = 0.62, new_cost = 2.58, alpha = 0.00, u = 0.65\n",
            "Step #810:  T = 0.00, cost = 0.62, new_cost = 1.21, alpha = 0.00, u = 0.21\n",
            "Step #811:  T = 0.00, cost = 0.62, new_cost = 1.05, alpha = 0.00, u = 0.44\n",
            "Step #812:  T = 0.00, cost = 0.62, new_cost = 1.06, alpha = 0.00, u = 0.14\n",
            "Step #813:  T = 0.00, cost = 0.62, new_cost = 4.49, alpha = 0.00, u = 0.68\n",
            "Step #814:  T = 0.00, cost = 0.62, new_cost = 1.04, alpha = 0.00, u = 0.89\n",
            "Step #815:  T = 0.00, cost = 0.62, new_cost = 1.09, alpha = 0.00, u = 0.50\n",
            "Step #816:  T = 0.00, cost = 0.62, new_cost = 1.04, alpha = 0.00, u = 0.57\n",
            "Step #817:  T = 0.00, cost = 0.62, new_cost = 1.74, alpha = 0.00, u = 0.45\n",
            "Step #818:  T = 0.00, cost = 0.62, new_cost = 1.86, alpha = 0.00, u = 0.27\n",
            "Step #819:  T = 0.00, cost = 0.62, new_cost = 2.03, alpha = 0.00, u = 0.92\n",
            "Step #820:  T = 0.00, cost = 0.62, new_cost = 1.54, alpha = 0.00, u = 0.07\n",
            "Step #821:  T = 0.00, cost = 0.62, new_cost = 1.58, alpha = 0.00, u = 0.56\n",
            "Step #822:  T = 0.00, cost = 0.62, new_cost = 1.47, alpha = 0.00, u = 0.48\n",
            "Step #823:  T = 0.00, cost = 0.62, new_cost = 1.06, alpha = 0.00, u = 0.02\n",
            "Step #824:  T = 0.00, cost = 0.62, new_cost = 1.04, alpha = 0.00, u = 0.17\n",
            "Step #825:  T = 0.00, cost = 0.62, new_cost = 0.72, alpha = 0.00, u = 1.00\n",
            "Step #826:  T = 0.00, cost = 0.62, new_cost = 1.48, alpha = 0.00, u = 0.05\n",
            "Step #827:  T = 0.00, cost = 0.62, new_cost = 1.63, alpha = 0.00, u = 0.39\n",
            "Step #828:  T = 0.00, cost = 0.62, new_cost = 3.38, alpha = 0.00, u = 0.17\n",
            "Step #829:  T = 0.00, cost = 0.62, new_cost = 0.78, alpha = 0.00, u = 0.96\n",
            "Step #830:  T = 0.00, cost = 0.62, new_cost = 0.76, alpha = 0.00, u = 0.66\n",
            "Step #831:  T = 0.00, cost = 0.62, new_cost = 1.14, alpha = 0.00, u = 0.54\n",
            "Step #832:  T = 0.00, cost = 0.62, new_cost = 2.11, alpha = 0.00, u = 0.67\n",
            "Step #833:  T = 0.00, cost = 0.62, new_cost = 1.25, alpha = 0.00, u = 0.10\n",
            "Step #834:  T = 0.00, cost = 0.62, new_cost = 1.10, alpha = 0.00, u = 0.39\n",
            "Step #835:  T = 0.00, cost = 0.62, new_cost = 1.30, alpha = 0.00, u = 0.22\n",
            "Step #836:  T = 0.00, cost = 0.62, new_cost = 1.91, alpha = 0.00, u = 0.74\n",
            "Step #837:  T = 0.00, cost = 0.62, new_cost = 1.66, alpha = 0.00, u = 0.55\n",
            "Step #838:  T = 0.00, cost = 0.62, new_cost = 1.63, alpha = 0.00, u = 0.07\n",
            "Step #839:  T = 0.00, cost = 0.62, new_cost = 2.17, alpha = 0.00, u = 0.44\n",
            "Step #840:  T = 0.00, cost = 0.62, new_cost = 1.89, alpha = 0.00, u = 0.56\n",
            "Step #841:  T = 0.00, cost = 0.62, new_cost = 1.54, alpha = 0.00, u = 0.69\n",
            "Step #842:  T = 0.00, cost = 0.62, new_cost = 1.40, alpha = 0.00, u = 0.45\n",
            "Step #843:  T = 0.00, cost = 0.62, new_cost = 1.98, alpha = 0.00, u = 0.43\n",
            "Step #844:  T = 0.00, cost = 0.62, new_cost = 2.33, alpha = 0.00, u = 0.27\n",
            "Step #845:  T = 0.00, cost = 0.62, new_cost = 1.25, alpha = 0.00, u = 0.90\n",
            "Step #846:  T = 0.00, cost = 0.62, new_cost = 1.71, alpha = 0.00, u = 0.88\n",
            "Step #847:  T = 0.00, cost = 0.62, new_cost = 4.87, alpha = 0.00, u = 0.86\n",
            "Step #848:  T = 0.00, cost = 0.62, new_cost = 1.63, alpha = 0.00, u = 0.14\n",
            "Step #849:  T = 0.00, cost = 0.62, new_cost = 1.73, alpha = 0.00, u = 0.26\n",
            "Step #850:  T = 0.00, cost = 0.62, new_cost = 1.34, alpha = 0.00, u = 0.11\n",
            "Step #851:  T = 0.00, cost = 0.62, new_cost = 1.27, alpha = 0.00, u = 0.23\n",
            "Step #852:  T = 0.00, cost = 0.62, new_cost = 1.04, alpha = 0.00, u = 0.60\n",
            "Step #853:  T = 0.00, cost = 0.62, new_cost = 1.37, alpha = 0.00, u = 0.13\n",
            "Step #854:  T = 0.00, cost = 0.62, new_cost = 1.33, alpha = 0.00, u = 0.07\n",
            "Step #855:  T = 0.00, cost = 0.62, new_cost = 1.40, alpha = 0.00, u = 0.76\n",
            "Step #856:  T = 0.00, cost = 0.62, new_cost = 1.20, alpha = 0.00, u = 0.28\n",
            "Step #857:  T = 0.00, cost = 0.62, new_cost = 1.28, alpha = 0.00, u = 0.93\n",
            "Step #858:  T = 0.00, cost = 0.62, new_cost = 0.89, alpha = 0.00, u = 0.31\n",
            "Step #859:  T = 0.00, cost = 0.62, new_cost = 1.15, alpha = 0.00, u = 0.20\n",
            "Step #860:  T = 0.00, cost = 0.62, new_cost = 1.61, alpha = 0.00, u = 0.97\n",
            "Step #861:  T = 0.00, cost = 0.62, new_cost = 1.44, alpha = 0.00, u = 0.26\n",
            "Step #862:  T = 0.00, cost = 0.62, new_cost = 1.47, alpha = 0.00, u = 0.15\n",
            "Step #863:  T = 0.00, cost = 0.62, new_cost = 1.85, alpha = 0.00, u = 0.57\n",
            "Step #864:  T = 0.00, cost = 0.62, new_cost = 1.66, alpha = 0.00, u = 0.84\n",
            "Step #865:  T = 0.00, cost = 0.62, new_cost = 1.27, alpha = 0.00, u = 0.04\n",
            "Step #866:  T = 0.00, cost = 0.62, new_cost = 1.42, alpha = 0.00, u = 0.45\n",
            "Step #867:  T = 0.00, cost = 0.62, new_cost = 1.26, alpha = 0.00, u = 0.95\n",
            "Step #868:  T = 0.00, cost = 0.62, new_cost = 0.89, alpha = 0.00, u = 0.41\n",
            "Step #869:  T = 0.00, cost = 0.62, new_cost = 1.09, alpha = 0.00, u = 0.40\n",
            "Step #870:  T = 0.00, cost = 0.62, new_cost = 1.49, alpha = 0.00, u = 1.00\n",
            "Step #871:  T = 0.00, cost = 0.62, new_cost = 1.96, alpha = 0.00, u = 0.48\n",
            "Step #872:  T = 0.00, cost = 0.62, new_cost = 2.37, alpha = 0.00, u = 0.04\n",
            "Step #873:  T = 0.00, cost = 0.62, new_cost = 2.26, alpha = 0.00, u = 0.38\n",
            "Step #874:  T = 0.00, cost = 0.62, new_cost = 1.83, alpha = 0.00, u = 0.32\n",
            "Step #875:  T = 0.00, cost = 0.62, new_cost = 1.44, alpha = 0.00, u = 0.36\n",
            "Step #876:  T = 0.00, cost = 0.62, new_cost = 2.66, alpha = 0.00, u = 0.21\n",
            "Step #877:  T = 0.00, cost = 0.62, new_cost = 2.28, alpha = 0.00, u = 0.71\n",
            "Step #878:  T = 0.00, cost = 0.62, new_cost = 1.13, alpha = 0.00, u = 0.96\n",
            "Step #879:  T = 0.00, cost = 0.62, new_cost = 1.39, alpha = 0.00, u = 0.60\n",
            "Step #880:  T = 0.00, cost = 0.62, new_cost = 2.14, alpha = 0.00, u = 0.98\n",
            "Step #881:  T = 0.00, cost = 0.62, new_cost = 2.71, alpha = 0.00, u = 0.24\n",
            "Step #882:  T = 0.00, cost = 0.62, new_cost = 1.09, alpha = 0.00, u = 0.62\n",
            "Step #883:  T = 0.00, cost = 0.62, new_cost = 0.91, alpha = 0.00, u = 0.52\n",
            "Step #884:  T = 0.00, cost = 0.62, new_cost = 1.02, alpha = 0.00, u = 0.25\n",
            "Step #885:  T = 0.00, cost = 0.62, new_cost = 1.00, alpha = 0.00, u = 0.46\n",
            "Step #886:  T = 0.00, cost = 0.62, new_cost = 1.85, alpha = 0.00, u = 0.90\n",
            "Step #887:  T = 0.00, cost = 0.62, new_cost = 1.35, alpha = 0.00, u = 0.72\n",
            "Step #888:  T = 0.00, cost = 0.62, new_cost = 1.20, alpha = 0.00, u = 0.61\n",
            "Step #889:  T = 0.00, cost = 0.62, new_cost = 1.44, alpha = 0.00, u = 0.78\n",
            "Step #890:  T = 0.00, cost = 0.62, new_cost = 0.84, alpha = 0.00, u = 0.36\n",
            "Step #891:  T = 0.00, cost = 0.62, new_cost = 1.18, alpha = 0.00, u = 0.08\n",
            "Step #892:  T = 0.00, cost = 0.62, new_cost = 1.51, alpha = 0.00, u = 0.01\n",
            "Step #893:  T = 0.00, cost = 0.62, new_cost = 0.81, alpha = 0.00, u = 0.21\n",
            "Step #894:  T = 0.00, cost = 0.62, new_cost = 1.62, alpha = 0.00, u = 0.65\n",
            "Step #895:  T = 0.00, cost = 0.62, new_cost = 1.16, alpha = 0.00, u = 0.93\n",
            "Step #896:  T = 0.00, cost = 0.62, new_cost = 1.85, alpha = 0.00, u = 0.65\n",
            "Step #897:  T = 0.00, cost = 0.62, new_cost = 1.22, alpha = 0.00, u = 0.40\n",
            "Step #898:  T = 0.00, cost = 0.62, new_cost = 1.45, alpha = 0.00, u = 0.08\n",
            "Step #899:  T = 0.00, cost = 0.62, new_cost = 1.46, alpha = 0.00, u = 0.29\n",
            "Step #900:  T = 0.00, cost = 0.62, new_cost = 1.04, alpha = 0.00, u = 0.30\n",
            "Step #901:  T = 0.00, cost = 0.62, new_cost = 0.67, alpha = 0.00, u = 0.39\n",
            "Step #902:  T = 0.00, cost = 0.62, new_cost = 3.16, alpha = 0.00, u = 0.85\n",
            "Step #903:  T = 0.00, cost = 0.62, new_cost = 1.02, alpha = 0.00, u = 0.81\n",
            "Step #904:  T = 0.00, cost = 0.62, new_cost = 1.07, alpha = 0.00, u = 0.53\n",
            "Step #905:  T = 0.00, cost = 0.62, new_cost = 1.26, alpha = 0.00, u = 0.92\n",
            "Step #906:  T = 0.00, cost = 0.62, new_cost = 1.52, alpha = 0.00, u = 0.70\n",
            "Step #907:  T = 0.00, cost = 0.62, new_cost = 2.67, alpha = 0.00, u = 0.28\n",
            "Step #908:  T = 0.00, cost = 0.62, new_cost = 1.20, alpha = 0.00, u = 0.38\n",
            "Step #909:  T = 0.00, cost = 0.62, new_cost = 2.04, alpha = 0.00, u = 0.70\n",
            "Step #910:  T = 0.00, cost = 0.62, new_cost = 1.10, alpha = 0.00, u = 0.69\n",
            "Step #911:  T = 0.00, cost = 0.62, new_cost = 2.14, alpha = 0.00, u = 0.23\n",
            "Step #912:  T = 0.00, cost = 0.62, new_cost = 0.89, alpha = 0.00, u = 0.61\n",
            "Step #913:  T = 0.00, cost = 0.62, new_cost = 2.62, alpha = 0.00, u = 0.47\n",
            "Step #914:  T = 0.00, cost = 0.62, new_cost = 1.74, alpha = 0.00, u = 0.47\n",
            "Step #915:  T = 0.00, cost = 0.62, new_cost = 0.56, alpha = 10453259999606573776589118356714036296215746700395153099027711028968046314172964292684608827049902810699425116602710540371893015559805710825929880333949292094220973599279598941004779974293516488240935859338196897652316330415035764634182026143997671657769663966740480.00, u = 0.50\n",
            "Step #916:  T = 0.00, cost = 0.56, new_cost = 1.21, alpha = 0.00, u = 0.30\n",
            "Step #917:  T = 0.00, cost = 0.56, new_cost = 0.90, alpha = 0.00, u = 0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwfT4YJshNXK",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBYkvW1x8fJh",
        "colab_type": "code",
        "outputId": "cd98007f-8247-41e3-bb29-d240cfbbc8a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "def plot(costs, temperatures):\n",
        "  plt.plot(costs)\n",
        "  plt.ylabel('Costs')\n",
        "  plt.show()\n",
        "\n",
        "plot(costs, temperatures)\n",
        "print('================== Initial model ==================')\n",
        "evaluate_model_state(initial_weights)\n",
        "print('================== Final model ==================')\n",
        "evaluate_model_state(states[-1])\n",
        "print('=================================================')\n",
        "print(f'Duration: {(finish_time - start_time):.3f} s')"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZnAqAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "================== Initial model ==================\n",
            "Train loss: 0.858, accuracy: 0.357\n",
            "Test loss: 1.002, accuracy: 0.263\n",
            "================== Final model ==================\n",
            "Train loss: 0.560, accuracy: 0.821\n",
            "Test loss: 0.587, accuracy: 0.842\n",
            "=================================================\n",
            "Duration: 3.356 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3DhAffHhVYg",
        "colab_type": "text"
      },
      "source": [
        "## Using backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL96v1-ShfMd",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7yVMuTJkAq0",
        "colab_type": "code",
        "outputId": "42da99b1-140e-4759-d0bd-db90095f88dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.set_weights(initial_weights)\n",
        "start_time = time.time()\n",
        "model.fit(X_train, y_train, batch_size=X_train.shape[0], epochs=num_steps-1, shuffle=False)\n",
        "finish_time = time.time()\n",
        "final_weights = model.get_weights()"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 84 samples\n",
            "Epoch 1/917\n",
            "84/84 [==============================] - 0s 40us/sample - loss: 0.8581 - acc: 0.3571\n",
            "Epoch 2/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.8569 - acc: 0.3571\n",
            "Epoch 3/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.8557 - acc: 0.3571\n",
            "Epoch 4/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.8546 - acc: 0.3571\n",
            "Epoch 5/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.8534 - acc: 0.3571\n",
            "Epoch 6/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.8523 - acc: 0.3571\n",
            "Epoch 7/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.8512 - acc: 0.3571\n",
            "Epoch 8/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.8501 - acc: 0.3571\n",
            "Epoch 9/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.8490 - acc: 0.3571\n",
            "Epoch 10/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.8479 - acc: 0.3571\n",
            "Epoch 11/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.8469 - acc: 0.3571\n",
            "Epoch 12/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.8458 - acc: 0.3571\n",
            "Epoch 13/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.8447 - acc: 0.3571\n",
            "Epoch 14/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.8437 - acc: 0.3571\n",
            "Epoch 15/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.8427 - acc: 0.3571\n",
            "Epoch 16/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.8416 - acc: 0.3571\n",
            "Epoch 17/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.8406 - acc: 0.3571\n",
            "Epoch 18/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.8396 - acc: 0.3571\n",
            "Epoch 19/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.8386 - acc: 0.3571\n",
            "Epoch 20/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.8376 - acc: 0.3571\n",
            "Epoch 21/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.8366 - acc: 0.3571\n",
            "Epoch 22/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.8356 - acc: 0.3571\n",
            "Epoch 23/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.8347 - acc: 0.3571\n",
            "Epoch 24/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.8337 - acc: 0.3571\n",
            "Epoch 25/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.8327 - acc: 0.3571\n",
            "Epoch 26/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.8318 - acc: 0.3571\n",
            "Epoch 27/917\n",
            "84/84 [==============================] - 0s 17us/sample - loss: 0.8309 - acc: 0.3571\n",
            "Epoch 28/917\n",
            "84/84 [==============================] - 0s 16us/sample - loss: 0.8299 - acc: 0.3571\n",
            "Epoch 29/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.8290 - acc: 0.3571\n",
            "Epoch 30/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.8281 - acc: 0.3571\n",
            "Epoch 31/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.8272 - acc: 0.3571\n",
            "Epoch 32/917\n",
            "84/84 [==============================] - 0s 17us/sample - loss: 0.8263 - acc: 0.3571\n",
            "Epoch 33/917\n",
            "84/84 [==============================] - 0s 17us/sample - loss: 0.8254 - acc: 0.3571\n",
            "Epoch 34/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.8245 - acc: 0.3571\n",
            "Epoch 35/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.8236 - acc: 0.3571\n",
            "Epoch 36/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.8228 - acc: 0.3571\n",
            "Epoch 37/917\n",
            "84/84 [==============================] - 0s 16us/sample - loss: 0.8219 - acc: 0.3571\n",
            "Epoch 38/917\n",
            "84/84 [==============================] - 0s 14us/sample - loss: 0.8210 - acc: 0.3571\n",
            "Epoch 39/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.8202 - acc: 0.3571\n",
            "Epoch 40/917\n",
            "84/84 [==============================] - 0s 16us/sample - loss: 0.8193 - acc: 0.3571\n",
            "Epoch 41/917\n",
            "84/84 [==============================] - 0s 17us/sample - loss: 0.8185 - acc: 0.3571\n",
            "Epoch 42/917\n",
            "84/84 [==============================] - 0s 17us/sample - loss: 0.8177 - acc: 0.3571\n",
            "Epoch 43/917\n",
            "84/84 [==============================] - 0s 17us/sample - loss: 0.8168 - acc: 0.3571\n",
            "Epoch 44/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.8160 - acc: 0.3571\n",
            "Epoch 45/917\n",
            "84/84 [==============================] - 0s 17us/sample - loss: 0.8152 - acc: 0.3571\n",
            "Epoch 46/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.8144 - acc: 0.3571\n",
            "Epoch 47/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.8136 - acc: 0.3571\n",
            "Epoch 48/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.8128 - acc: 0.3571\n",
            "Epoch 49/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.8120 - acc: 0.3571\n",
            "Epoch 50/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.8112 - acc: 0.3571\n",
            "Epoch 51/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.8105 - acc: 0.3571\n",
            "Epoch 52/917\n",
            "84/84 [==============================] - 0s 16us/sample - loss: 0.8097 - acc: 0.3571\n",
            "Epoch 53/917\n",
            "84/84 [==============================] - 0s 15us/sample - loss: 0.8089 - acc: 0.3571\n",
            "Epoch 54/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.8082 - acc: 0.3571\n",
            "Epoch 55/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.8074 - acc: 0.3571\n",
            "Epoch 56/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.8067 - acc: 0.3571\n",
            "Epoch 57/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.8060 - acc: 0.3571\n",
            "Epoch 58/917\n",
            "84/84 [==============================] - 0s 17us/sample - loss: 0.8053 - acc: 0.3571\n",
            "Epoch 59/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.8046 - acc: 0.3571\n",
            "Epoch 60/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.8039 - acc: 0.3571\n",
            "Epoch 61/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.8033 - acc: 0.3571\n",
            "Epoch 62/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.8027 - acc: 0.3571\n",
            "Epoch 63/917\n",
            "84/84 [==============================] - 0s 16us/sample - loss: 0.8021 - acc: 0.3571\n",
            "Epoch 64/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.8014 - acc: 0.3571\n",
            "Epoch 65/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.8008 - acc: 0.3571\n",
            "Epoch 66/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.8002 - acc: 0.3571\n",
            "Epoch 67/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.7997 - acc: 0.3571\n",
            "Epoch 68/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7991 - acc: 0.3571\n",
            "Epoch 69/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7985 - acc: 0.3571\n",
            "Epoch 70/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7980 - acc: 0.3571\n",
            "Epoch 71/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7974 - acc: 0.3571\n",
            "Epoch 72/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7969 - acc: 0.3571\n",
            "Epoch 73/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7963 - acc: 0.3571\n",
            "Epoch 74/917\n",
            "84/84 [==============================] - 0s 15us/sample - loss: 0.7958 - acc: 0.3571\n",
            "Epoch 75/917\n",
            "84/84 [==============================] - 0s 41us/sample - loss: 0.7953 - acc: 0.3571\n",
            "Epoch 76/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.7947 - acc: 0.3571\n",
            "Epoch 77/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7942 - acc: 0.3571\n",
            "Epoch 78/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7937 - acc: 0.3571\n",
            "Epoch 79/917\n",
            "84/84 [==============================] - 0s 17us/sample - loss: 0.7932 - acc: 0.3571\n",
            "Epoch 80/917\n",
            "84/84 [==============================] - 0s 14us/sample - loss: 0.7926 - acc: 0.3571\n",
            "Epoch 81/917\n",
            "84/84 [==============================] - 0s 15us/sample - loss: 0.7921 - acc: 0.3571\n",
            "Epoch 82/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7916 - acc: 0.3571\n",
            "Epoch 83/917\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.7911 - acc: 0.3571\n",
            "Epoch 84/917\n",
            "84/84 [==============================] - 0s 46us/sample - loss: 0.7906 - acc: 0.3571\n",
            "Epoch 85/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7901 - acc: 0.3571\n",
            "Epoch 86/917\n",
            "84/84 [==============================] - 0s 71us/sample - loss: 0.7896 - acc: 0.3571\n",
            "Epoch 87/917\n",
            "84/84 [==============================] - 0s 15us/sample - loss: 0.7891 - acc: 0.3571\n",
            "Epoch 88/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7886 - acc: 0.3571\n",
            "Epoch 89/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7881 - acc: 0.3571\n",
            "Epoch 90/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7876 - acc: 0.3571\n",
            "Epoch 91/917\n",
            "84/84 [==============================] - 0s 41us/sample - loss: 0.7871 - acc: 0.3571\n",
            "Epoch 92/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7866 - acc: 0.3571\n",
            "Epoch 93/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7862 - acc: 0.3571\n",
            "Epoch 94/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7857 - acc: 0.3571\n",
            "Epoch 95/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7852 - acc: 0.3571\n",
            "Epoch 96/917\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.7847 - acc: 0.3571\n",
            "Epoch 97/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7843 - acc: 0.3571\n",
            "Epoch 98/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7838 - acc: 0.3571\n",
            "Epoch 99/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7834 - acc: 0.3571\n",
            "Epoch 100/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.7830 - acc: 0.3571\n",
            "Epoch 101/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7826 - acc: 0.3571\n",
            "Epoch 102/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7821 - acc: 0.3571\n",
            "Epoch 103/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7817 - acc: 0.3571\n",
            "Epoch 104/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7813 - acc: 0.3571\n",
            "Epoch 105/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7809 - acc: 0.3571\n",
            "Epoch 106/917\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.7805 - acc: 0.3571\n",
            "Epoch 107/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7801 - acc: 0.3571\n",
            "Epoch 108/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7797 - acc: 0.3571\n",
            "Epoch 109/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7794 - acc: 0.3571\n",
            "Epoch 110/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7790 - acc: 0.3571\n",
            "Epoch 111/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7786 - acc: 0.3571\n",
            "Epoch 112/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7782 - acc: 0.3571\n",
            "Epoch 113/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7778 - acc: 0.3571\n",
            "Epoch 114/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7775 - acc: 0.3571\n",
            "Epoch 115/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7771 - acc: 0.3571\n",
            "Epoch 116/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7767 - acc: 0.3571\n",
            "Epoch 117/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7764 - acc: 0.3571\n",
            "Epoch 118/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7760 - acc: 0.3571\n",
            "Epoch 119/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7757 - acc: 0.3571\n",
            "Epoch 120/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7753 - acc: 0.3571\n",
            "Epoch 121/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7749 - acc: 0.3571\n",
            "Epoch 122/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7746 - acc: 0.3571\n",
            "Epoch 123/917\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.7742 - acc: 0.3571\n",
            "Epoch 124/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7739 - acc: 0.3571\n",
            "Epoch 125/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7736 - acc: 0.3571\n",
            "Epoch 126/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7732 - acc: 0.3571\n",
            "Epoch 127/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7729 - acc: 0.3571\n",
            "Epoch 128/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7725 - acc: 0.3690\n",
            "Epoch 129/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7722 - acc: 0.3690\n",
            "Epoch 130/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.7719 - acc: 0.3690\n",
            "Epoch 131/917\n",
            "84/84 [==============================] - 0s 15us/sample - loss: 0.7715 - acc: 0.3810\n",
            "Epoch 132/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.7712 - acc: 0.3810\n",
            "Epoch 133/917\n",
            "84/84 [==============================] - 0s 16us/sample - loss: 0.7709 - acc: 0.3810\n",
            "Epoch 134/917\n",
            "84/84 [==============================] - 0s 16us/sample - loss: 0.7705 - acc: 0.3810\n",
            "Epoch 135/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7702 - acc: 0.3810\n",
            "Epoch 136/917\n",
            "84/84 [==============================] - 0s 16us/sample - loss: 0.7699 - acc: 0.3810\n",
            "Epoch 137/917\n",
            "84/84 [==============================] - 0s 17us/sample - loss: 0.7695 - acc: 0.3810\n",
            "Epoch 138/917\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.7692 - acc: 0.3810\n",
            "Epoch 139/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7689 - acc: 0.3810\n",
            "Epoch 140/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7686 - acc: 0.3810\n",
            "Epoch 141/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7683 - acc: 0.3929\n",
            "Epoch 142/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7680 - acc: 0.3929\n",
            "Epoch 143/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7677 - acc: 0.3929\n",
            "Epoch 144/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7673 - acc: 0.3929\n",
            "Epoch 145/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7670 - acc: 0.3929\n",
            "Epoch 146/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7667 - acc: 0.3929\n",
            "Epoch 147/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7664 - acc: 0.3929\n",
            "Epoch 148/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7661 - acc: 0.3810\n",
            "Epoch 149/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7658 - acc: 0.3810\n",
            "Epoch 150/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7655 - acc: 0.3810\n",
            "Epoch 151/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7652 - acc: 0.3810\n",
            "Epoch 152/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7649 - acc: 0.3810\n",
            "Epoch 153/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.7646 - acc: 0.3810\n",
            "Epoch 154/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7643 - acc: 0.3810\n",
            "Epoch 155/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7640 - acc: 0.3810\n",
            "Epoch 156/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7637 - acc: 0.3810\n",
            "Epoch 157/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.7634 - acc: 0.3810\n",
            "Epoch 158/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7632 - acc: 0.3810\n",
            "Epoch 159/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7629 - acc: 0.3810\n",
            "Epoch 160/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7626 - acc: 0.3810\n",
            "Epoch 161/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7623 - acc: 0.3810\n",
            "Epoch 162/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7620 - acc: 0.3810\n",
            "Epoch 163/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.7618 - acc: 0.3810\n",
            "Epoch 164/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7615 - acc: 0.3810\n",
            "Epoch 165/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7612 - acc: 0.3810\n",
            "Epoch 166/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7609 - acc: 0.3810\n",
            "Epoch 167/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7607 - acc: 0.3810\n",
            "Epoch 168/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7604 - acc: 0.3810\n",
            "Epoch 169/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7601 - acc: 0.3810\n",
            "Epoch 170/917\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.7599 - acc: 0.3810\n",
            "Epoch 171/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7596 - acc: 0.3810\n",
            "Epoch 172/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7593 - acc: 0.3810\n",
            "Epoch 173/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7591 - acc: 0.3810\n",
            "Epoch 174/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7588 - acc: 0.3810\n",
            "Epoch 175/917\n",
            "84/84 [==============================] - 0s 41us/sample - loss: 0.7586 - acc: 0.3810\n",
            "Epoch 176/917\n",
            "84/84 [==============================] - 0s 39us/sample - loss: 0.7583 - acc: 0.3810\n",
            "Epoch 177/917\n",
            "84/84 [==============================] - 0s 70us/sample - loss: 0.7580 - acc: 0.3929\n",
            "Epoch 178/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.7578 - acc: 0.3929\n",
            "Epoch 179/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.7575 - acc: 0.3929\n",
            "Epoch 180/917\n",
            "84/84 [==============================] - 0s 39us/sample - loss: 0.7573 - acc: 0.3929\n",
            "Epoch 181/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7570 - acc: 0.3929\n",
            "Epoch 182/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7568 - acc: 0.3929\n",
            "Epoch 183/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7565 - acc: 0.3929\n",
            "Epoch 184/917\n",
            "84/84 [==============================] - 0s 16us/sample - loss: 0.7563 - acc: 0.3929\n",
            "Epoch 185/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7560 - acc: 0.3929\n",
            "Epoch 186/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7558 - acc: 0.3929\n",
            "Epoch 187/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.7556 - acc: 0.3929\n",
            "Epoch 188/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7553 - acc: 0.3929\n",
            "Epoch 189/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7551 - acc: 0.3929\n",
            "Epoch 190/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7548 - acc: 0.3929\n",
            "Epoch 191/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7546 - acc: 0.3929\n",
            "Epoch 192/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7543 - acc: 0.3929\n",
            "Epoch 193/917\n",
            "84/84 [==============================] - 0s 41us/sample - loss: 0.7541 - acc: 0.3810\n",
            "Epoch 194/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7538 - acc: 0.3810\n",
            "Epoch 195/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7536 - acc: 0.3810\n",
            "Epoch 196/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7534 - acc: 0.3810\n",
            "Epoch 197/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7531 - acc: 0.3810\n",
            "Epoch 198/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7529 - acc: 0.3810\n",
            "Epoch 199/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7527 - acc: 0.3810\n",
            "Epoch 200/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7524 - acc: 0.3810\n",
            "Epoch 201/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7522 - acc: 0.3929\n",
            "Epoch 202/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7520 - acc: 0.3929\n",
            "Epoch 203/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7517 - acc: 0.3929\n",
            "Epoch 204/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7515 - acc: 0.3929\n",
            "Epoch 205/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7513 - acc: 0.3929\n",
            "Epoch 206/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7511 - acc: 0.3929\n",
            "Epoch 207/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7508 - acc: 0.3929\n",
            "Epoch 208/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7506 - acc: 0.3929\n",
            "Epoch 209/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7504 - acc: 0.3929\n",
            "Epoch 210/917\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.7502 - acc: 0.3929\n",
            "Epoch 211/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7500 - acc: 0.3929\n",
            "Epoch 212/917\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.7498 - acc: 0.3929\n",
            "Epoch 213/917\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.7495 - acc: 0.3929\n",
            "Epoch 214/917\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.7493 - acc: 0.3929\n",
            "Epoch 215/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7491 - acc: 0.3929\n",
            "Epoch 216/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7489 - acc: 0.3929\n",
            "Epoch 217/917\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.7487 - acc: 0.3929\n",
            "Epoch 218/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7485 - acc: 0.3929\n",
            "Epoch 219/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7483 - acc: 0.3929\n",
            "Epoch 220/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.7481 - acc: 0.3929\n",
            "Epoch 221/917\n",
            "84/84 [==============================] - 0s 40us/sample - loss: 0.7479 - acc: 0.3929\n",
            "Epoch 222/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7477 - acc: 0.3929\n",
            "Epoch 223/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.7474 - acc: 0.3929\n",
            "Epoch 224/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7472 - acc: 0.3929\n",
            "Epoch 225/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.7470 - acc: 0.3929\n",
            "Epoch 226/917\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.7468 - acc: 0.3929\n",
            "Epoch 227/917\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.7466 - acc: 0.3929\n",
            "Epoch 228/917\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.7464 - acc: 0.3929\n",
            "Epoch 229/917\n",
            "84/84 [==============================] - 0s 40us/sample - loss: 0.7462 - acc: 0.3929\n",
            "Epoch 230/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.7460 - acc: 0.3929\n",
            "Epoch 231/917\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.7458 - acc: 0.3929\n",
            "Epoch 232/917\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.7456 - acc: 0.3929\n",
            "Epoch 233/917\n",
            "84/84 [==============================] - 0s 41us/sample - loss: 0.7454 - acc: 0.3929\n",
            "Epoch 234/917\n",
            "84/84 [==============================] - 0s 42us/sample - loss: 0.7452 - acc: 0.3929\n",
            "Epoch 235/917\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.7450 - acc: 0.3929\n",
            "Epoch 236/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7448 - acc: 0.3929\n",
            "Epoch 237/917\n",
            "84/84 [==============================] - 0s 40us/sample - loss: 0.7446 - acc: 0.3929\n",
            "Epoch 238/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7444 - acc: 0.3929\n",
            "Epoch 239/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7442 - acc: 0.3929\n",
            "Epoch 240/917\n",
            "84/84 [==============================] - 0s 39us/sample - loss: 0.7440 - acc: 0.3929\n",
            "Epoch 241/917\n",
            "84/84 [==============================] - 0s 43us/sample - loss: 0.7438 - acc: 0.3929\n",
            "Epoch 242/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7436 - acc: 0.3929\n",
            "Epoch 243/917\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.7434 - acc: 0.3929\n",
            "Epoch 244/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7432 - acc: 0.3929\n",
            "Epoch 245/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7430 - acc: 0.3929\n",
            "Epoch 246/917\n",
            "84/84 [==============================] - 0s 16us/sample - loss: 0.7428 - acc: 0.4286\n",
            "Epoch 247/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7427 - acc: 0.4405\n",
            "Epoch 248/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7425 - acc: 0.4405\n",
            "Epoch 249/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7423 - acc: 0.4405\n",
            "Epoch 250/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7421 - acc: 0.4405\n",
            "Epoch 251/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7419 - acc: 0.4405\n",
            "Epoch 252/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7417 - acc: 0.4405\n",
            "Epoch 253/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7415 - acc: 0.4643\n",
            "Epoch 254/917\n",
            "84/84 [==============================] - 0s 15us/sample - loss: 0.7413 - acc: 0.4643\n",
            "Epoch 255/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.7411 - acc: 0.4643\n",
            "Epoch 256/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.7410 - acc: 0.4643\n",
            "Epoch 257/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7408 - acc: 0.4643\n",
            "Epoch 258/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7406 - acc: 0.4643\n",
            "Epoch 259/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7404 - acc: 0.4643\n",
            "Epoch 260/917\n",
            "84/84 [==============================] - 0s 15us/sample - loss: 0.7402 - acc: 0.4643\n",
            "Epoch 261/917\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.7400 - acc: 0.4643\n",
            "Epoch 262/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7398 - acc: 0.4643\n",
            "Epoch 263/917\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.7397 - acc: 0.4643\n",
            "Epoch 264/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.7395 - acc: 0.4643\n",
            "Epoch 265/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.7393 - acc: 0.4643\n",
            "Epoch 266/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7391 - acc: 0.4643\n",
            "Epoch 267/917\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.7390 - acc: 0.4643\n",
            "Epoch 268/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7388 - acc: 0.4643\n",
            "Epoch 269/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7386 - acc: 0.4762\n",
            "Epoch 270/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7384 - acc: 0.4762\n",
            "Epoch 271/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7382 - acc: 0.4762\n",
            "Epoch 272/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7381 - acc: 0.4762\n",
            "Epoch 273/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7379 - acc: 0.4762\n",
            "Epoch 274/917\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.7377 - acc: 0.4762\n",
            "Epoch 275/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7375 - acc: 0.4762\n",
            "Epoch 276/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7374 - acc: 0.4762\n",
            "Epoch 277/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7372 - acc: 0.4762\n",
            "Epoch 278/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7370 - acc: 0.4762\n",
            "Epoch 279/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7368 - acc: 0.4762\n",
            "Epoch 280/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.7367 - acc: 0.4762\n",
            "Epoch 281/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7365 - acc: 0.4762\n",
            "Epoch 282/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7363 - acc: 0.4762\n",
            "Epoch 283/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7362 - acc: 0.4762\n",
            "Epoch 284/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7360 - acc: 0.4762\n",
            "Epoch 285/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7358 - acc: 0.4762\n",
            "Epoch 286/917\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.7356 - acc: 0.4762\n",
            "Epoch 287/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7355 - acc: 0.4762\n",
            "Epoch 288/917\n",
            "84/84 [==============================] - 0s 40us/sample - loss: 0.7353 - acc: 0.4762\n",
            "Epoch 289/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.7351 - acc: 0.4762\n",
            "Epoch 290/917\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.7349 - acc: 0.4762\n",
            "Epoch 291/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.7348 - acc: 0.4762\n",
            "Epoch 292/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.7346 - acc: 0.4762\n",
            "Epoch 293/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7344 - acc: 0.4762\n",
            "Epoch 294/917\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.7343 - acc: 0.4762\n",
            "Epoch 295/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7341 - acc: 0.4762\n",
            "Epoch 296/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7339 - acc: 0.4762\n",
            "Epoch 297/917\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.7337 - acc: 0.4762\n",
            "Epoch 298/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.7336 - acc: 0.4762\n",
            "Epoch 299/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7334 - acc: 0.4762\n",
            "Epoch 300/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7332 - acc: 0.4881\n",
            "Epoch 301/917\n",
            "84/84 [==============================] - 0s 47us/sample - loss: 0.7331 - acc: 0.4881\n",
            "Epoch 302/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7329 - acc: 0.4881\n",
            "Epoch 303/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7327 - acc: 0.4881\n",
            "Epoch 304/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.7325 - acc: 0.4881\n",
            "Epoch 305/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.7324 - acc: 0.4881\n",
            "Epoch 306/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7322 - acc: 0.4881\n",
            "Epoch 307/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7320 - acc: 0.4881\n",
            "Epoch 308/917\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.7319 - acc: 0.4881\n",
            "Epoch 309/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7317 - acc: 0.4881\n",
            "Epoch 310/917\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.7315 - acc: 0.4881\n",
            "Epoch 311/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7313 - acc: 0.4881\n",
            "Epoch 312/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7312 - acc: 0.4881\n",
            "Epoch 313/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7310 - acc: 0.4881\n",
            "Epoch 314/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7308 - acc: 0.4881\n",
            "Epoch 315/917\n",
            "84/84 [==============================] - 0s 16us/sample - loss: 0.7307 - acc: 0.4881\n",
            "Epoch 316/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7305 - acc: 0.4881\n",
            "Epoch 317/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7303 - acc: 0.4881\n",
            "Epoch 318/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.7302 - acc: 0.4881\n",
            "Epoch 319/917\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.7300 - acc: 0.4881\n",
            "Epoch 320/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7299 - acc: 0.4881\n",
            "Epoch 321/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7297 - acc: 0.4881\n",
            "Epoch 322/917\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.7295 - acc: 0.4881\n",
            "Epoch 323/917\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.7294 - acc: 0.4881\n",
            "Epoch 324/917\n",
            "84/84 [==============================] - 0s 45us/sample - loss: 0.7292 - acc: 0.4881\n",
            "Epoch 325/917\n",
            "84/84 [==============================] - 0s 52us/sample - loss: 0.7290 - acc: 0.4881\n",
            "Epoch 326/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.7289 - acc: 0.4881\n",
            "Epoch 327/917\n",
            "84/84 [==============================] - 0s 39us/sample - loss: 0.7287 - acc: 0.4881\n",
            "Epoch 328/917\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.7285 - acc: 0.4881\n",
            "Epoch 329/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.7284 - acc: 0.4881\n",
            "Epoch 330/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7282 - acc: 0.4881\n",
            "Epoch 331/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.7281 - acc: 0.4881\n",
            "Epoch 332/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7279 - acc: 0.4881\n",
            "Epoch 333/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7277 - acc: 0.4881\n",
            "Epoch 334/917\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.7276 - acc: 0.4881\n",
            "Epoch 335/917\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.7274 - acc: 0.4881\n",
            "Epoch 336/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7272 - acc: 0.5000\n",
            "Epoch 337/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7271 - acc: 0.5000\n",
            "Epoch 338/917\n",
            "84/84 [==============================] - 0s 44us/sample - loss: 0.7269 - acc: 0.5000\n",
            "Epoch 339/917\n",
            "84/84 [==============================] - 0s 45us/sample - loss: 0.7267 - acc: 0.5119\n",
            "Epoch 340/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.7266 - acc: 0.5119\n",
            "Epoch 341/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7264 - acc: 0.5119\n",
            "Epoch 342/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7262 - acc: 0.5119\n",
            "Epoch 343/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7261 - acc: 0.5119\n",
            "Epoch 344/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7259 - acc: 0.5238\n",
            "Epoch 345/917\n",
            "84/84 [==============================] - 0s 40us/sample - loss: 0.7257 - acc: 0.5357\n",
            "Epoch 346/917\n",
            "84/84 [==============================] - 0s 39us/sample - loss: 0.7256 - acc: 0.5357\n",
            "Epoch 347/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7254 - acc: 0.5357\n",
            "Epoch 348/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7252 - acc: 0.5357\n",
            "Epoch 349/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.7250 - acc: 0.5476\n",
            "Epoch 350/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7249 - acc: 0.5476\n",
            "Epoch 351/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7247 - acc: 0.5476\n",
            "Epoch 352/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7245 - acc: 0.5476\n",
            "Epoch 353/917\n",
            "84/84 [==============================] - 0s 43us/sample - loss: 0.7244 - acc: 0.5595\n",
            "Epoch 354/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7242 - acc: 0.5595\n",
            "Epoch 355/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7240 - acc: 0.5595\n",
            "Epoch 356/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7238 - acc: 0.5595\n",
            "Epoch 357/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7237 - acc: 0.5595\n",
            "Epoch 358/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.7235 - acc: 0.5595\n",
            "Epoch 359/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7233 - acc: 0.5595\n",
            "Epoch 360/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7232 - acc: 0.5595\n",
            "Epoch 361/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7230 - acc: 0.5595\n",
            "Epoch 362/917\n",
            "84/84 [==============================] - 0s 41us/sample - loss: 0.7228 - acc: 0.5595\n",
            "Epoch 363/917\n",
            "84/84 [==============================] - 0s 41us/sample - loss: 0.7226 - acc: 0.5595\n",
            "Epoch 364/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7225 - acc: 0.5595\n",
            "Epoch 365/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7223 - acc: 0.5595\n",
            "Epoch 366/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7221 - acc: 0.5595\n",
            "Epoch 367/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7219 - acc: 0.5595\n",
            "Epoch 368/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7218 - acc: 0.5595\n",
            "Epoch 369/917\n",
            "84/84 [==============================] - 0s 40us/sample - loss: 0.7216 - acc: 0.5595\n",
            "Epoch 370/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7214 - acc: 0.5595\n",
            "Epoch 371/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7212 - acc: 0.5595\n",
            "Epoch 372/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7211 - acc: 0.5595\n",
            "Epoch 373/917\n",
            "84/84 [==============================] - 0s 43us/sample - loss: 0.7209 - acc: 0.5595\n",
            "Epoch 374/917\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.7207 - acc: 0.5595\n",
            "Epoch 375/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.7205 - acc: 0.5595\n",
            "Epoch 376/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7204 - acc: 0.5595\n",
            "Epoch 377/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.7202 - acc: 0.5595\n",
            "Epoch 378/917\n",
            "84/84 [==============================] - 0s 15us/sample - loss: 0.7200 - acc: 0.5595\n",
            "Epoch 379/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7198 - acc: 0.5595\n",
            "Epoch 380/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7197 - acc: 0.5595\n",
            "Epoch 381/917\n",
            "84/84 [==============================] - 0s 47us/sample - loss: 0.7195 - acc: 0.5595\n",
            "Epoch 382/917\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.7193 - acc: 0.5595\n",
            "Epoch 383/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7192 - acc: 0.5595\n",
            "Epoch 384/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7190 - acc: 0.5595\n",
            "Epoch 385/917\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.7188 - acc: 0.5595\n",
            "Epoch 386/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7186 - acc: 0.5595\n",
            "Epoch 387/917\n",
            "84/84 [==============================] - 0s 41us/sample - loss: 0.7185 - acc: 0.5595\n",
            "Epoch 388/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7183 - acc: 0.5595\n",
            "Epoch 389/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7181 - acc: 0.5595\n",
            "Epoch 390/917\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.7179 - acc: 0.5595\n",
            "Epoch 391/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7178 - acc: 0.5595\n",
            "Epoch 392/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7176 - acc: 0.5595\n",
            "Epoch 393/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7174 - acc: 0.5595\n",
            "Epoch 394/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.7172 - acc: 0.5595\n",
            "Epoch 395/917\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.7171 - acc: 0.5595\n",
            "Epoch 396/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7169 - acc: 0.5595\n",
            "Epoch 397/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.7167 - acc: 0.5595\n",
            "Epoch 398/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7165 - acc: 0.5595\n",
            "Epoch 399/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7164 - acc: 0.5595\n",
            "Epoch 400/917\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.7162 - acc: 0.5595\n",
            "Epoch 401/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.7160 - acc: 0.5595\n",
            "Epoch 402/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7158 - acc: 0.5595\n",
            "Epoch 403/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7157 - acc: 0.5595\n",
            "Epoch 404/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7155 - acc: 0.5595\n",
            "Epoch 405/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7153 - acc: 0.5595\n",
            "Epoch 406/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7151 - acc: 0.5595\n",
            "Epoch 407/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7150 - acc: 0.5595\n",
            "Epoch 408/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7148 - acc: 0.5595\n",
            "Epoch 409/917\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.7146 - acc: 0.5595\n",
            "Epoch 410/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7144 - acc: 0.5595\n",
            "Epoch 411/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7142 - acc: 0.5595\n",
            "Epoch 412/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.7141 - acc: 0.5595\n",
            "Epoch 413/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7139 - acc: 0.5595\n",
            "Epoch 414/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7137 - acc: 0.5595\n",
            "Epoch 415/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7135 - acc: 0.5595\n",
            "Epoch 416/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.7133 - acc: 0.5595\n",
            "Epoch 417/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7132 - acc: 0.5595\n",
            "Epoch 418/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7130 - acc: 0.5595\n",
            "Epoch 419/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7128 - acc: 0.5595\n",
            "Epoch 420/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7126 - acc: 0.5595\n",
            "Epoch 421/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7124 - acc: 0.5714\n",
            "Epoch 422/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.7123 - acc: 0.5714\n",
            "Epoch 423/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7121 - acc: 0.5714\n",
            "Epoch 424/917\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.7119 - acc: 0.5714\n",
            "Epoch 425/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7117 - acc: 0.5714\n",
            "Epoch 426/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.7115 - acc: 0.5714\n",
            "Epoch 427/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7113 - acc: 0.5714\n",
            "Epoch 428/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7111 - acc: 0.5714\n",
            "Epoch 429/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7109 - acc: 0.5714\n",
            "Epoch 430/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.7107 - acc: 0.5714\n",
            "Epoch 431/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7106 - acc: 0.5714\n",
            "Epoch 432/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7104 - acc: 0.5714\n",
            "Epoch 433/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7102 - acc: 0.5714\n",
            "Epoch 434/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7100 - acc: 0.5714\n",
            "Epoch 435/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7098 - acc: 0.5714\n",
            "Epoch 436/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7096 - acc: 0.5714\n",
            "Epoch 437/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.7094 - acc: 0.5714\n",
            "Epoch 438/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7092 - acc: 0.5714\n",
            "Epoch 439/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7090 - acc: 0.5714\n",
            "Epoch 440/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7088 - acc: 0.5714\n",
            "Epoch 441/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7086 - acc: 0.5714\n",
            "Epoch 442/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7084 - acc: 0.5714\n",
            "Epoch 443/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7083 - acc: 0.5714\n",
            "Epoch 444/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.7081 - acc: 0.5714\n",
            "Epoch 445/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.7079 - acc: 0.5714\n",
            "Epoch 446/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.7077 - acc: 0.5714\n",
            "Epoch 447/917\n",
            "84/84 [==============================] - 0s 16us/sample - loss: 0.7075 - acc: 0.5714\n",
            "Epoch 448/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7073 - acc: 0.5714\n",
            "Epoch 449/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7071 - acc: 0.5714\n",
            "Epoch 450/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7069 - acc: 0.5714\n",
            "Epoch 451/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7067 - acc: 0.5714\n",
            "Epoch 452/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7065 - acc: 0.5714\n",
            "Epoch 453/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.7063 - acc: 0.5714\n",
            "Epoch 454/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7061 - acc: 0.5714\n",
            "Epoch 455/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.7059 - acc: 0.5714\n",
            "Epoch 456/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7057 - acc: 0.5714\n",
            "Epoch 457/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7055 - acc: 0.5714\n",
            "Epoch 458/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.7053 - acc: 0.5714\n",
            "Epoch 459/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7051 - acc: 0.5714\n",
            "Epoch 460/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7049 - acc: 0.5714\n",
            "Epoch 461/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7047 - acc: 0.5714\n",
            "Epoch 462/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.7045 - acc: 0.5714\n",
            "Epoch 463/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7043 - acc: 0.5714\n",
            "Epoch 464/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7041 - acc: 0.5714\n",
            "Epoch 465/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7039 - acc: 0.5714\n",
            "Epoch 466/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7037 - acc: 0.5714\n",
            "Epoch 467/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.7035 - acc: 0.5714\n",
            "Epoch 468/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7033 - acc: 0.5714\n",
            "Epoch 469/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7031 - acc: 0.5714\n",
            "Epoch 470/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7029 - acc: 0.5714\n",
            "Epoch 471/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.7026 - acc: 0.5714\n",
            "Epoch 472/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7024 - acc: 0.5714\n",
            "Epoch 473/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7022 - acc: 0.5714\n",
            "Epoch 474/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7020 - acc: 0.5833\n",
            "Epoch 475/917\n",
            "84/84 [==============================] - 0s 39us/sample - loss: 0.7018 - acc: 0.5833\n",
            "Epoch 476/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7016 - acc: 0.5833\n",
            "Epoch 477/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7014 - acc: 0.5833\n",
            "Epoch 478/917\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.7012 - acc: 0.5833\n",
            "Epoch 479/917\n",
            "84/84 [==============================] - 0s 40us/sample - loss: 0.7010 - acc: 0.5833\n",
            "Epoch 480/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7007 - acc: 0.5833\n",
            "Epoch 481/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7005 - acc: 0.5833\n",
            "Epoch 482/917\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.7003 - acc: 0.5833\n",
            "Epoch 483/917\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.7001 - acc: 0.5833\n",
            "Epoch 484/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.6999 - acc: 0.5833\n",
            "Epoch 485/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6997 - acc: 0.5833\n",
            "Epoch 486/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.6994 - acc: 0.5952\n",
            "Epoch 487/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6992 - acc: 0.5952\n",
            "Epoch 488/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.6990 - acc: 0.5952\n",
            "Epoch 489/917\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.6988 - acc: 0.5952\n",
            "Epoch 490/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6985 - acc: 0.5952\n",
            "Epoch 491/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6983 - acc: 0.5952\n",
            "Epoch 492/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6981 - acc: 0.5952\n",
            "Epoch 493/917\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.6979 - acc: 0.5952\n",
            "Epoch 494/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6976 - acc: 0.5952\n",
            "Epoch 495/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6974 - acc: 0.5952\n",
            "Epoch 496/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.6972 - acc: 0.5952\n",
            "Epoch 497/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6970 - acc: 0.5952\n",
            "Epoch 498/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6967 - acc: 0.5952\n",
            "Epoch 499/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6965 - acc: 0.5952\n",
            "Epoch 500/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6963 - acc: 0.5952\n",
            "Epoch 501/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6960 - acc: 0.5952\n",
            "Epoch 502/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6958 - acc: 0.5952\n",
            "Epoch 503/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6956 - acc: 0.5952\n",
            "Epoch 504/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6953 - acc: 0.5952\n",
            "Epoch 505/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.6951 - acc: 0.5952\n",
            "Epoch 506/917\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.6949 - acc: 0.5952\n",
            "Epoch 507/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6946 - acc: 0.5952\n",
            "Epoch 508/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6944 - acc: 0.5952\n",
            "Epoch 509/917\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.6941 - acc: 0.5952\n",
            "Epoch 510/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6939 - acc: 0.5952\n",
            "Epoch 511/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.6937 - acc: 0.5952\n",
            "Epoch 512/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6934 - acc: 0.5952\n",
            "Epoch 513/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6932 - acc: 0.5952\n",
            "Epoch 514/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6929 - acc: 0.5952\n",
            "Epoch 515/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.6927 - acc: 0.5952\n",
            "Epoch 516/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.6924 - acc: 0.5952\n",
            "Epoch 517/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.6922 - acc: 0.5952\n",
            "Epoch 518/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.6919 - acc: 0.5952\n",
            "Epoch 519/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.6917 - acc: 0.5952\n",
            "Epoch 520/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6914 - acc: 0.5952\n",
            "Epoch 521/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6912 - acc: 0.5952\n",
            "Epoch 522/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6909 - acc: 0.5952\n",
            "Epoch 523/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.6907 - acc: 0.5952\n",
            "Epoch 524/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6904 - acc: 0.5952\n",
            "Epoch 525/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6902 - acc: 0.5952\n",
            "Epoch 526/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6899 - acc: 0.5952\n",
            "Epoch 527/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6897 - acc: 0.5952\n",
            "Epoch 528/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6894 - acc: 0.5952\n",
            "Epoch 529/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.6891 - acc: 0.5952\n",
            "Epoch 530/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6889 - acc: 0.5952\n",
            "Epoch 531/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6886 - acc: 0.5952\n",
            "Epoch 532/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6884 - acc: 0.5952\n",
            "Epoch 533/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6881 - acc: 0.5952\n",
            "Epoch 534/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.6878 - acc: 0.5952\n",
            "Epoch 535/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6876 - acc: 0.5952\n",
            "Epoch 536/917\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.6873 - acc: 0.5952\n",
            "Epoch 537/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6870 - acc: 0.5952\n",
            "Epoch 538/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6868 - acc: 0.5952\n",
            "Epoch 539/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6865 - acc: 0.5952\n",
            "Epoch 540/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6862 - acc: 0.5952\n",
            "Epoch 541/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.6860 - acc: 0.5952\n",
            "Epoch 542/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6857 - acc: 0.5952\n",
            "Epoch 543/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6855 - acc: 0.5952\n",
            "Epoch 544/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.6852 - acc: 0.5952\n",
            "Epoch 545/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6850 - acc: 0.5952\n",
            "Epoch 546/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6847 - acc: 0.5952\n",
            "Epoch 547/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.6845 - acc: 0.5952\n",
            "Epoch 548/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.6842 - acc: 0.5952\n",
            "Epoch 549/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6839 - acc: 0.5952\n",
            "Epoch 550/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6837 - acc: 0.5952\n",
            "Epoch 551/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6834 - acc: 0.5952\n",
            "Epoch 552/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.6831 - acc: 0.5952\n",
            "Epoch 553/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6829 - acc: 0.5952\n",
            "Epoch 554/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6826 - acc: 0.5952\n",
            "Epoch 555/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.6824 - acc: 0.5952\n",
            "Epoch 556/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6821 - acc: 0.5952\n",
            "Epoch 557/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.6818 - acc: 0.5952\n",
            "Epoch 558/917\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.6815 - acc: 0.5952\n",
            "Epoch 559/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.6813 - acc: 0.5952\n",
            "Epoch 560/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6810 - acc: 0.5952\n",
            "Epoch 561/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.6807 - acc: 0.5952\n",
            "Epoch 562/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6805 - acc: 0.5952\n",
            "Epoch 563/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6802 - acc: 0.5952\n",
            "Epoch 564/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.6799 - acc: 0.5952\n",
            "Epoch 565/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.6796 - acc: 0.5952\n",
            "Epoch 566/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6794 - acc: 0.5952\n",
            "Epoch 567/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.6791 - acc: 0.5952\n",
            "Epoch 568/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6788 - acc: 0.5952\n",
            "Epoch 569/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6785 - acc: 0.5952\n",
            "Epoch 570/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.6783 - acc: 0.5952\n",
            "Epoch 571/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.6780 - acc: 0.5952\n",
            "Epoch 572/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6777 - acc: 0.5952\n",
            "Epoch 573/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.6774 - acc: 0.5952\n",
            "Epoch 574/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6771 - acc: 0.5952\n",
            "Epoch 575/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6768 - acc: 0.5952\n",
            "Epoch 576/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6765 - acc: 0.5952\n",
            "Epoch 577/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6763 - acc: 0.5952\n",
            "Epoch 578/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6760 - acc: 0.5952\n",
            "Epoch 579/917\n",
            "84/84 [==============================] - 0s 50us/sample - loss: 0.6757 - acc: 0.5952\n",
            "Epoch 580/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6754 - acc: 0.5952\n",
            "Epoch 581/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6751 - acc: 0.5952\n",
            "Epoch 582/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.6748 - acc: 0.5952\n",
            "Epoch 583/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6745 - acc: 0.5952\n",
            "Epoch 584/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6742 - acc: 0.5952\n",
            "Epoch 585/917\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.6739 - acc: 0.5952\n",
            "Epoch 586/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.6736 - acc: 0.5952\n",
            "Epoch 587/917\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.6733 - acc: 0.5952\n",
            "Epoch 588/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6730 - acc: 0.5952\n",
            "Epoch 589/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6727 - acc: 0.5952\n",
            "Epoch 590/917\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.6725 - acc: 0.6071\n",
            "Epoch 591/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6721 - acc: 0.6071\n",
            "Epoch 592/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6718 - acc: 0.6071\n",
            "Epoch 593/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6715 - acc: 0.6071\n",
            "Epoch 594/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6712 - acc: 0.6071\n",
            "Epoch 595/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.6709 - acc: 0.6071\n",
            "Epoch 596/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.6706 - acc: 0.6071\n",
            "Epoch 597/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.6703 - acc: 0.6190\n",
            "Epoch 598/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6699 - acc: 0.6190\n",
            "Epoch 599/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6696 - acc: 0.6190\n",
            "Epoch 600/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.6692 - acc: 0.6190\n",
            "Epoch 601/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.6689 - acc: 0.6190\n",
            "Epoch 602/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.6686 - acc: 0.6190\n",
            "Epoch 603/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6682 - acc: 0.6190\n",
            "Epoch 604/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.6679 - acc: 0.6190\n",
            "Epoch 605/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6675 - acc: 0.6190\n",
            "Epoch 606/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.6672 - acc: 0.6190\n",
            "Epoch 607/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6669 - acc: 0.6190\n",
            "Epoch 608/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6665 - acc: 0.6190\n",
            "Epoch 609/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.6662 - acc: 0.6190\n",
            "Epoch 610/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6658 - acc: 0.6190\n",
            "Epoch 611/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6655 - acc: 0.6190\n",
            "Epoch 612/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.6651 - acc: 0.6190\n",
            "Epoch 613/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6647 - acc: 0.6190\n",
            "Epoch 614/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6644 - acc: 0.6190\n",
            "Epoch 615/917\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.6640 - acc: 0.6190\n",
            "Epoch 616/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.6637 - acc: 0.6190\n",
            "Epoch 617/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6633 - acc: 0.6190\n",
            "Epoch 618/917\n",
            "84/84 [==============================] - 0s 39us/sample - loss: 0.6630 - acc: 0.6190\n",
            "Epoch 619/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6626 - acc: 0.6190\n",
            "Epoch 620/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6622 - acc: 0.6190\n",
            "Epoch 621/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.6619 - acc: 0.6190\n",
            "Epoch 622/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.6615 - acc: 0.6190\n",
            "Epoch 623/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6612 - acc: 0.6190\n",
            "Epoch 624/917\n",
            "84/84 [==============================] - 0s 39us/sample - loss: 0.6608 - acc: 0.6190\n",
            "Epoch 625/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6604 - acc: 0.6190\n",
            "Epoch 626/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6600 - acc: 0.6190\n",
            "Epoch 627/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6597 - acc: 0.6190\n",
            "Epoch 628/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6593 - acc: 0.6190\n",
            "Epoch 629/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.6589 - acc: 0.6190\n",
            "Epoch 630/917\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.6585 - acc: 0.6190\n",
            "Epoch 631/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6581 - acc: 0.6310\n",
            "Epoch 632/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6577 - acc: 0.6310\n",
            "Epoch 633/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.6573 - acc: 0.6310\n",
            "Epoch 634/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6569 - acc: 0.6310\n",
            "Epoch 635/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6565 - acc: 0.6310\n",
            "Epoch 636/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.6561 - acc: 0.6310\n",
            "Epoch 637/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6557 - acc: 0.6310\n",
            "Epoch 638/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.6553 - acc: 0.6310\n",
            "Epoch 639/917\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.6549 - acc: 0.6310\n",
            "Epoch 640/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6545 - acc: 0.6310\n",
            "Epoch 641/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6541 - acc: 0.6310\n",
            "Epoch 642/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6537 - acc: 0.6310\n",
            "Epoch 643/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6532 - acc: 0.6310\n",
            "Epoch 644/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6528 - acc: 0.6310\n",
            "Epoch 645/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6524 - acc: 0.6310\n",
            "Epoch 646/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6520 - acc: 0.6310\n",
            "Epoch 647/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6516 - acc: 0.6310\n",
            "Epoch 648/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6511 - acc: 0.6310\n",
            "Epoch 649/917\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.6507 - acc: 0.6310\n",
            "Epoch 650/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6503 - acc: 0.6310\n",
            "Epoch 651/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6499 - acc: 0.6310\n",
            "Epoch 652/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6494 - acc: 0.6310\n",
            "Epoch 653/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.6490 - acc: 0.6310\n",
            "Epoch 654/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6486 - acc: 0.6310\n",
            "Epoch 655/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6482 - acc: 0.6310\n",
            "Epoch 656/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6477 - acc: 0.6310\n",
            "Epoch 657/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6473 - acc: 0.6310\n",
            "Epoch 658/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.6469 - acc: 0.6310\n",
            "Epoch 659/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6464 - acc: 0.6310\n",
            "Epoch 660/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6460 - acc: 0.6310\n",
            "Epoch 661/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6455 - acc: 0.6310\n",
            "Epoch 662/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6451 - acc: 0.6310\n",
            "Epoch 663/917\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.6447 - acc: 0.6310\n",
            "Epoch 664/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.6442 - acc: 0.6310\n",
            "Epoch 665/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.6438 - acc: 0.6310\n",
            "Epoch 666/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6433 - acc: 0.6310\n",
            "Epoch 667/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.6429 - acc: 0.6310\n",
            "Epoch 668/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6424 - acc: 0.6310\n",
            "Epoch 669/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6420 - acc: 0.6310\n",
            "Epoch 670/917\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.6415 - acc: 0.6310\n",
            "Epoch 671/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6411 - acc: 0.6310\n",
            "Epoch 672/917\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.6406 - acc: 0.6310\n",
            "Epoch 673/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6401 - acc: 0.6310\n",
            "Epoch 674/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6397 - acc: 0.6310\n",
            "Epoch 675/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6392 - acc: 0.6310\n",
            "Epoch 676/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6388 - acc: 0.6310\n",
            "Epoch 677/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6383 - acc: 0.6310\n",
            "Epoch 678/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6378 - acc: 0.6310\n",
            "Epoch 679/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6373 - acc: 0.6310\n",
            "Epoch 680/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6369 - acc: 0.6310\n",
            "Epoch 681/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6364 - acc: 0.6310\n",
            "Epoch 682/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6359 - acc: 0.6310\n",
            "Epoch 683/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6355 - acc: 0.6310\n",
            "Epoch 684/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.6350 - acc: 0.6310\n",
            "Epoch 685/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6345 - acc: 0.6310\n",
            "Epoch 686/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6340 - acc: 0.6310\n",
            "Epoch 687/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6335 - acc: 0.6310\n",
            "Epoch 688/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6331 - acc: 0.6310\n",
            "Epoch 689/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.6326 - acc: 0.6310\n",
            "Epoch 690/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6321 - acc: 0.6310\n",
            "Epoch 691/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.6316 - acc: 0.6310\n",
            "Epoch 692/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.6311 - acc: 0.6310\n",
            "Epoch 693/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.6306 - acc: 0.6310\n",
            "Epoch 694/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.6301 - acc: 0.6310\n",
            "Epoch 695/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.6296 - acc: 0.6310\n",
            "Epoch 696/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6292 - acc: 0.6310\n",
            "Epoch 697/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6287 - acc: 0.6310\n",
            "Epoch 698/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.6282 - acc: 0.6310\n",
            "Epoch 699/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6277 - acc: 0.6310\n",
            "Epoch 700/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6272 - acc: 0.6310\n",
            "Epoch 701/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6267 - acc: 0.6310\n",
            "Epoch 702/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6262 - acc: 0.6310\n",
            "Epoch 703/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6256 - acc: 0.6310\n",
            "Epoch 704/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.6251 - acc: 0.6310\n",
            "Epoch 705/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6246 - acc: 0.6429\n",
            "Epoch 706/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6240 - acc: 0.6429\n",
            "Epoch 707/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.6235 - acc: 0.6429\n",
            "Epoch 708/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6229 - acc: 0.6429\n",
            "Epoch 709/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6224 - acc: 0.6429\n",
            "Epoch 710/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6219 - acc: 0.6429\n",
            "Epoch 711/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6213 - acc: 0.6429\n",
            "Epoch 712/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.6208 - acc: 0.6429\n",
            "Epoch 713/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6202 - acc: 0.6429\n",
            "Epoch 714/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6197 - acc: 0.6429\n",
            "Epoch 715/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.6191 - acc: 0.6429\n",
            "Epoch 716/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6186 - acc: 0.6429\n",
            "Epoch 717/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.6180 - acc: 0.6429\n",
            "Epoch 718/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.6174 - acc: 0.6429\n",
            "Epoch 719/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6169 - acc: 0.6429\n",
            "Epoch 720/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6163 - acc: 0.6429\n",
            "Epoch 721/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6158 - acc: 0.6429\n",
            "Epoch 722/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6152 - acc: 0.6429\n",
            "Epoch 723/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6146 - acc: 0.6429\n",
            "Epoch 724/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.6141 - acc: 0.6429\n",
            "Epoch 725/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.6135 - acc: 0.6429\n",
            "Epoch 726/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.6130 - acc: 0.6429\n",
            "Epoch 727/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.6124 - acc: 0.6429\n",
            "Epoch 728/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6118 - acc: 0.6429\n",
            "Epoch 729/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6112 - acc: 0.6429\n",
            "Epoch 730/917\n",
            "84/84 [==============================] - 0s 50us/sample - loss: 0.6107 - acc: 0.6429\n",
            "Epoch 731/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6101 - acc: 0.6429\n",
            "Epoch 732/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6095 - acc: 0.6429\n",
            "Epoch 733/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6090 - acc: 0.6429\n",
            "Epoch 734/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6084 - acc: 0.6429\n",
            "Epoch 735/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6078 - acc: 0.6429\n",
            "Epoch 736/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6073 - acc: 0.6429\n",
            "Epoch 737/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.6067 - acc: 0.6429\n",
            "Epoch 738/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.6061 - acc: 0.6429\n",
            "Epoch 739/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6056 - acc: 0.6429\n",
            "Epoch 740/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6050 - acc: 0.6429\n",
            "Epoch 741/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6044 - acc: 0.6429\n",
            "Epoch 742/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6039 - acc: 0.6429\n",
            "Epoch 743/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.6033 - acc: 0.6429\n",
            "Epoch 744/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6027 - acc: 0.6429\n",
            "Epoch 745/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6021 - acc: 0.6429\n",
            "Epoch 746/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.6016 - acc: 0.6429\n",
            "Epoch 747/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.6010 - acc: 0.6429\n",
            "Epoch 748/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.6004 - acc: 0.6429\n",
            "Epoch 749/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5999 - acc: 0.6429\n",
            "Epoch 750/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.5993 - acc: 0.6429\n",
            "Epoch 751/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5987 - acc: 0.6429\n",
            "Epoch 752/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5981 - acc: 0.6429\n",
            "Epoch 753/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.5976 - acc: 0.6429\n",
            "Epoch 754/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5970 - acc: 0.6429\n",
            "Epoch 755/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.5964 - acc: 0.6429\n",
            "Epoch 756/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5958 - acc: 0.6429\n",
            "Epoch 757/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5953 - acc: 0.6429\n",
            "Epoch 758/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5947 - acc: 0.6429\n",
            "Epoch 759/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.5941 - acc: 0.6429\n",
            "Epoch 760/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.5935 - acc: 0.6429\n",
            "Epoch 761/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.5930 - acc: 0.6429\n",
            "Epoch 762/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5924 - acc: 0.6429\n",
            "Epoch 763/917\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.5918 - acc: 0.6429\n",
            "Epoch 764/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.5912 - acc: 0.6429\n",
            "Epoch 765/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5907 - acc: 0.6429\n",
            "Epoch 766/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.5901 - acc: 0.6429\n",
            "Epoch 767/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.5895 - acc: 0.6429\n",
            "Epoch 768/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.5889 - acc: 0.6429\n",
            "Epoch 769/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.5884 - acc: 0.6429\n",
            "Epoch 770/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.5878 - acc: 0.6429\n",
            "Epoch 771/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.5872 - acc: 0.6429\n",
            "Epoch 772/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.5866 - acc: 0.6429\n",
            "Epoch 773/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.5861 - acc: 0.6429\n",
            "Epoch 774/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5855 - acc: 0.6429\n",
            "Epoch 775/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5849 - acc: 0.6429\n",
            "Epoch 776/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.5844 - acc: 0.6548\n",
            "Epoch 777/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.5838 - acc: 0.6548\n",
            "Epoch 778/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.5832 - acc: 0.6548\n",
            "Epoch 779/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.5827 - acc: 0.6548\n",
            "Epoch 780/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5821 - acc: 0.6548\n",
            "Epoch 781/917\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.5816 - acc: 0.6548\n",
            "Epoch 782/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.5810 - acc: 0.6548\n",
            "Epoch 783/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5804 - acc: 0.6548\n",
            "Epoch 784/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5799 - acc: 0.6667\n",
            "Epoch 785/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5793 - acc: 0.6667\n",
            "Epoch 786/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5787 - acc: 0.6667\n",
            "Epoch 787/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5781 - acc: 0.6667\n",
            "Epoch 788/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5775 - acc: 0.6667\n",
            "Epoch 789/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5769 - acc: 0.6667\n",
            "Epoch 790/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5763 - acc: 0.6667\n",
            "Epoch 791/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5757 - acc: 0.6667\n",
            "Epoch 792/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5751 - acc: 0.6667\n",
            "Epoch 793/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.5745 - acc: 0.6667\n",
            "Epoch 794/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.5739 - acc: 0.6667\n",
            "Epoch 795/917\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.5734 - acc: 0.6667\n",
            "Epoch 796/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5728 - acc: 0.6667\n",
            "Epoch 797/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.5722 - acc: 0.6667\n",
            "Epoch 798/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.5716 - acc: 0.6667\n",
            "Epoch 799/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5710 - acc: 0.6667\n",
            "Epoch 800/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.5704 - acc: 0.6667\n",
            "Epoch 801/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.5698 - acc: 0.6667\n",
            "Epoch 802/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.5693 - acc: 0.6667\n",
            "Epoch 803/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5687 - acc: 0.6667\n",
            "Epoch 804/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5681 - acc: 0.6667\n",
            "Epoch 805/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.5675 - acc: 0.6667\n",
            "Epoch 806/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.5669 - acc: 0.6667\n",
            "Epoch 807/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.5664 - acc: 0.6667\n",
            "Epoch 808/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.5658 - acc: 0.6667\n",
            "Epoch 809/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.5652 - acc: 0.6667\n",
            "Epoch 810/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.5646 - acc: 0.6667\n",
            "Epoch 811/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5641 - acc: 0.6667\n",
            "Epoch 812/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5635 - acc: 0.6667\n",
            "Epoch 813/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5629 - acc: 0.6667\n",
            "Epoch 814/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5624 - acc: 0.6667\n",
            "Epoch 815/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.5618 - acc: 0.6667\n",
            "Epoch 816/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5612 - acc: 0.6667\n",
            "Epoch 817/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5607 - acc: 0.6667\n",
            "Epoch 818/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.5601 - acc: 0.6667\n",
            "Epoch 819/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5595 - acc: 0.6667\n",
            "Epoch 820/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.5590 - acc: 0.6667\n",
            "Epoch 821/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5584 - acc: 0.6667\n",
            "Epoch 822/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.5579 - acc: 0.6667\n",
            "Epoch 823/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5573 - acc: 0.6667\n",
            "Epoch 824/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5568 - acc: 0.6667\n",
            "Epoch 825/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5562 - acc: 0.6667\n",
            "Epoch 826/917\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.5557 - acc: 0.6667\n",
            "Epoch 827/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5551 - acc: 0.6667\n",
            "Epoch 828/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5546 - acc: 0.6667\n",
            "Epoch 829/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.5540 - acc: 0.6667\n",
            "Epoch 830/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.5535 - acc: 0.6667\n",
            "Epoch 831/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.5530 - acc: 0.6667\n",
            "Epoch 832/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.5524 - acc: 0.6667\n",
            "Epoch 833/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.5519 - acc: 0.6667\n",
            "Epoch 834/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5514 - acc: 0.6667\n",
            "Epoch 835/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5509 - acc: 0.6667\n",
            "Epoch 836/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.5503 - acc: 0.6667\n",
            "Epoch 837/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5498 - acc: 0.6667\n",
            "Epoch 838/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5493 - acc: 0.6667\n",
            "Epoch 839/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.5488 - acc: 0.6667\n",
            "Epoch 840/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.5483 - acc: 0.6667\n",
            "Epoch 841/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5477 - acc: 0.6667\n",
            "Epoch 842/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.5472 - acc: 0.6667\n",
            "Epoch 843/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.5467 - acc: 0.6667\n",
            "Epoch 844/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.5462 - acc: 0.6667\n",
            "Epoch 845/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5457 - acc: 0.6667\n",
            "Epoch 846/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5452 - acc: 0.6667\n",
            "Epoch 847/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.5447 - acc: 0.6667\n",
            "Epoch 848/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.5442 - acc: 0.6667\n",
            "Epoch 849/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.5437 - acc: 0.6667\n",
            "Epoch 850/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.5432 - acc: 0.6667\n",
            "Epoch 851/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5428 - acc: 0.6667\n",
            "Epoch 852/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5423 - acc: 0.6667\n",
            "Epoch 853/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.5418 - acc: 0.6667\n",
            "Epoch 854/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.5413 - acc: 0.6667\n",
            "Epoch 855/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5408 - acc: 0.6667\n",
            "Epoch 856/917\n",
            "84/84 [==============================] - 0s 39us/sample - loss: 0.5403 - acc: 0.6667\n",
            "Epoch 857/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.5399 - acc: 0.6667\n",
            "Epoch 858/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.5394 - acc: 0.6667\n",
            "Epoch 859/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.5389 - acc: 0.6667\n",
            "Epoch 860/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5385 - acc: 0.6667\n",
            "Epoch 861/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5380 - acc: 0.6667\n",
            "Epoch 862/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5375 - acc: 0.6667\n",
            "Epoch 863/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5371 - acc: 0.6667\n",
            "Epoch 864/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5366 - acc: 0.6667\n",
            "Epoch 865/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5362 - acc: 0.6667\n",
            "Epoch 866/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.5357 - acc: 0.6667\n",
            "Epoch 867/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5353 - acc: 0.6667\n",
            "Epoch 868/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.5348 - acc: 0.6667\n",
            "Epoch 869/917\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.5344 - acc: 0.6667\n",
            "Epoch 870/917\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.5339 - acc: 0.6667\n",
            "Epoch 871/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5335 - acc: 0.6667\n",
            "Epoch 872/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.5331 - acc: 0.6667\n",
            "Epoch 873/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.5326 - acc: 0.6667\n",
            "Epoch 874/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5322 - acc: 0.6667\n",
            "Epoch 875/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5318 - acc: 0.6667\n",
            "Epoch 876/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.5314 - acc: 0.6667\n",
            "Epoch 877/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.5309 - acc: 0.6667\n",
            "Epoch 878/917\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.5305 - acc: 0.6667\n",
            "Epoch 879/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5301 - acc: 0.6667\n",
            "Epoch 880/917\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.5297 - acc: 0.6667\n",
            "Epoch 881/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.5293 - acc: 0.6667\n",
            "Epoch 882/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5289 - acc: 0.6667\n",
            "Epoch 883/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5285 - acc: 0.6667\n",
            "Epoch 884/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.5281 - acc: 0.6667\n",
            "Epoch 885/917\n",
            "84/84 [==============================] - 0s 15us/sample - loss: 0.5277 - acc: 0.6667\n",
            "Epoch 886/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.5273 - acc: 0.6667\n",
            "Epoch 887/917\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.5269 - acc: 0.6667\n",
            "Epoch 888/917\n",
            "84/84 [==============================] - 0s 39us/sample - loss: 0.5265 - acc: 0.6667\n",
            "Epoch 889/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.5261 - acc: 0.6667\n",
            "Epoch 890/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.5257 - acc: 0.6667\n",
            "Epoch 891/917\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.5254 - acc: 0.6667\n",
            "Epoch 892/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.5250 - acc: 0.6667\n",
            "Epoch 893/917\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.5246 - acc: 0.6667\n",
            "Epoch 894/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5242 - acc: 0.6667\n",
            "Epoch 895/917\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.5238 - acc: 0.6667\n",
            "Epoch 896/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.5235 - acc: 0.6667\n",
            "Epoch 897/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5231 - acc: 0.6667\n",
            "Epoch 898/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.5227 - acc: 0.6667\n",
            "Epoch 899/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5224 - acc: 0.6667\n",
            "Epoch 900/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5220 - acc: 0.6667\n",
            "Epoch 901/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5217 - acc: 0.6667\n",
            "Epoch 902/917\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.5213 - acc: 0.6667\n",
            "Epoch 903/917\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.5209 - acc: 0.6667\n",
            "Epoch 904/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5206 - acc: 0.6667\n",
            "Epoch 905/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5202 - acc: 0.6667\n",
            "Epoch 906/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5199 - acc: 0.6667\n",
            "Epoch 907/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5196 - acc: 0.6667\n",
            "Epoch 908/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5192 - acc: 0.6667\n",
            "Epoch 909/917\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.5189 - acc: 0.6667\n",
            "Epoch 910/917\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.5185 - acc: 0.6667\n",
            "Epoch 911/917\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.5182 - acc: 0.6667\n",
            "Epoch 912/917\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.5179 - acc: 0.6667\n",
            "Epoch 913/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.5175 - acc: 0.6667\n",
            "Epoch 914/917\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.5172 - acc: 0.6667\n",
            "Epoch 915/917\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.5169 - acc: 0.6667\n",
            "Epoch 916/917\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.5166 - acc: 0.6667\n",
            "Epoch 917/917\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.5162 - acc: 0.6667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NCK1ceJhghH",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vAeDinagkdw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ed99b4b3-8484-43ea-a0fb-b3d26e571826"
      },
      "source": [
        "print('================== Initial model ==================')\n",
        "evaluate_model_state(initial_weights)\n",
        "print('================== Final model ==================')\n",
        "evaluate_model_state(final_weights)\n",
        "print('=================================================')\n",
        "print(f'Duration: {(finish_time - start_time):.3f} s')"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================== Initial model ==================\n",
            "Train loss: 0.858, accuracy: 0.357\n",
            "Test loss: 1.002, accuracy: 0.263\n",
            "================== Final model ==================\n",
            "Train loss: 0.516, accuracy: 0.667\n",
            "Test loss: 0.543, accuracy: 0.658\n",
            "=================================================\n",
            "Duration: 5.338 s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}