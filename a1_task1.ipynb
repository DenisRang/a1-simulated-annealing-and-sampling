{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a1-task1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTnck0rdiKjz",
        "colab_type": "text"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukPpwJ5BkqaV",
        "colab_type": "text"
      },
      "source": [
        "#### Import needful packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5EU-yHmSIK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import numpy.random as rn\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASOdaWsJkYAQ",
        "colab_type": "text"
      },
      "source": [
        "#### Observe Iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auWPkEhTSL9n",
        "colab_type": "code",
        "outputId": "ab314b11-4b6a-4775-b30b-a265e90ba1b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "    \n",
        "# Load the Iris dataset\n",
        "iris_dataset = load_iris()\n",
        "\n",
        "print('The feature values for Obs 0 are: ', iris_dataset.data[0])\n",
        "print('The feature names are: ', iris_dataset.feature_names)\n",
        "print('The target value for Obs 0 is:', iris_dataset.target[0])\n",
        "print('The target name for Obs 0 is:', iris_dataset.target_names[iris_dataset.target[0]])\n",
        "print('The minimum values of the four features are:', np.min(iris_dataset.data, axis = 0))\n",
        "print('The maximum values of the four features are:', np.max(iris_dataset.data, axis = 0))\n",
        "print('The unique target values are:', np.unique(iris_dataset.target))"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The feature values for Obs 0 are:  [5.1 3.5 1.4 0.2]\n",
            "The feature names are:  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "The target value for Obs 0 is: 0\n",
            "The target name for Obs 0 is: setosa\n",
            "The minimum values of the four features are: [4.3 2.  1.  0.1]\n",
            "The maximum values of the four features are: [7.9 4.4 6.9 2.5]\n",
            "The unique target values are: [0 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNSVPe18SyUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "NUM_CLASSES = 3\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    iris_dataset.data, iris_dataset.target)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_valid = keras.utils.to_categorical(y_valid, NUM_CLASSES)\n",
        "y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLuaD8xJWLKN",
        "colab_type": "text"
      },
      "source": [
        "#### Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOh8P8_Gh1Za",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential([\n",
        "    keras.layers.Dense(5, activation=\"relu\", use_bias=False, input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(5, activation=\"relu\", use_bias=False),\n",
        "    keras.layers.Dense(NUM_CLASSES, activation=\"softmax\",use_bias=False)\n",
        "  ])\n",
        "  opt = keras.optimizers.SGD(lr=.01)\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "def evaluate_model_state(weights):\n",
        "  model.set_weights(weights)\n",
        "  train_evaluating = model.evaluate(X_train,y_train,batch_size=X_train.shape[0],verbose=0)\n",
        "  test_evaluating = model.evaluate(X_test,y_test,batch_size=X_test.shape[0],verbose=0)\n",
        "  print(f'Train loss: {train_evaluating[0]:.3f}, accuracy: {train_evaluating[1]:.3f}')\n",
        "  print(f'Test loss: {test_evaluating[0]:.3f}, accuracy: {test_evaluating[1]:.3f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svl8blzwtz3o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "6317c756-dc43-4f9e-e382-542bfa224408"
      },
      "source": [
        "model = build_model()\n",
        "model.summary()\n",
        "initial_weights = model.get_weights()\n",
        "evaluate_model_state(initial_weights)"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_62\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_186 (Dense)            (None, 5)                 20        \n",
            "_________________________________________________________________\n",
            "dense_187 (Dense)            (None, 5)                 25        \n",
            "_________________________________________________________________\n",
            "dense_188 (Dense)            (None, 3)                 15        \n",
            "=================================================================\n",
            "Total params: 60\n",
            "Trainable params: 60\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "84/84 [==============================] - 0s 2ms/sample - loss: 1.1432 - acc: 0.3333\n",
            "38/38 [==============================] - 0s 41us/sample - loss: 1.1655 - acc: 0.2632\n",
            "Train loss: 1.143, accuracy: 0.333\n",
            "Test loss: 1.166, accuracy: 0.263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y175_2yJth9P",
        "colab_type": "text"
      },
      "source": [
        "#### By default, Keras uses Glorot initialization with a uniform distribution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swYV6vLJV3vw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "880df257-b00d-40f5-a4de-96e3ca273634"
      },
      "source": [
        "print(initial_weights)"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[-0.08256513,  0.45436323,  0.81061864, -0.2992763 ,  0.6332445 ],\n",
            "       [-0.21427041, -0.776036  , -0.14705509, -0.5507741 , -0.25438994],\n",
            "       [-0.00430137,  0.13732326, -0.38635516, -0.0692212 ,  0.43802118],\n",
            "       [ 0.58402586, -0.28919035, -0.02547407, -0.49915358, -0.5042521 ]],\n",
            "      dtype=float32), array([[-0.02057576,  0.20132071,  0.36510134, -0.31935096, -0.34521016],\n",
            "       [ 0.70446455,  0.48994172,  0.57743657,  0.20924693, -0.14663118],\n",
            "       [ 0.41803932, -0.6428888 , -0.5613319 ,  0.15174401,  0.4509014 ],\n",
            "       [-0.2514503 ,  0.11144924, -0.4458567 , -0.28211153,  0.06424367],\n",
            "       [ 0.05873102, -0.6525482 , -0.41218928,  0.6517674 ,  0.371657  ]],\n",
            "      dtype=float32), array([[ 0.8522356 ,  0.03918016, -0.00314796],\n",
            "       [-0.72490054,  0.48966175, -0.17812341],\n",
            "       [-0.8168433 , -0.41320932,  0.22749275],\n",
            "       [-0.1287561 ,  0.6172833 ,  0.55576605],\n",
            "       [ 0.8263592 ,  0.51314443, -0.6807401 ]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy2_5u8khAKH",
        "colab_type": "text"
      },
      "source": [
        "### Using simulated annealing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s4Kr415hJ47",
        "colab_type": "text"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br1MpqjUmaYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def annealing(random_start,\n",
        "            cost_function,\n",
        "            random_neighbour,\n",
        "            acceptance,\n",
        "            temperature,\n",
        "            debug=True):\n",
        "  \"\"\" Optimize the black-box function 'cost_function' with the simulated annealing algorithm.\"\"\"\n",
        "  state = copy.deepcopy(initial_weights)\n",
        "  cost = cost_function(state)\n",
        "  step = 1\n",
        "  T = 1\n",
        "  costs, temperatures = [cost], [T]\n",
        "  temperature_cooled_down = 0.1\n",
        "  while T > temperature_cooled_down:\n",
        "    new_state = random_neighbour(state)\n",
        "    new_cost = cost_function(new_state)\n",
        "    alpha = get_alpha(cost, new_cost, T)\n",
        "    u = rn.random()\n",
        "    if debug: print(f\"Step #{step}:  T = {T:.2f}, cost = {cost:.2f}, new_cost = {new_cost:.2f}, alpha = {alpha:.2f}, u = {u:.2f}\")\n",
        "    if alpha >= u:\n",
        "      state, cost = new_state, new_cost\n",
        "      costs.append(cost)\n",
        "      temperatures.append(T)\n",
        "    step += 1\n",
        "    T = temperature(T)\n",
        "  return step, state, costs, temperatures\n",
        "\n",
        "def cost_function(weights):\n",
        "  \"\"\" Function to minimize.\"\"\"\n",
        "  model.set_weights(weights)\n",
        "  return model.evaluate(X_train,y_train,batch_size=X_train.shape[0],verbose=0)[0]\n",
        "\n",
        "def clip(weight):\n",
        "  \"\"\" Force weight to be in the appropriate interval for neural network weights.\"\"\"\n",
        "  interval = (-1, 1)\n",
        "  a, b = interval\n",
        "  return max(min(weight, b), a)\n",
        "\n",
        "def random_neighbour(weights):\n",
        "  \"\"\"Generate new weights using the normal distribution with mean equal to previous weights\"\"\"\n",
        "  sigma = 0.5\n",
        "  for layer_idx in range(0,len(weights)):\n",
        "    for neuron_idx in range(0,len(weights[layer_idx])):     \n",
        "      for weight_idx in range(0,len(weights[layer_idx][neuron_idx])):\n",
        "        random_weight=rn.normal(weights[layer_idx][neuron_idx][weight_idx],sigma)\n",
        "        weights[layer_idx][neuron_idx][weight_idx]=clip(random_weight)\n",
        "  return weights\n",
        "\n",
        "def get_alpha(cost, new_cost, temperature):\n",
        "  \"\"\"Calculate acceptance ratio\"\"\"\n",
        "  return np.exp(- (new_cost - cost) / temperature)\n",
        "\n",
        "def temperature(old_temperature):\n",
        "  \"\"\" Temperature dicreasing as the process goes on.\"\"\"\n",
        "  a = 0.99\n",
        "  return old_temperature*a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgFUEWoDmtLp",
        "colab_type": "code",
        "outputId": "9ad39e56-fe02-4aa3-eba9-1dc708b0fb73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start_time = time.time()\n",
        "num_steps, final_weights, costs, temperatures = annealing(random_start,\n",
        "                                                          cost_function,\n",
        "                                                          random_neighbour,\n",
        "                                                          acceptance_probability,\n",
        "                                                          temperature,\n",
        "                                                          debug=True)\n",
        "finish_time = time.time()"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step #1:  T = 1.00, cost = 1.14, new_cost = 1.83, alpha = 0.50, u = 0.85\n",
            "Step #2:  T = 0.99, cost = 1.14, new_cost = 1.66, alpha = 0.60, u = 0.13\n",
            "Step #3:  T = 0.98, cost = 1.66, new_cost = 1.95, alpha = 0.74, u = 0.65\n",
            "Step #4:  T = 0.97, cost = 1.95, new_cost = 3.42, alpha = 0.22, u = 0.92\n",
            "Step #5:  T = 0.96, cost = 1.95, new_cost = 2.06, alpha = 0.89, u = 0.39\n",
            "Step #6:  T = 0.95, cost = 2.06, new_cost = 2.55, alpha = 0.60, u = 0.55\n",
            "Step #7:  T = 0.94, cost = 2.55, new_cost = 1.97, alpha = 1.85, u = 0.03\n",
            "Step #8:  T = 0.93, cost = 1.97, new_cost = 1.16, alpha = 2.38, u = 0.17\n",
            "Step #9:  T = 0.92, cost = 1.16, new_cost = 3.34, alpha = 0.09, u = 0.62\n",
            "Step #10:  T = 0.91, cost = 1.16, new_cost = 5.19, alpha = 0.01, u = 0.33\n",
            "Step #11:  T = 0.90, cost = 1.16, new_cost = 4.41, alpha = 0.03, u = 0.37\n",
            "Step #12:  T = 0.90, cost = 1.16, new_cost = 4.42, alpha = 0.03, u = 0.92\n",
            "Step #13:  T = 0.89, cost = 1.16, new_cost = 2.04, alpha = 0.37, u = 0.97\n",
            "Step #14:  T = 0.88, cost = 1.16, new_cost = 2.64, alpha = 0.19, u = 0.72\n",
            "Step #15:  T = 0.87, cost = 1.16, new_cost = 2.78, alpha = 0.16, u = 0.78\n",
            "Step #16:  T = 0.86, cost = 1.16, new_cost = 2.53, alpha = 0.20, u = 0.18\n",
            "Step #17:  T = 0.85, cost = 2.53, new_cost = 3.31, alpha = 0.40, u = 0.61\n",
            "Step #18:  T = 0.84, cost = 2.53, new_cost = 6.42, alpha = 0.01, u = 0.84\n",
            "Step #19:  T = 0.83, cost = 2.53, new_cost = 4.71, alpha = 0.07, u = 0.14\n",
            "Step #20:  T = 0.83, cost = 2.53, new_cost = 5.90, alpha = 0.02, u = 0.47\n",
            "Step #21:  T = 0.82, cost = 2.53, new_cost = 3.19, alpha = 0.45, u = 0.43\n",
            "Step #22:  T = 0.81, cost = 3.19, new_cost = 5.48, alpha = 0.06, u = 0.66\n",
            "Step #23:  T = 0.80, cost = 3.19, new_cost = 4.74, alpha = 0.14, u = 1.00\n",
            "Step #24:  T = 0.79, cost = 3.19, new_cost = 2.73, alpha = 1.78, u = 0.59\n",
            "Step #25:  T = 0.79, cost = 2.73, new_cost = 1.54, alpha = 4.54, u = 0.10\n",
            "Step #26:  T = 0.78, cost = 1.54, new_cost = 1.46, alpha = 1.10, u = 0.45\n",
            "Step #27:  T = 0.77, cost = 1.46, new_cost = 2.60, alpha = 0.23, u = 0.22\n",
            "Step #28:  T = 0.76, cost = 2.60, new_cost = 2.13, alpha = 1.85, u = 0.66\n",
            "Step #29:  T = 0.75, cost = 2.13, new_cost = 1.72, alpha = 1.74, u = 0.20\n",
            "Step #30:  T = 0.75, cost = 1.72, new_cost = 1.20, alpha = 2.00, u = 0.54\n",
            "Step #31:  T = 0.74, cost = 1.20, new_cost = 1.75, alpha = 0.48, u = 0.72\n",
            "Step #32:  T = 0.73, cost = 1.20, new_cost = 0.90, alpha = 1.50, u = 0.89\n",
            "Step #33:  T = 0.72, cost = 0.90, new_cost = 0.69, alpha = 1.34, u = 0.13\n",
            "Step #34:  T = 0.72, cost = 0.69, new_cost = 0.61, alpha = 1.12, u = 0.48\n",
            "Step #35:  T = 0.71, cost = 0.61, new_cost = 1.84, alpha = 0.18, u = 0.18\n",
            "Step #36:  T = 0.70, cost = 1.84, new_cost = 1.38, alpha = 1.93, u = 0.39\n",
            "Step #37:  T = 0.70, cost = 1.38, new_cost = 1.59, alpha = 0.74, u = 0.78\n",
            "Step #38:  T = 0.69, cost = 1.38, new_cost = 2.40, alpha = 0.23, u = 0.91\n",
            "Step #39:  T = 0.68, cost = 1.38, new_cost = 1.57, alpha = 0.76, u = 0.91\n",
            "Step #40:  T = 0.68, cost = 1.38, new_cost = 2.99, alpha = 0.09, u = 0.09\n",
            "Step #41:  T = 0.67, cost = 2.99, new_cost = 4.83, alpha = 0.06, u = 0.32\n",
            "Step #42:  T = 0.66, cost = 2.99, new_cost = 1.83, alpha = 5.71, u = 0.53\n",
            "Step #43:  T = 0.66, cost = 1.83, new_cost = 0.65, alpha = 6.04, u = 0.59\n",
            "Step #44:  T = 0.65, cost = 0.65, new_cost = 1.13, alpha = 0.48, u = 0.74\n",
            "Step #45:  T = 0.64, cost = 0.65, new_cost = 1.94, alpha = 0.14, u = 0.14\n",
            "Step #46:  T = 0.64, cost = 0.65, new_cost = 0.69, alpha = 0.94, u = 0.84\n",
            "Step #47:  T = 0.63, cost = 0.69, new_cost = 1.95, alpha = 0.14, u = 0.09\n",
            "Step #48:  T = 0.62, cost = 1.95, new_cost = 0.94, alpha = 5.13, u = 0.66\n",
            "Step #49:  T = 0.62, cost = 0.94, new_cost = 1.25, alpha = 0.60, u = 0.33\n",
            "Step #50:  T = 0.61, cost = 1.25, new_cost = 1.33, alpha = 0.89, u = 0.66\n",
            "Step #51:  T = 0.61, cost = 1.33, new_cost = 1.81, alpha = 0.45, u = 0.41\n",
            "Step #52:  T = 0.60, cost = 1.81, new_cost = 2.27, alpha = 0.47, u = 0.29\n",
            "Step #53:  T = 0.59, cost = 2.27, new_cost = 1.46, alpha = 3.87, u = 0.60\n",
            "Step #54:  T = 0.59, cost = 1.46, new_cost = 1.52, alpha = 0.92, u = 0.96\n",
            "Step #55:  T = 0.58, cost = 1.46, new_cost = 1.20, alpha = 1.59, u = 0.54\n",
            "Step #56:  T = 0.58, cost = 1.20, new_cost = 1.57, alpha = 0.52, u = 0.14\n",
            "Step #57:  T = 0.57, cost = 1.57, new_cost = 1.39, alpha = 1.36, u = 0.94\n",
            "Step #58:  T = 0.56, cost = 1.39, new_cost = 0.75, alpha = 3.13, u = 0.53\n",
            "Step #59:  T = 0.56, cost = 0.75, new_cost = 0.87, alpha = 0.81, u = 0.67\n",
            "Step #60:  T = 0.55, cost = 0.87, new_cost = 0.71, alpha = 1.33, u = 0.86\n",
            "Step #61:  T = 0.55, cost = 0.71, new_cost = 0.82, alpha = 0.82, u = 0.44\n",
            "Step #62:  T = 0.54, cost = 0.82, new_cost = 0.76, alpha = 1.11, u = 0.13\n",
            "Step #63:  T = 0.54, cost = 0.76, new_cost = 0.81, alpha = 0.92, u = 0.82\n",
            "Step #64:  T = 0.53, cost = 0.81, new_cost = 0.77, alpha = 1.06, u = 0.18\n",
            "Step #65:  T = 0.53, cost = 0.77, new_cost = 1.04, alpha = 0.60, u = 0.22\n",
            "Step #66:  T = 0.52, cost = 1.04, new_cost = 3.49, alpha = 0.01, u = 0.31\n",
            "Step #67:  T = 0.52, cost = 1.04, new_cost = 2.52, alpha = 0.06, u = 0.16\n",
            "Step #68:  T = 0.51, cost = 1.04, new_cost = 1.84, alpha = 0.21, u = 0.17\n",
            "Step #69:  T = 0.50, cost = 1.84, new_cost = 1.01, alpha = 5.14, u = 0.44\n",
            "Step #70:  T = 0.50, cost = 1.01, new_cost = 1.29, alpha = 0.58, u = 1.00\n",
            "Step #71:  T = 0.49, cost = 1.01, new_cost = 0.91, alpha = 1.23, u = 0.15\n",
            "Step #72:  T = 0.49, cost = 0.91, new_cost = 1.24, alpha = 0.51, u = 0.78\n",
            "Step #73:  T = 0.48, cost = 0.91, new_cost = 1.06, alpha = 0.73, u = 0.48\n",
            "Step #74:  T = 0.48, cost = 1.06, new_cost = 1.72, alpha = 0.26, u = 0.01\n",
            "Step #75:  T = 0.48, cost = 1.72, new_cost = 1.43, alpha = 1.84, u = 0.37\n",
            "Step #76:  T = 0.47, cost = 1.43, new_cost = 1.54, alpha = 0.79, u = 0.36\n",
            "Step #77:  T = 0.47, cost = 1.54, new_cost = 1.96, alpha = 0.41, u = 0.68\n",
            "Step #78:  T = 0.46, cost = 1.54, new_cost = 2.80, alpha = 0.06, u = 0.38\n",
            "Step #79:  T = 0.46, cost = 1.54, new_cost = 2.25, alpha = 0.21, u = 0.57\n",
            "Step #80:  T = 0.45, cost = 1.54, new_cost = 2.32, alpha = 0.18, u = 0.77\n",
            "Step #81:  T = 0.45, cost = 1.54, new_cost = 1.82, alpha = 0.53, u = 0.19\n",
            "Step #82:  T = 0.44, cost = 1.82, new_cost = 1.15, alpha = 4.58, u = 0.62\n",
            "Step #83:  T = 0.44, cost = 1.15, new_cost = 1.82, alpha = 0.22, u = 0.99\n",
            "Step #84:  T = 0.43, cost = 1.15, new_cost = 1.41, alpha = 0.55, u = 0.02\n",
            "Step #85:  T = 0.43, cost = 1.41, new_cost = 1.76, alpha = 0.44, u = 0.44\n",
            "Step #86:  T = 0.43, cost = 1.41, new_cost = 1.32, alpha = 1.22, u = 0.90\n",
            "Step #87:  T = 0.42, cost = 1.32, new_cost = 1.62, alpha = 0.49, u = 0.66\n",
            "Step #88:  T = 0.42, cost = 1.32, new_cost = 1.77, alpha = 0.35, u = 0.03\n",
            "Step #89:  T = 0.41, cost = 1.77, new_cost = 1.14, alpha = 4.55, u = 0.79\n",
            "Step #90:  T = 0.41, cost = 1.14, new_cost = 1.14, alpha = 0.99, u = 0.44\n",
            "Step #91:  T = 0.40, cost = 1.14, new_cost = 2.40, alpha = 0.04, u = 0.27\n",
            "Step #92:  T = 0.40, cost = 1.14, new_cost = 2.16, alpha = 0.08, u = 0.61\n",
            "Step #93:  T = 0.40, cost = 1.14, new_cost = 2.76, alpha = 0.02, u = 0.27\n",
            "Step #94:  T = 0.39, cost = 1.14, new_cost = 1.24, alpha = 0.77, u = 0.11\n",
            "Step #95:  T = 0.39, cost = 1.24, new_cost = 1.48, alpha = 0.55, u = 0.20\n",
            "Step #96:  T = 0.38, cost = 1.48, new_cost = 2.05, alpha = 0.23, u = 0.63\n",
            "Step #97:  T = 0.38, cost = 1.48, new_cost = 2.52, alpha = 0.06, u = 0.38\n",
            "Step #98:  T = 0.38, cost = 1.48, new_cost = 2.06, alpha = 0.21, u = 0.96\n",
            "Step #99:  T = 0.37, cost = 1.48, new_cost = 2.04, alpha = 0.22, u = 0.34\n",
            "Step #100:  T = 0.37, cost = 1.48, new_cost = 1.36, alpha = 1.38, u = 0.34\n",
            "Step #101:  T = 0.37, cost = 1.36, new_cost = 0.91, alpha = 3.44, u = 0.24\n",
            "Step #102:  T = 0.36, cost = 0.91, new_cost = 0.62, alpha = 2.18, u = 0.65\n",
            "Step #103:  T = 0.36, cost = 0.62, new_cost = 0.98, alpha = 0.37, u = 0.31\n",
            "Step #104:  T = 0.36, cost = 0.98, new_cost = 1.82, alpha = 0.09, u = 0.64\n",
            "Step #105:  T = 0.35, cost = 0.98, new_cost = 1.52, alpha = 0.21, u = 0.40\n",
            "Step #106:  T = 0.35, cost = 0.98, new_cost = 1.00, alpha = 0.93, u = 0.50\n",
            "Step #107:  T = 0.34, cost = 1.00, new_cost = 1.14, alpha = 0.67, u = 0.90\n",
            "Step #108:  T = 0.34, cost = 1.00, new_cost = 1.59, alpha = 0.18, u = 0.38\n",
            "Step #109:  T = 0.34, cost = 1.00, new_cost = 0.78, alpha = 1.89, u = 0.68\n",
            "Step #110:  T = 0.33, cost = 0.78, new_cost = 0.94, alpha = 0.62, u = 0.57\n",
            "Step #111:  T = 0.33, cost = 0.94, new_cost = 1.90, alpha = 0.06, u = 0.03\n",
            "Step #112:  T = 0.33, cost = 1.90, new_cost = 1.91, alpha = 0.96, u = 1.00\n",
            "Step #113:  T = 0.32, cost = 1.90, new_cost = 1.65, alpha = 2.15, u = 0.62\n",
            "Step #114:  T = 0.32, cost = 1.65, new_cost = 1.43, alpha = 2.00, u = 0.55\n",
            "Step #115:  T = 0.32, cost = 1.43, new_cost = 2.00, alpha = 0.17, u = 0.94\n",
            "Step #116:  T = 0.31, cost = 1.43, new_cost = 1.26, alpha = 1.71, u = 0.63\n",
            "Step #117:  T = 0.31, cost = 1.26, new_cost = 3.88, alpha = 0.00, u = 0.66\n",
            "Step #118:  T = 0.31, cost = 1.26, new_cost = 1.83, alpha = 0.16, u = 0.01\n",
            "Step #119:  T = 0.31, cost = 1.83, new_cost = 2.29, alpha = 0.22, u = 0.14\n",
            "Step #120:  T = 0.30, cost = 2.29, new_cost = 1.86, alpha = 4.07, u = 0.34\n",
            "Step #121:  T = 0.30, cost = 1.86, new_cost = 1.68, alpha = 1.87, u = 0.32\n",
            "Step #122:  T = 0.30, cost = 1.68, new_cost = 1.61, alpha = 1.25, u = 0.39\n",
            "Step #123:  T = 0.29, cost = 1.61, new_cost = 0.96, alpha = 9.20, u = 0.02\n",
            "Step #124:  T = 0.29, cost = 0.96, new_cost = 1.38, alpha = 0.23, u = 0.02\n",
            "Step #125:  T = 0.29, cost = 1.38, new_cost = 1.08, alpha = 2.83, u = 0.79\n",
            "Step #126:  T = 0.28, cost = 1.08, new_cost = 1.42, alpha = 0.30, u = 0.96\n",
            "Step #127:  T = 0.28, cost = 1.08, new_cost = 1.77, alpha = 0.09, u = 0.39\n",
            "Step #128:  T = 0.28, cost = 1.08, new_cost = 1.79, alpha = 0.08, u = 0.92\n",
            "Step #129:  T = 0.28, cost = 1.08, new_cost = 2.28, alpha = 0.01, u = 0.43\n",
            "Step #130:  T = 0.27, cost = 1.08, new_cost = 1.62, alpha = 0.14, u = 0.14\n",
            "Step #131:  T = 0.27, cost = 1.62, new_cost = 1.59, alpha = 1.11, u = 0.38\n",
            "Step #132:  T = 0.27, cost = 1.59, new_cost = 1.68, alpha = 0.70, u = 0.85\n",
            "Step #133:  T = 0.27, cost = 1.59, new_cost = 3.23, alpha = 0.00, u = 0.80\n",
            "Step #134:  T = 0.26, cost = 1.59, new_cost = 1.96, alpha = 0.25, u = 0.06\n",
            "Step #135:  T = 0.26, cost = 1.96, new_cost = 2.47, alpha = 0.14, u = 0.60\n",
            "Step #136:  T = 0.26, cost = 1.96, new_cost = 1.71, alpha = 2.57, u = 0.33\n",
            "Step #137:  T = 0.25, cost = 1.71, new_cost = 4.29, alpha = 0.00, u = 0.42\n",
            "Step #138:  T = 0.25, cost = 1.71, new_cost = 4.13, alpha = 0.00, u = 0.14\n",
            "Step #139:  T = 0.25, cost = 1.71, new_cost = 5.65, alpha = 0.00, u = 0.36\n",
            "Step #140:  T = 0.25, cost = 1.71, new_cost = 4.61, alpha = 0.00, u = 0.63\n",
            "Step #141:  T = 0.24, cost = 1.71, new_cost = 0.94, alpha = 23.17, u = 0.53\n",
            "Step #142:  T = 0.24, cost = 0.94, new_cost = 1.27, alpha = 0.27, u = 0.80\n",
            "Step #143:  T = 0.24, cost = 0.94, new_cost = 1.38, alpha = 0.16, u = 0.72\n",
            "Step #144:  T = 0.24, cost = 0.94, new_cost = 1.32, alpha = 0.20, u = 0.27\n",
            "Step #145:  T = 0.24, cost = 0.94, new_cost = 2.24, alpha = 0.00, u = 0.27\n",
            "Step #146:  T = 0.23, cost = 0.94, new_cost = 1.64, alpha = 0.05, u = 0.67\n",
            "Step #147:  T = 0.23, cost = 0.94, new_cost = 1.40, alpha = 0.14, u = 0.78\n",
            "Step #148:  T = 0.23, cost = 0.94, new_cost = 2.07, alpha = 0.01, u = 0.49\n",
            "Step #149:  T = 0.23, cost = 0.94, new_cost = 1.83, alpha = 0.02, u = 0.78\n",
            "Step #150:  T = 0.22, cost = 0.94, new_cost = 1.24, alpha = 0.26, u = 0.34\n",
            "Step #151:  T = 0.22, cost = 0.94, new_cost = 1.09, alpha = 0.51, u = 0.90\n",
            "Step #152:  T = 0.22, cost = 0.94, new_cost = 0.97, alpha = 0.88, u = 0.34\n",
            "Step #153:  T = 0.22, cost = 0.97, new_cost = 0.98, alpha = 0.96, u = 0.29\n",
            "Step #154:  T = 0.21, cost = 0.98, new_cost = 0.95, alpha = 1.15, u = 0.25\n",
            "Step #155:  T = 0.21, cost = 0.95, new_cost = 1.88, alpha = 0.01, u = 0.05\n",
            "Step #156:  T = 0.21, cost = 0.95, new_cost = 0.85, alpha = 1.61, u = 0.23\n",
            "Step #157:  T = 0.21, cost = 0.85, new_cost = 1.18, alpha = 0.21, u = 0.32\n",
            "Step #158:  T = 0.21, cost = 0.85, new_cost = 0.73, alpha = 1.79, u = 0.30\n",
            "Step #159:  T = 0.20, cost = 0.73, new_cost = 0.74, alpha = 0.94, u = 0.74\n",
            "Step #160:  T = 0.20, cost = 0.74, new_cost = 0.87, alpha = 0.53, u = 0.08\n",
            "Step #161:  T = 0.20, cost = 0.87, new_cost = 1.18, alpha = 0.22, u = 0.02\n",
            "Step #162:  T = 0.20, cost = 1.18, new_cost = 1.10, alpha = 1.44, u = 0.61\n",
            "Step #163:  T = 0.20, cost = 1.10, new_cost = 1.31, alpha = 0.36, u = 0.06\n",
            "Step #164:  T = 0.19, cost = 1.31, new_cost = 1.23, alpha = 1.52, u = 0.33\n",
            "Step #165:  T = 0.19, cost = 1.23, new_cost = 1.25, alpha = 0.90, u = 0.26\n",
            "Step #166:  T = 0.19, cost = 1.25, new_cost = 1.05, alpha = 2.79, u = 0.29\n",
            "Step #167:  T = 0.19, cost = 1.05, new_cost = 0.93, alpha = 1.88, u = 0.52\n",
            "Step #168:  T = 0.19, cost = 0.93, new_cost = 1.11, alpha = 0.39, u = 0.25\n",
            "Step #169:  T = 0.18, cost = 1.11, new_cost = 1.09, alpha = 1.09, u = 0.40\n",
            "Step #170:  T = 0.18, cost = 1.09, new_cost = 0.84, alpha = 3.90, u = 0.48\n",
            "Step #171:  T = 0.18, cost = 0.84, new_cost = 1.09, alpha = 0.26, u = 0.81\n",
            "Step #172:  T = 0.18, cost = 0.84, new_cost = 0.81, alpha = 1.18, u = 0.16\n",
            "Step #173:  T = 0.18, cost = 0.81, new_cost = 1.24, alpha = 0.09, u = 0.98\n",
            "Step #174:  T = 0.18, cost = 0.81, new_cost = 1.00, alpha = 0.34, u = 0.83\n",
            "Step #175:  T = 0.17, cost = 0.81, new_cost = 1.24, alpha = 0.09, u = 0.48\n",
            "Step #176:  T = 0.17, cost = 0.81, new_cost = 2.73, alpha = 0.00, u = 0.43\n",
            "Step #177:  T = 0.17, cost = 0.81, new_cost = 2.48, alpha = 0.00, u = 0.45\n",
            "Step #178:  T = 0.17, cost = 0.81, new_cost = 4.29, alpha = 0.00, u = 0.93\n",
            "Step #179:  T = 0.17, cost = 0.81, new_cost = 1.86, alpha = 0.00, u = 0.98\n",
            "Step #180:  T = 0.17, cost = 0.81, new_cost = 1.70, alpha = 0.00, u = 0.76\n",
            "Step #181:  T = 0.16, cost = 0.81, new_cost = 1.57, alpha = 0.01, u = 0.85\n",
            "Step #182:  T = 0.16, cost = 0.81, new_cost = 2.92, alpha = 0.00, u = 0.13\n",
            "Step #183:  T = 0.16, cost = 0.81, new_cost = 1.92, alpha = 0.00, u = 0.62\n",
            "Step #184:  T = 0.16, cost = 0.81, new_cost = 2.53, alpha = 0.00, u = 0.51\n",
            "Step #185:  T = 0.16, cost = 0.81, new_cost = 1.43, alpha = 0.02, u = 0.08\n",
            "Step #186:  T = 0.16, cost = 0.81, new_cost = 1.12, alpha = 0.14, u = 0.99\n",
            "Step #187:  T = 0.15, cost = 0.81, new_cost = 1.95, alpha = 0.00, u = 0.92\n",
            "Step #188:  T = 0.15, cost = 0.81, new_cost = 1.37, alpha = 0.03, u = 0.76\n",
            "Step #189:  T = 0.15, cost = 0.81, new_cost = 1.34, alpha = 0.03, u = 0.52\n",
            "Step #190:  T = 0.15, cost = 0.81, new_cost = 1.25, alpha = 0.05, u = 0.77\n",
            "Step #191:  T = 0.15, cost = 0.81, new_cost = 0.81, alpha = 1.03, u = 0.78\n",
            "Step #192:  T = 0.15, cost = 0.81, new_cost = 1.47, alpha = 0.01, u = 0.88\n",
            "Step #193:  T = 0.15, cost = 0.81, new_cost = 0.99, alpha = 0.30, u = 0.78\n",
            "Step #194:  T = 0.14, cost = 0.81, new_cost = 1.67, alpha = 0.00, u = 0.86\n",
            "Step #195:  T = 0.14, cost = 0.81, new_cost = 1.04, alpha = 0.20, u = 0.54\n",
            "Step #196:  T = 0.14, cost = 0.81, new_cost = 1.25, alpha = 0.04, u = 0.61\n",
            "Step #197:  T = 0.14, cost = 0.81, new_cost = 1.23, alpha = 0.05, u = 0.06\n",
            "Step #198:  T = 0.14, cost = 0.81, new_cost = 1.24, alpha = 0.04, u = 0.74\n",
            "Step #199:  T = 0.14, cost = 0.81, new_cost = 2.20, alpha = 0.00, u = 0.67\n",
            "Step #200:  T = 0.14, cost = 0.81, new_cost = 1.70, alpha = 0.00, u = 0.05\n",
            "Step #201:  T = 0.13, cost = 0.81, new_cost = 1.93, alpha = 0.00, u = 0.98\n",
            "Step #202:  T = 0.13, cost = 0.81, new_cost = 2.55, alpha = 0.00, u = 0.24\n",
            "Step #203:  T = 0.13, cost = 0.81, new_cost = 0.65, alpha = 3.37, u = 0.79\n",
            "Step #204:  T = 0.13, cost = 0.65, new_cost = 1.12, alpha = 0.03, u = 0.00\n",
            "Step #205:  T = 0.13, cost = 1.12, new_cost = 1.39, alpha = 0.12, u = 0.87\n",
            "Step #206:  T = 0.13, cost = 1.12, new_cost = 1.43, alpha = 0.09, u = 0.75\n",
            "Step #207:  T = 0.13, cost = 1.12, new_cost = 1.47, alpha = 0.06, u = 0.42\n",
            "Step #208:  T = 0.12, cost = 1.12, new_cost = 1.49, alpha = 0.05, u = 0.88\n",
            "Step #209:  T = 0.12, cost = 1.12, new_cost = 1.27, alpha = 0.30, u = 0.06\n",
            "Step #210:  T = 0.12, cost = 1.27, new_cost = 1.30, alpha = 0.75, u = 0.22\n",
            "Step #211:  T = 0.12, cost = 1.30, new_cost = 1.39, alpha = 0.49, u = 0.94\n",
            "Step #212:  T = 0.12, cost = 1.30, new_cost = 1.55, alpha = 0.12, u = 0.11\n",
            "Step #213:  T = 0.12, cost = 1.55, new_cost = 2.46, alpha = 0.00, u = 0.52\n",
            "Step #214:  T = 0.12, cost = 1.55, new_cost = 1.60, alpha = 0.65, u = 0.56\n",
            "Step #215:  T = 0.12, cost = 1.60, new_cost = 1.14, alpha = 53.14, u = 0.35\n",
            "Step #216:  T = 0.12, cost = 1.14, new_cost = 1.02, alpha = 2.85, u = 0.44\n",
            "Step #217:  T = 0.11, cost = 1.02, new_cost = 1.09, alpha = 0.56, u = 0.83\n",
            "Step #218:  T = 0.11, cost = 1.02, new_cost = 1.55, alpha = 0.01, u = 0.23\n",
            "Step #219:  T = 0.11, cost = 1.02, new_cost = 1.67, alpha = 0.00, u = 0.79\n",
            "Step #220:  T = 0.11, cost = 1.02, new_cost = 3.06, alpha = 0.00, u = 0.41\n",
            "Step #221:  T = 0.11, cost = 1.02, new_cost = 1.47, alpha = 0.02, u = 0.07\n",
            "Step #222:  T = 0.11, cost = 1.02, new_cost = 1.15, alpha = 0.31, u = 0.58\n",
            "Step #223:  T = 0.11, cost = 1.02, new_cost = 1.41, alpha = 0.03, u = 0.71\n",
            "Step #224:  T = 0.11, cost = 1.02, new_cost = 1.20, alpha = 0.18, u = 0.57\n",
            "Step #225:  T = 0.11, cost = 1.02, new_cost = 1.30, alpha = 0.07, u = 0.27\n",
            "Step #226:  T = 0.10, cost = 1.02, new_cost = 1.30, alpha = 0.07, u = 0.19\n",
            "Step #227:  T = 0.10, cost = 1.02, new_cost = 1.39, alpha = 0.03, u = 0.95\n",
            "Step #228:  T = 0.10, cost = 1.02, new_cost = 1.13, alpha = 0.33, u = 0.35\n",
            "Step #229:  T = 0.10, cost = 1.02, new_cost = 1.41, alpha = 0.02, u = 0.47\n",
            "Step #230:  T = 0.10, cost = 1.02, new_cost = 1.12, alpha = 0.37, u = 0.64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwfT4YJshNXK",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBYkvW1x8fJh",
        "colab_type": "code",
        "outputId": "55a90a57-6992-46a3-a42f-42e5088e3611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "def plot(costs, temperatures):\n",
        "  plt.plot(temperatures, costs)\n",
        "  plt.xlabel('Temperatures')\n",
        "  plt.ylabel('Costs')\n",
        "  plt.show()\n",
        "\n",
        "plot(costs, temperatures)\n",
        "print('================== Initial model ==================')\n",
        "evaluate_model_state(initial_weights)\n",
        "print('================== Final model ==================')\n",
        "evaluate_model_state(final_weights)\n",
        "print('=================================================')\n",
        "print(f'Duration: {(finish_time - start_time):.3f} s')"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "================== Initial model ==================\n",
            "Train loss: 1.143, accuracy: 0.333\n",
            "Test loss: 1.166, accuracy: 0.263\n",
            "================== Final model ==================\n",
            "Train loss: 1.119, accuracy: 0.321\n",
            "Test loss: 1.001, accuracy: 0.526\n",
            "=================================================\n",
            "Duration: 1.006 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3DhAffHhVYg",
        "colab_type": "text"
      },
      "source": [
        "### Using backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL96v1-ShfMd",
        "colab_type": "text"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7yVMuTJkAq0",
        "colab_type": "code",
        "outputId": "241a48ce-b69b-4f75-8362-bb3dfb80fb0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.set_weights(initial_weights)\n",
        "start_time = time.time()\n",
        "model.fit(X_train, y_train, batch_size=X_train.shape[0], epochs=num_steps-1, shuffle=False)\n",
        "finish_time = time.time()\n",
        "final_weights = model.get_weights()"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 84 samples\n",
            "Epoch 1/230\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 1.1432 - acc: 0.3333\n",
            "Epoch 2/230\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 1.1391 - acc: 0.3452\n",
            "Epoch 3/230\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 1.1353 - acc: 0.3571\n",
            "Epoch 4/230\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 1.1315 - acc: 0.3571\n",
            "Epoch 5/230\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 1.1277 - acc: 0.3690\n",
            "Epoch 6/230\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 1.1241 - acc: 0.3690\n",
            "Epoch 7/230\n",
            "84/84 [==============================] - 0s 17us/sample - loss: 1.1206 - acc: 0.3810\n",
            "Epoch 8/230\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 1.1172 - acc: 0.3810\n",
            "Epoch 9/230\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 1.1140 - acc: 0.3810\n",
            "Epoch 10/230\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 1.1107 - acc: 0.3810\n",
            "Epoch 11/230\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 1.1075 - acc: 0.3929\n",
            "Epoch 12/230\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 1.1042 - acc: 0.3929\n",
            "Epoch 13/230\n",
            "84/84 [==============================] - 0s 16us/sample - loss: 1.1010 - acc: 0.3929\n",
            "Epoch 14/230\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 1.0978 - acc: 0.3929\n",
            "Epoch 15/230\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 1.0947 - acc: 0.4167\n",
            "Epoch 16/230\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 1.0917 - acc: 0.4167\n",
            "Epoch 17/230\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 1.0887 - acc: 0.4524\n",
            "Epoch 18/230\n",
            "84/84 [==============================] - 0s 17us/sample - loss: 1.0858 - acc: 0.4524\n",
            "Epoch 19/230\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 1.0830 - acc: 0.4524\n",
            "Epoch 20/230\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 1.0802 - acc: 0.4524\n",
            "Epoch 21/230\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 1.0774 - acc: 0.4524\n",
            "Epoch 22/230\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 1.0748 - acc: 0.4524\n",
            "Epoch 23/230\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 1.0723 - acc: 0.4524\n",
            "Epoch 24/230\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 1.0698 - acc: 0.4524\n",
            "Epoch 25/230\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 1.0674 - acc: 0.4524\n",
            "Epoch 26/230\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 1.0651 - acc: 0.4643\n",
            "Epoch 27/230\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 1.0628 - acc: 0.4643\n",
            "Epoch 28/230\n",
            "84/84 [==============================] - 0s 17us/sample - loss: 1.0606 - acc: 0.4762\n",
            "Epoch 29/230\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 1.0584 - acc: 0.4762\n",
            "Epoch 30/230\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 1.0563 - acc: 0.4881\n",
            "Epoch 31/230\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 1.0542 - acc: 0.5000\n",
            "Epoch 32/230\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 1.0522 - acc: 0.5000\n",
            "Epoch 33/230\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 1.0501 - acc: 0.5000\n",
            "Epoch 34/230\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 1.0480 - acc: 0.5000\n",
            "Epoch 35/230\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 1.0457 - acc: 0.5000\n",
            "Epoch 36/230\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 1.0434 - acc: 0.5119\n",
            "Epoch 37/230\n",
            "84/84 [==============================] - 0s 17us/sample - loss: 1.0412 - acc: 0.5119\n",
            "Epoch 38/230\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 1.0389 - acc: 0.5357\n",
            "Epoch 39/230\n",
            "84/84 [==============================] - 0s 16us/sample - loss: 1.0365 - acc: 0.5357\n",
            "Epoch 40/230\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 1.0342 - acc: 0.5476\n",
            "Epoch 41/230\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 1.0319 - acc: 0.5476\n",
            "Epoch 42/230\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 1.0296 - acc: 0.5476\n",
            "Epoch 43/230\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 1.0274 - acc: 0.5476\n",
            "Epoch 44/230\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 1.0251 - acc: 0.5476\n",
            "Epoch 45/230\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 1.0228 - acc: 0.5476\n",
            "Epoch 46/230\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 1.0205 - acc: 0.5714\n",
            "Epoch 47/230\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 1.0181 - acc: 0.5714\n",
            "Epoch 48/230\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 1.0156 - acc: 0.5714\n",
            "Epoch 49/230\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 1.0131 - acc: 0.5714\n",
            "Epoch 50/230\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 1.0107 - acc: 0.5714\n",
            "Epoch 51/230\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 1.0083 - acc: 0.5714\n",
            "Epoch 52/230\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 1.0060 - acc: 0.5833\n",
            "Epoch 53/230\n",
            "84/84 [==============================] - 0s 50us/sample - loss: 1.0037 - acc: 0.5833\n",
            "Epoch 54/230\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 1.0014 - acc: 0.5952\n",
            "Epoch 55/230\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.9991 - acc: 0.5952\n",
            "Epoch 56/230\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.9967 - acc: 0.5952\n",
            "Epoch 57/230\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.9943 - acc: 0.6190\n",
            "Epoch 58/230\n",
            "84/84 [==============================] - 0s 42us/sample - loss: 0.9918 - acc: 0.6310\n",
            "Epoch 59/230\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.9895 - acc: 0.6429\n",
            "Epoch 60/230\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.9872 - acc: 0.6429\n",
            "Epoch 61/230\n",
            "84/84 [==============================] - 0s 41us/sample - loss: 0.9850 - acc: 0.6548\n",
            "Epoch 62/230\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.9827 - acc: 0.6667\n",
            "Epoch 63/230\n",
            "84/84 [==============================] - 0s 45us/sample - loss: 0.9805 - acc: 0.6786\n",
            "Epoch 64/230\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.9783 - acc: 0.6786\n",
            "Epoch 65/230\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.9761 - acc: 0.6786\n",
            "Epoch 66/230\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.9738 - acc: 0.6786\n",
            "Epoch 67/230\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.9715 - acc: 0.6905\n",
            "Epoch 68/230\n",
            "84/84 [==============================] - 0s 51us/sample - loss: 0.9690 - acc: 0.7024\n",
            "Epoch 69/230\n",
            "84/84 [==============================] - 0s 51us/sample - loss: 0.9664 - acc: 0.6905\n",
            "Epoch 70/230\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.9639 - acc: 0.7143\n",
            "Epoch 71/230\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.9614 - acc: 0.7262\n",
            "Epoch 72/230\n",
            "84/84 [==============================] - 0s 40us/sample - loss: 0.9588 - acc: 0.7381\n",
            "Epoch 73/230\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.9563 - acc: 0.7500\n",
            "Epoch 74/230\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.9537 - acc: 0.7500\n",
            "Epoch 75/230\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.9513 - acc: 0.7500\n",
            "Epoch 76/230\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.9490 - acc: 0.7619\n",
            "Epoch 77/230\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.9467 - acc: 0.7619\n",
            "Epoch 78/230\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.9444 - acc: 0.7619\n",
            "Epoch 79/230\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.9422 - acc: 0.7738\n",
            "Epoch 80/230\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.9402 - acc: 0.7857\n",
            "Epoch 81/230\n",
            "84/84 [==============================] - 0s 47us/sample - loss: 0.9382 - acc: 0.7857\n",
            "Epoch 82/230\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.9363 - acc: 0.7857\n",
            "Epoch 83/230\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.9344 - acc: 0.7857\n",
            "Epoch 84/230\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.9326 - acc: 0.7976\n",
            "Epoch 85/230\n",
            "84/84 [==============================] - 0s 56us/sample - loss: 0.9308 - acc: 0.7976\n",
            "Epoch 86/230\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.9290 - acc: 0.7976\n",
            "Epoch 87/230\n",
            "84/84 [==============================] - 0s 48us/sample - loss: 0.9272 - acc: 0.7976\n",
            "Epoch 88/230\n",
            "84/84 [==============================] - 0s 40us/sample - loss: 0.9254 - acc: 0.7976\n",
            "Epoch 89/230\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.9235 - acc: 0.7976\n",
            "Epoch 90/230\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.9217 - acc: 0.7976\n",
            "Epoch 91/230\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.9199 - acc: 0.7976\n",
            "Epoch 92/230\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.9182 - acc: 0.8095\n",
            "Epoch 93/230\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.9164 - acc: 0.8095\n",
            "Epoch 94/230\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.9146 - acc: 0.8095\n",
            "Epoch 95/230\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.9129 - acc: 0.8095\n",
            "Epoch 96/230\n",
            "84/84 [==============================] - 0s 50us/sample - loss: 0.9111 - acc: 0.8095\n",
            "Epoch 97/230\n",
            "84/84 [==============================] - 0s 58us/sample - loss: 0.9094 - acc: 0.8095\n",
            "Epoch 98/230\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.9078 - acc: 0.8095\n",
            "Epoch 99/230\n",
            "84/84 [==============================] - 0s 39us/sample - loss: 0.9061 - acc: 0.8095\n",
            "Epoch 100/230\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.9043 - acc: 0.8214\n",
            "Epoch 101/230\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.9025 - acc: 0.8214\n",
            "Epoch 102/230\n",
            "84/84 [==============================] - 0s 40us/sample - loss: 0.9007 - acc: 0.8214\n",
            "Epoch 103/230\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.8990 - acc: 0.8214\n",
            "Epoch 104/230\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.8972 - acc: 0.8214\n",
            "Epoch 105/230\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.8954 - acc: 0.8214\n",
            "Epoch 106/230\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.8937 - acc: 0.8214\n",
            "Epoch 107/230\n",
            "84/84 [==============================] - 0s 43us/sample - loss: 0.8919 - acc: 0.8214\n",
            "Epoch 108/230\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.8901 - acc: 0.8214\n",
            "Epoch 109/230\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.8884 - acc: 0.8333\n",
            "Epoch 110/230\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.8866 - acc: 0.8333\n",
            "Epoch 111/230\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.8848 - acc: 0.8214\n",
            "Epoch 112/230\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.8830 - acc: 0.8214\n",
            "Epoch 113/230\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.8812 - acc: 0.8214\n",
            "Epoch 114/230\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.8794 - acc: 0.8214\n",
            "Epoch 115/230\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.8776 - acc: 0.8214\n",
            "Epoch 116/230\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.8758 - acc: 0.8214\n",
            "Epoch 117/230\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.8740 - acc: 0.8214\n",
            "Epoch 118/230\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.8721 - acc: 0.8214\n",
            "Epoch 119/230\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.8703 - acc: 0.8214\n",
            "Epoch 120/230\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.8684 - acc: 0.8095\n",
            "Epoch 121/230\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.8664 - acc: 0.8214\n",
            "Epoch 122/230\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.8644 - acc: 0.8214\n",
            "Epoch 123/230\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.8626 - acc: 0.8333\n",
            "Epoch 124/230\n",
            "84/84 [==============================] - 0s 40us/sample - loss: 0.8607 - acc: 0.8333\n",
            "Epoch 125/230\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.8589 - acc: 0.8333\n",
            "Epoch 126/230\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.8569 - acc: 0.8333\n",
            "Epoch 127/230\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.8550 - acc: 0.8690\n",
            "Epoch 128/230\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.8531 - acc: 0.8690\n",
            "Epoch 129/230\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.8512 - acc: 0.8690\n",
            "Epoch 130/230\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.8492 - acc: 0.8690\n",
            "Epoch 131/230\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.8473 - acc: 0.8690\n",
            "Epoch 132/230\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.8453 - acc: 0.8690\n",
            "Epoch 133/230\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.8433 - acc: 0.8571\n",
            "Epoch 134/230\n",
            "84/84 [==============================] - 0s 39us/sample - loss: 0.8414 - acc: 0.8571\n",
            "Epoch 135/230\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.8394 - acc: 0.8690\n",
            "Epoch 136/230\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.8375 - acc: 0.8690\n",
            "Epoch 137/230\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.8356 - acc: 0.8690\n",
            "Epoch 138/230\n",
            "84/84 [==============================] - 0s 43us/sample - loss: 0.8336 - acc: 0.8690\n",
            "Epoch 139/230\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.8316 - acc: 0.8690\n",
            "Epoch 140/230\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.8296 - acc: 0.8690\n",
            "Epoch 141/230\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.8276 - acc: 0.8690\n",
            "Epoch 142/230\n",
            "84/84 [==============================] - 0s 18us/sample - loss: 0.8256 - acc: 0.8690\n",
            "Epoch 143/230\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.8236 - acc: 0.8690\n",
            "Epoch 144/230\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.8216 - acc: 0.8690\n",
            "Epoch 145/230\n",
            "84/84 [==============================] - 0s 42us/sample - loss: 0.8197 - acc: 0.8690\n",
            "Epoch 146/230\n",
            "84/84 [==============================] - 0s 41us/sample - loss: 0.8178 - acc: 0.8690\n",
            "Epoch 147/230\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.8159 - acc: 0.8690\n",
            "Epoch 148/230\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.8140 - acc: 0.8690\n",
            "Epoch 149/230\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.8121 - acc: 0.8690\n",
            "Epoch 150/230\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.8102 - acc: 0.8690\n",
            "Epoch 151/230\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.8084 - acc: 0.8690\n",
            "Epoch 152/230\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.8065 - acc: 0.8690\n",
            "Epoch 153/230\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.8046 - acc: 0.8690\n",
            "Epoch 154/230\n",
            "84/84 [==============================] - 0s 23us/sample - loss: 0.8027 - acc: 0.8690\n",
            "Epoch 155/230\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.8008 - acc: 0.8690\n",
            "Epoch 156/230\n",
            "84/84 [==============================] - 0s 57us/sample - loss: 0.7989 - acc: 0.8690\n",
            "Epoch 157/230\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7970 - acc: 0.8690\n",
            "Epoch 158/230\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7951 - acc: 0.8690\n",
            "Epoch 159/230\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7932 - acc: 0.8690\n",
            "Epoch 160/230\n",
            "84/84 [==============================] - 0s 41us/sample - loss: 0.7913 - acc: 0.8571\n",
            "Epoch 161/230\n",
            "84/84 [==============================] - 0s 45us/sample - loss: 0.7894 - acc: 0.8571\n",
            "Epoch 162/230\n",
            "84/84 [==============================] - 0s 43us/sample - loss: 0.7875 - acc: 0.8571\n",
            "Epoch 163/230\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7857 - acc: 0.8571\n",
            "Epoch 164/230\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.7838 - acc: 0.8571\n",
            "Epoch 165/230\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7820 - acc: 0.8571\n",
            "Epoch 166/230\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7802 - acc: 0.8571\n",
            "Epoch 167/230\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.7783 - acc: 0.8571\n",
            "Epoch 168/230\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7765 - acc: 0.8571\n",
            "Epoch 169/230\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7746 - acc: 0.8571\n",
            "Epoch 170/230\n",
            "84/84 [==============================] - 0s 40us/sample - loss: 0.7728 - acc: 0.8571\n",
            "Epoch 171/230\n",
            "84/84 [==============================] - 0s 65us/sample - loss: 0.7709 - acc: 0.8571\n",
            "Epoch 172/230\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.7691 - acc: 0.8571\n",
            "Epoch 173/230\n",
            "84/84 [==============================] - 0s 72us/sample - loss: 0.7672 - acc: 0.8571\n",
            "Epoch 174/230\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7653 - acc: 0.8571\n",
            "Epoch 175/230\n",
            "84/84 [==============================] - 0s 63us/sample - loss: 0.7634 - acc: 0.8571\n",
            "Epoch 176/230\n",
            "84/84 [==============================] - 0s 60us/sample - loss: 0.7615 - acc: 0.8571\n",
            "Epoch 177/230\n",
            "84/84 [==============================] - 0s 51us/sample - loss: 0.7596 - acc: 0.8571\n",
            "Epoch 178/230\n",
            "84/84 [==============================] - 0s 63us/sample - loss: 0.7577 - acc: 0.8571\n",
            "Epoch 179/230\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7558 - acc: 0.8571\n",
            "Epoch 180/230\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.7539 - acc: 0.8571\n",
            "Epoch 181/230\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.7520 - acc: 0.8571\n",
            "Epoch 182/230\n",
            "84/84 [==============================] - 0s 43us/sample - loss: 0.7501 - acc: 0.8571\n",
            "Epoch 183/230\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7482 - acc: 0.8571\n",
            "Epoch 184/230\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.7463 - acc: 0.8571\n",
            "Epoch 185/230\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7444 - acc: 0.8571\n",
            "Epoch 186/230\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.7425 - acc: 0.8571\n",
            "Epoch 187/230\n",
            "84/84 [==============================] - 0s 41us/sample - loss: 0.7405 - acc: 0.8571\n",
            "Epoch 188/230\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7386 - acc: 0.8571\n",
            "Epoch 189/230\n",
            "84/84 [==============================] - 0s 46us/sample - loss: 0.7367 - acc: 0.8571\n",
            "Epoch 190/230\n",
            "84/84 [==============================] - 0s 45us/sample - loss: 0.7348 - acc: 0.8571\n",
            "Epoch 191/230\n",
            "84/84 [==============================] - 0s 26us/sample - loss: 0.7328 - acc: 0.8571\n",
            "Epoch 192/230\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.7309 - acc: 0.8571\n",
            "Epoch 193/230\n",
            "84/84 [==============================] - 0s 68us/sample - loss: 0.7289 - acc: 0.8571\n",
            "Epoch 194/230\n",
            "84/84 [==============================] - 0s 38us/sample - loss: 0.7270 - acc: 0.8571\n",
            "Epoch 195/230\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.7250 - acc: 0.8571\n",
            "Epoch 196/230\n",
            "84/84 [==============================] - 0s 39us/sample - loss: 0.7230 - acc: 0.8571\n",
            "Epoch 197/230\n",
            "84/84 [==============================] - 0s 41us/sample - loss: 0.7211 - acc: 0.8571\n",
            "Epoch 198/230\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.7191 - acc: 0.8571\n",
            "Epoch 199/230\n",
            "84/84 [==============================] - 0s 36us/sample - loss: 0.7171 - acc: 0.8571\n",
            "Epoch 200/230\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.7152 - acc: 0.8571\n",
            "Epoch 201/230\n",
            "84/84 [==============================] - 0s 24us/sample - loss: 0.7132 - acc: 0.8571\n",
            "Epoch 202/230\n",
            "84/84 [==============================] - 0s 22us/sample - loss: 0.7112 - acc: 0.8571\n",
            "Epoch 203/230\n",
            "84/84 [==============================] - 0s 21us/sample - loss: 0.7093 - acc: 0.8571\n",
            "Epoch 204/230\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.7073 - acc: 0.8571\n",
            "Epoch 205/230\n",
            "84/84 [==============================] - 0s 39us/sample - loss: 0.7053 - acc: 0.8571\n",
            "Epoch 206/230\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.7034 - acc: 0.8452\n",
            "Epoch 207/230\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.7014 - acc: 0.8452\n",
            "Epoch 208/230\n",
            "84/84 [==============================] - 0s 19us/sample - loss: 0.6994 - acc: 0.8452\n",
            "Epoch 209/230\n",
            "84/84 [==============================] - 0s 51us/sample - loss: 0.6974 - acc: 0.8452\n",
            "Epoch 210/230\n",
            "84/84 [==============================] - 0s 27us/sample - loss: 0.6954 - acc: 0.8452\n",
            "Epoch 211/230\n",
            "84/84 [==============================] - 0s 40us/sample - loss: 0.6934 - acc: 0.8452\n",
            "Epoch 212/230\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.6913 - acc: 0.8452\n",
            "Epoch 213/230\n",
            "84/84 [==============================] - 0s 39us/sample - loss: 0.6893 - acc: 0.8452\n",
            "Epoch 214/230\n",
            "84/84 [==============================] - 0s 37us/sample - loss: 0.6872 - acc: 0.8452\n",
            "Epoch 215/230\n",
            "84/84 [==============================] - 0s 30us/sample - loss: 0.6852 - acc: 0.8452\n",
            "Epoch 216/230\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6831 - acc: 0.8452\n",
            "Epoch 217/230\n",
            "84/84 [==============================] - 0s 34us/sample - loss: 0.6811 - acc: 0.8452\n",
            "Epoch 218/230\n",
            "84/84 [==============================] - 0s 20us/sample - loss: 0.6790 - acc: 0.8452\n",
            "Epoch 219/230\n",
            "84/84 [==============================] - 0s 29us/sample - loss: 0.6770 - acc: 0.8452\n",
            "Epoch 220/230\n",
            "84/84 [==============================] - 0s 35us/sample - loss: 0.6750 - acc: 0.8452\n",
            "Epoch 221/230\n",
            "84/84 [==============================] - 0s 31us/sample - loss: 0.6729 - acc: 0.8452\n",
            "Epoch 222/230\n",
            "84/84 [==============================] - 0s 32us/sample - loss: 0.6709 - acc: 0.8452\n",
            "Epoch 223/230\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6688 - acc: 0.8452\n",
            "Epoch 224/230\n",
            "84/84 [==============================] - 0s 33us/sample - loss: 0.6668 - acc: 0.8452\n",
            "Epoch 225/230\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6647 - acc: 0.8452\n",
            "Epoch 226/230\n",
            "84/84 [==============================] - 0s 46us/sample - loss: 0.6627 - acc: 0.8452\n",
            "Epoch 227/230\n",
            "84/84 [==============================] - 0s 47us/sample - loss: 0.6606 - acc: 0.8452\n",
            "Epoch 228/230\n",
            "84/84 [==============================] - 0s 25us/sample - loss: 0.6585 - acc: 0.8452\n",
            "Epoch 229/230\n",
            "84/84 [==============================] - 0s 28us/sample - loss: 0.6565 - acc: 0.8452\n",
            "Epoch 230/230\n",
            "84/84 [==============================] - 0s 48us/sample - loss: 0.6544 - acc: 0.8452\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NCK1ceJhghH",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vAeDinagkdw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "c4faa9bb-5ad3-4db4-ae25-01915217deed"
      },
      "source": [
        "print('================== Initial model ==================')\n",
        "evaluate_model_state(initial_weights)\n",
        "print('================== Final model ==================')\n",
        "evaluate_model_state(final_weights)\n",
        "print('=================================================')\n",
        "print(f'Duration: {(finish_time - start_time):.3f} s')"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================== Initial model ==================\n",
            "Train loss: 1.143, accuracy: 0.333\n",
            "Test loss: 1.166, accuracy: 0.263\n",
            "================== Final model ==================\n",
            "Train loss: 0.652, accuracy: 0.845\n",
            "Test loss: 0.794, accuracy: 0.579\n",
            "=================================================\n",
            "Duration: 1.608 s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}